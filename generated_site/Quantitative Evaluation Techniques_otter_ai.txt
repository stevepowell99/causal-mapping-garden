So when you do an evaluation, what's the product? What do you get? Obviously, you can think of the evaluation report, and that might answer pre determined questions, and it might also include material that goes beyond specific questions we were tasked with answering, for example, to address unanticipated material but also maybe just to describe or contextualize. And of course, another important output is the relational, hopefully people have come together in a way that helps to expand learning and perhaps I to develop project or relationships. But today I want to just talk about something different when you do quantitative research, you might have specific research questions, but often one of the Major outputs is a statistical model of the phenomenon. Now, there might be an effort to go beyond the data and hope that the model generalizes more widely, but that's also not something I want to look at right now. I on in the simplest case, the model might be of a suspected causal relationship between the amount of screen time in the evening, I'm difficult to falling asleep. Do so at the very least, the model will allow us to look at a member of the data set. Look at the case of the data set, and say, on this day, this person looked at a screen for three hours. I it and had, let's say, difficulty of four out of five on falling asleep, on some self rating scale. Now, because we have the model, we can explain that and say, Yes, this is quite a high level of difficulty, and it's explained by at least partly quite a high level of screen time, at least in this individual case, I and it might enable us to make predictions like yes, people who, at least in this context, spend more than three hours on screens in the evening are going to, on Average, experience a high level of difficulty in falling asleep. So that's a very simple model, a more sophisticated one will, on the one hand, probably capture quite a few more variables, and on the other Hand, many kinds of model like directive acyclic graphs, will link up these kinds of connections into a causal network so you can Explain or perhaps even predict how tweaking one variable will affect another. And of course, there's other kinds of models, apart from causal models, but I find these particularly interesting. What there's a relatively small but extremely well funded section of evaluation activity, which is based based around this kind of statistical causal model and randomized control, trials are one facet of that I it so ideally, a randomized control trial will be tasked not primarily with producing a model, but as its most important output, answering a specific question, like, which is the better of these two interventions? Or the does this treatment work better than a placebo? But these are calculations that are conducted on the underlying model, which is a from the point of view of The Workflow involved the major output of the work I it, and which can then be with caveats used more widely. So there's two ways you might want to go on to use that kind of model. One is to answer further questions about the very same data set. For example, to ask whether this particular subset, say people over 70 differ in the way screen time influences difficulty falling asleep, more or less than it does with other subgroups, and if it's an interesting and sophisticated model, i It might allow us to generalize beyond this specific context, For example that might include more general variables, like attention style or eye movement speed, which might help us to I might make a contribution to explaining and predicting behavior in context beyond this very specific one I it on the other hand, of course, most quantitative researchers make a big deal about generalizing the model so beyond the specific use case, beyond the specific context. In fact, the whole point of the study is normally to do that. And there's a whole armory of tools and concepts and arguments about how and under what conditions you can generalize a model to other people, or other people in a different year, or even to other People in a different year in a different Country. With different kinds of screens, etc. I numerically, most evaluations aren't like that, although they might include a quite specific quantitative question somewhere amongst their terms of reference. In most cases, we think of the research outputs as just like a report In which the original possibly modified list of questions I is answered with additional narrative to summarize and link these sections I now going beyond strictly quantitative paradigms. I some evaluation projects will also include what we might call qualitative modeling, for example, if we're Using QCA apart from answering specific questions in the course of answering them, we've quite likely also, we must have also produced a Some QCA style tables, which could help us answer other questions Beyond the ones we are actually tasked with answering the it, and you might see those tables As annexes to the report. And the same goes perhaps for causal loop diagrams and some other techniques, which are I probably best understood as essentially quantitative models, but with a more restricted set of numbers. So for example, in causal loop diagrams, we might model a variable like inflation with a number that goes from minus one through zero to plus one and the same for variables like unemployment or military threats, and we can build models of the relationship between these things, not using absolute numbers. But these simplified sets of numbers. I Is it still listening to me? I so what I want to argue here is that any halfway decent evaluation which at least implicitly gathers qualitative information about how things Within the value and influence one another within the objects of the evaluation influence one another. This kind of evaluation, the entire evaluation, can be considered as constructing a qualitative causal model. This is quite irrespective of the specific methods it uses, even if sync loses, includes something that's explicitly called causal pathways analysis or causal loop monitoring or causal mapping. Quite apart from that, we could consider each and every piece of causal evidence gathered to implicitly form part of a causal network. I don't know about you, but that's what I always thought I was doing when I was doing evaluation, long before I'd ever heard of the term causal mapping, I it. What I was always doing was trying to record information from all the different sources and encode it in the form of things influencing one another. So essentially, you end up with a massive database of fragments of information about contributions of influences. And as a sidebar, we should affirm for the 100th time that no one here is talking about deterministic causation, but simply about influence or contribution. It's which allows for the fact that obviously, one event or factor can be influenced by multiple other factors, and I a change in one factor can be attributed to multiple different factors, possibly in some specific combination. I So under this, according to this view, the results of a questionnaire survey or a randomized control trial or a systematic set of interviews, or even that insightful chat you have with the driver on the way back from the ministry, are all pieces of qualitative causal information which can fit into a broader qualitative causal network and kind of have to if you're going to combine it all to reach evaluative judgments. Of course, the different causal claims don't all have the same might be coded in different ways as having very different characteristics. For example, the testimony of expert before a commission is going To be tagged very differently, and to be analyzed or form part of the analysis in such a way that It's this testimony has more impact on the findings. I and to go back to what I was saying about quantitative models as kind of original paradigm, when we talk about revisiting the same model to get different answer different questions. We often call this querying, querying the model or interrogating the model. So I always thought that the task of simply recording all of these different causal links wasn't really the main problem. The main problem was to group the cause and effect labels into some kind of common language or code book. So for example, if we have information about how attending a certain training, having heard of a particular horrendous case, I influenced a school head teacher to change a mental health policy that's not, in principle, hard To note down, but the question is, how to it's to formulate the labels in such a way that we use a label which is common to other causal links, causes and effects within other causal links elsewhere in the data set, so that we use the same label where we should use the same label And don't use the same label where we shouldn't. I in practice, these fundamental questions of how to structure our causal database is a hard evaluation task. I do we use a completely separate past set of labels for pre COVID and post COVID, or do we simply tag relevant causes and effects with the COVID tag. And do we even call the tag COVID, or do we call it unexpected pandemic, and so on and so forth. So this, so to speak, qualitative, causal evaluation, Analysis Framework is, I think, something that's extremely difficult to specify completely in advance and even with you can specify this. You can, in fact, you can indeed specify this framework, if you like, the shape of causal links, data set, 

Transcribed by https://otter.ai
