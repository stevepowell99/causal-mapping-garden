[{"title": "index", "path": "/index.html", "text": "title: Causal Mapping a Garden of Ideas Starting to use the Causal Map app and want to know more about causal mapping? You've skimmed through a couple of our publications but want to see how it all fits together? Here you'll find dozens of one pagers setting out the key ideas in causal mapping as we see it, curated and assembled from existing publications and blog posts. This site is a work in progress."}, {"title": "better-evaluation", "path": "/000 reference articles/010 better-evaluation.html", "text": "Causal mapping Causal mapping helps make sense of the causal claims (about \"what causes what\") that people make in interviews, conversations, and documents. This data is coded, combined, and displayed in the form of maps. These maps show individuals' and groups' mental models and can support further investigation of causal connections. Causal mapping is designed for the analysis and visualisation of qualitative data about causal links. It can be used to test an existing theory of change or create collective empirical theories of change about how a program works based on stakeholders\u2019 experiences. People\u2019s narratives and reflections about their experiences provide qualitative data that can be coded and displayed as maps to present the cognitive structures (mental models) of individuals and groups and to support further exploration to understand actual causal connections. These causal maps can help to answer questions about what people think happened and what they think caused this by building links between different factors, such as different kinds of outcomes and inputs. Mapping the chains of results and their linkages builds pictures of causal pathways showing the intermediate steps and connections between them. Causal mapping distinguishes carefully between evidence for a causal link and the causal link itself. It does not provide any specific way to make causal inferences from one to the other. Causal mapping can help the evaluator to identify, code, simplify and synthesise the evidence for causal connections, but the evaluative step to make a judgement about whether one thing in fact causally influences another is left to the evaluator. This method is not useful if each piece of evidence is not clearly identified with a source. Examples As explained on the Causal Mapping website: \" A global causal map resulting from a research project can contain a large number of links and causal factors. By applying filters and other algorithms, a causal map can be queried in different ways to answer different questions, for example to simplify it, to trace specific causal paths, to identify significantly different sub maps for different groups of sources, etc. With certain assumptions, it is possible to ask and answer questions like 'which is the largest influence' or 'which is the most positive effect'. \" The figure below shows a map from the application Causal Map, showing coded causal statements for a project that provided farmers with agricultural training and advice in order to increase crop yields. The map has been filtered to show only outcomes downstream of the influence factor \u2018Agricultural training and advice\u2019. Numbers shown indicate how many times the links were made across all interviews. !Causal map from the Causal Map App showing text boxes connected from left to right by arrows Source: BDSR, 2021, p 4 Advice for choosing this method Causal mapping is useful when seeking to understand the causal pathways influencing the outcomes of programs operating in complex settings. It helps make sense of a program and its context in stakeholders\u2019 own words. This includes providing ways to make sense of and organise the different, but sometimes overlapping, labels that different groups use to describe the causal factors that are important to them. Causal mapping is particularly useful for evaluations that focus on learning to inform program improvement as visual representation of causal links between context, activities and outcomes can help to facilitate the sharing and collaborative use of findings. Causal mapping can be used during a program lifespan to inform adaptive management and as part of a final evaluation. Causal mapping can be used to help make sense of large amounts of qualitative data. Using this method requires expertise in coding and analysis of qualitative data. Causal mapping is less frequently used to analyse quantitative data or to do precise mathematical modelling, e.g. of future states of a system under certain conditions. Advice for using this method Care should be taken to ensure that findings are accurately described \u2013 consider the implications of sampling decisions and data collection processes when presenting results. Resources Tools Causal map app This site includes a range of resources on causal mapping and the use of the causal map app including a guide that covers basic and advanced coding and analysis. List of causal mapping software This Wikipedia page provides a list of software applications that can be used for causal mapping. Overview What is causal mapping? This is a one page introduction to causal mapping. Websites Causal map app This site includes a range of resources on causal mapping and the use of the causal map app including a guide that covers basic and advanced coding and analysis. Discussion Papers Causal mapping for evaluators This article provides a background and detailed description of the use of causal mapping in evaluation. Guides From narrative text to causal maps: QuIP analysis and visualisation This paper focuses on analysing raw data to produce useful visual summaries, describing in detail the processes involved in a QuIP analysis. Decision Explorer\u00ae user's guide (PDF) Decision Explorer provides a causal mapping framework that helps to facilitate decision making. The software allows the user to work with a model of inter linked ideas using maps. Trial version available. Comparative Causal Mapping: The CMAP3 Method An introduction to the conceptual backgrounds of causal (cognitive) mapping and to the typical methods in comparative and composite causal mapping, based on either interview or questionnaire primary data or on secondary documentary data. The research is supported by CMAP3, a freely downloadable Windows software platform for causal mapping."}, {"title": "eval2024", "path": "/000 reference articles/eval2024.html", "text": "Pre publication article 2023 Causal mapping for evaluators Steve Powell, Causal Map Ltd James Copestake, Department of Social and Policy Sciences, University of Bath Fiona Remnant, Bath Social and Development Research Ltd Note: This is a pre publication version and as such will be slightly different to the published version in the journal Evaluation which can be accessed at https://journals.sagepub.com/eprint/K9VKYVWWVXZJCVCPNK9S/full Abstract Evaluators are interested in capturing how things causally influence one another. They are also interested in capturing how stakeholders think things causally influence one another. Causal mapping \u2013 the collection, coding and visualisation of interconnected causal claims \u2013 has been used widely for several decades across many disciplines for this purpose. It makes the provenance or source of such claims explicit and provides tools for gathering and dealing with this kind of data and for managing its Janus like double life: on the one hand, providing information about what people believe causes what, and on the other hand, preparing this information for possible evaluative judgements about what causes what. Specific reference to causal mapping in the evaluation literature is sparse, which we aim to redress here. In particular, the authors address the Janus dilemma by suggesting that causal maps can be understood neither as models of beliefs about causal pathways nor as models of causal pathways per se but as repositories of evidence for those pathways. Introduction This article aims to share the experience of the authors in using the approach of causal mapping within the sphere of evaluation and their development of more specific guidelines for evaluators who might be interested in using this as a tool for collating and analysing evidence for causal pathways. During 20 years of conducting evaluations, the lead author became persuaded of the central importance of collecting and being able to aggregate causal propositions embedded in written and spoken data. Further interest in causal mapping arose from discussions with the other authors who had developed a qualitative impact evaluation protocol (the QuIP), which relies on causal mapping for analysis of narrative data. This prompted further expansion of the search for literature and software that could assist in systematically constructing causal maps as a way of presenting the outcome of such impact evaluation studies. A main product of this action research has been the design of new software and detailed guidelines for causal mapping, which have already been used in many evaluations around the world. This article draws on our joint experience of causal mapping to outline the scope for evaluators to use the approach more systematically and widely. This article starts with a brief history of causal mapping and clarifies some definitions. It then reviews causal mapping within evaluative practice, distinguishing the tasks of gathering data, coding causal claims and answering evaluative questions. The final section concludes with reflections on the strengths, weaknesses and future potential of causal mapping in evaluation. A brief history of causal mapping Causal mapping \u2013 diagramming beliefs about what causes what \u2013 has been used since the 1970s across a range of disciplines from management science to ecology. The idea of wanting to understand the behaviour of actors in terms of their internal maps of the world can be traced back further to field theory (Tolman, 1948) which influenced Kelly\u2019s \u2018personal construct theory\u2019 (Kelly, 1955). A seminal contribution was made by Robert Axelrod in political science, with the book The Structure of Decision (Axelrod, 1976). Causal mapping is largely based on \u2018concept mapping\u2019 and \u2018cognitive mapping\u2019, and sometimes the three terms are used interchangeably, although \u2018causal mapping\u2019 strictly involves maps that only include explicit causal links, rather than, for example, relationships like \u2018membership\u2019.3 Axelrod\u2019s book presents a comprehensive idiographic approach to how individuals make decisions which he himself mostly refers to as \u2018cognitive mapping\u2019 (although his definition makes it clear that all links are causal). An appendix to the book (Wrightson, 1976) gives details about how to code causal links. Bougon et al. (1977) applied a similar approach to a study of the Utrecht Jazz Orchestra as an organisational unit, eliciting \u2018cause maps\u2019 from several individual members and amalgamating them. One strand of literature about causal mapping can be located within the wider literature on sensemaking in organisations pioneered by Weick (1995), and applications within organisations were present almost from the start. By 1990, there were many different applications of similar ideas, including an edited book (Huff, 1990) that offered a unitary approach to \u2018concept mapping\u2019 in the United States. Most authors (Ackermann and Alexander, 2016: 892; Clarkson and Hodgkinson, 2005: 319; Fiol and Huff, 1992: 268; Laukkanen, 2012: 2; Narayanan, 2005: 2) use a broadly similar definition of a causal map: A causal map is a diagram, or graphical structure, in which nodes (which we call factors) are joined by directed edges or arrows (which we call links), so that a link from factor C to factor E means that someone (P) believes that C in some sense causally influences E. There is a constructive ambiguity (Eden, 1992) about what a collective map is a map of: While maps constructed as a consensus within a group can plausibly be claimed to map \u2018what the group thinks\u2019, this is more problematic for maps constructed post hoc by synthesising individual maps. We found no significant deviations from this basic definition of a causal map across all the variants of causal mapping reviewed in the following sections, with the caveat that there is variation in how explicit different authors are in describing causal links as representing bare causation as opposed to beliefs about causation. In the following decades, Eden et al. (1992) applied the approach to understanding and supporting decision making in organisations, increasingly using the phrase \u2018causal mapping\u2019 rather than \u2018cognitive mapping\u2019, and they subsequently extended the application of causal maps to fields as varied as risk elicitation and information systems development (Ackermann and Eden, 2011; Ackermann et al., 2014), also developing a series of software packages beginning with Decision Explorer (Ackermann et al., 1996). There is now a wealth of literature on using causal mapping for decision support in organisations (including sophisticated approaches to formalise decision support (Montibeller et al., 2008) and even to rank options (Rodrigues et al., 2017)). Laukkanen (1994, 2012; Laukkanen and Eriksson, 2013) also wrote extensively on causal mapping and developed a software programme called CMAP3 for processing both idiographic and comparative causal maps by importing, combining and analysing factors and links attributed to one or more sources. A broadly similar approach was taken by Clarkson and Hodgkinson (2005) with their Cognizer approach and software. Table 1 shows some highlights from the extensive literature on causal mapping. Many of the key ideas were already in place by the end of the 1970s. The subsequent literature covers a variety of specific techniques to elicit maps from documents, individuals, sets of individuals and groups, with or without software support, following protocols from the purely open ended to those which use strictly pre defined lists of factors and links (see Hodgkinson et al (2004) for a comparison of methods), and with aims ranging from strictly idiographic (understanding individuals in specific contexts as Axelrod did) to more nomothetic, such as Tegarden et al. (2016). Renewed interest in causal mapping may also be reinforced by the \u2018causal revolution\u2019 in quantitative data science initiated by Judea Pearl (Pearl, 2000; Pearl and Mackenzie, 2018), which has fundamentally challenged the almost total taboo placed on making or assessing explicit causal claims, which was dominant in statistics for much of the twentieth century (Powell, 2018), and this has in turn helped rekindle interest in explicitly addressing causation using qualitative methods. Causal mapping and most related approaches share the basic idea that causal knowledge \u2013 whether generalised or about a specific case or context \u2013 can be at least partially captured in small, relatively portable \u2018nuggets\u2019 of information (Powell, 2018: 52). These can be assembled into larger models of how things worked, or might work, in some cases. More ambitiously, they may contribute to constructing \u2018middle level theory\u2019 theory, useful for understanding causal processes in other contexts, without necessarily reaching the level of overarching scientific laws (Cartwright, 2020). Causal nuggets are also related to the mechanisms that help to explain how people behave in different contexts (Pawson and Tilley, 1997; Schmitt, 2020). These can be thought of as causal schema and linked to the hypothesis that human knowledge is stored in chunks that are activated and combined with others in relevant circumstances. This would suggest that we humans do not have a comprehensive set of causal maps in our heads at any one time, but we do have a set of more basic components and the ability to assemble them when the situation calls for it, including when prompted by a researcher. Table 1. Major milestones in the development of the evaluation tool \u2018causal mapping\u2019 \u2013 the collection, coding and visualisation of interconnected causal claims. | Reference | Main application of causal mapping | Mode of construction | Dealing with multiple sources | Analysis procedures | | | | | | | | (Axelrod, 1976) | Understand and critique decision making | Coding documents | Mainly idiographic | Compute polarity of indirect effects in some cases. | | (Bougon et al., 1977) | Understand how organisations are constructed and can be influenced. | Semi structured interview to identify a fixed list of factors aka \u201cvariables\u201d; respondents then say which are linked and give the polarity. | Compare individual maps and combine into global \u201caverage\u201d map. | Identify variables X with high outdegree and Y with high indegree and construct an \u2018etiograph\u2019 to show all the multiple paths from one point to another; discuss how respondents might have influence over some variables. | | (Ackermann and Eden, 2004, 2011; Eden, 1992; Eden et al., 1979, 1992) | Decision support and problem solving in organisations. Maps are seen primarily as useful tools rather than research about reality. | Open interviewing of several respondents based on Kelly\u2019s Personal Construct Theory. Also map construction directly with groups (1988). | Comparing maps between individuals and analysing group maps directly. | Various structural measures, presence of isolated clusters, hierarchical trees, loops Simplify individual maps by collapsing X Y Z into X Z. | | (Laukkanen, 1994, 2012; Laukkanen and Eriksson, 2013; Laukkanen and Wang, 2016) | Explicitly cognitive, to improve knowledge and understanding in management | Systematic comparative method with semi structured interviewing: respondents are given anchor topic(s) then asked for causes, effects, causes of causes, effects of effects. Compress the data by standardising factor names. Comprehensive coverage of different map construction possibilities. | Comparative study of different individual maps, combining data into a database. | Display combined maps for subgroups, e.g. all local managers. | This approach suggests that our everyday causal understanding is as primary as our perception of, say, colour and arises from more than empirical observations of associations between objects or events; our ability to infer causation goes beyond and is not primarily based on noting correlations. And for all its complexity and intuitive brilliance, it is also just as fallible as our perception of colour or size. This reaffirms our practice as evaluators of taking the causal claims and opinions of humans (experts and non experts) seriously (Maxwell, 2004a, 2004b); indeed, this kind of information is the bread and butter of most evaluations. Distinguishing causal mapping from related approaches Most evaluators are probably more familiar with related approaches under the term \u2018systems mapping\u2019, recently covered by Barbrook Johnson and Penn (2022). They provide an overview table of relevant methods on pp. 169 ff. \u2013 fuzzy cognitive maps (FCM), participatory systems mapping (PSM), Bayesian belief networks (BBN), causal loop diagramming (CLD), systems dynamics (SD) and theory of change (ToC) \u2013 which will be briefly mentioned here. SD, CLDs, FCMs and BBNs are all ways to encode information about networks of interconnected causal links and follow formal inference rules to make deductions based on them, for example, to calculate the strength of indirect effects or to predict behaviour over time. The oldest of the three methods, SD (Forrester, 1971), models flows of a substance (for example, of energy or money) within a network over time, whereas the other three methods model \u2018bare\u2019 causal connections between network elements. SD uses general mathematical functions to model the connections and explicitly models non linear relationships. CLDs are related but mathematically simpler, modelling causal effects in a semi quantitative way. FCMs might seem to be of more interest for causal mapping; Kosko\u2019s original article on FCM (Kosko, 1986) takes Axelrod\u2019s work as its starting point. This tradition (Chaib Draa and Desharnais, 1998; Khan and Quaddus, 2004; Taber, 1991) was originally introduced to model causal reasoning (Kosko, 1986: 65): If person or group P believes the set of causal propositions making up a map M, the model attempts to predict the strength with which they could or should also believe some other propositions, for example, about indirect effects and how they might change over time. In practice, however, FCM is less interested in cognition than in making predictions about the world. The difference between FCM and the other three methods is more about the fuzzy logic used to make the predictions rather than about the cognitive nature of the data. BBNs are also designed to make causal inferences by doing calculations with data about causal connections. While FCMs make essentially qualitative predictions such as \u2018increasing\u2019 and \u2018decreasing\u2019, BBNs use directed acyclic graphs (networks without loops) to make quantitative predictions about the probability of events, particularly about the probability that one event was the cause of another. All four approaches are primarily ways to make predictions about causal effects within a network of factors, and (despite the words \u2018cognitive\u2019 and \u2018belief\u2019 in the names of two of the four) the relative lack of interest in who is doing the reasoning sets FCM, BBNs and SD apart from causal mapping as outlined earlier. In the last few years, PSM has featured in several publications in evaluation journals and guides (Barbrook Johnson and Penn, 2021; Hayward et al., 2020; Sedlacko et al., 2014; Wilkinson et al., 2021), alongside mapping of \u2018systems effects\u2019 (Craven, 2020). Indeed, Craven\u2019s work (see also Craven, 2017) can be considered causal mapping with a particular emphasis on systems aspects. Barbrook Johnson and Penn (2022) explicitly exclude causal maps from their overview of systems mapping because they are arguably included via FCM and because they \u2018sometimes emphasise developing representations of individual mental models rather than representations of systems\u2019 (p. 11). Nevertheless, PSM is closer to the tradition of causal mapping (and of more direct interest to evaluators) than the previous four approaches because it is a more concrete and pragmatic intervention to construct a map with specific group of stakeholders to support decisions. A devotee of causal mapping could claim that approaches like PSM are just variants of what they have been doing for the last 50 years, just as a devotee of systems mapping might consider causal mapping as a form of PSM. Finally, logic models and ToC can be considered causal maps in which they make assertions about past or future causal links that one or more stakeholders believe to be important. They are also political artefacts that aim to justify and inform action by establishing an agreed synthesis of multiple perceptions of change and may also gain legitimacy by being the product of an agreed process of participatory planning and co design. They do not, however, normally retain information about which stakeholder(s) believe which claim. Reflecting on logic models and theories of change provides one entry point for thinking more carefully both about who actually makes these claims and about the symbols and rules employed to construct them (Davies, 2018). We think it is useful to distinguish this tradition of causal mapping from related activities in six ways, as set out in the following section. None of these distinctions are definitive, and many are shared with other approaches. To systems people who want to say that causal mapping is just systems mapping and to causal mappers who want to say that systems mapping is just causal mapping (and we have heard both arguments many times), we can only say, perhaps we should all just get to know each other first. First, the raw material for causal maps comprises claims about, perceptions of or evidence for causal links. Causal maps are primarily epistemic, meaning that their constituent parts are about beliefs or evidence, not facts; yet their logic tends to be parallel to, and based upon, the logic of non epistemic systems maps and similar diagrams that are broadly used across a range of sciences. Some systems mapping techniques are also sometimes concerned with stakeholder beliefs; causal mapping does this more systematically. Second, causal maps tend to be unsophisticated about the types of causal connection they encode. To explain this, we should note that causal claims in ordinary language are expressed in an endless variety of ways: \u2018C made E happen\u2019, \u2018C influenced E\u2019, \u2018C may have been necessary for E\u2019, \u2018C was one factor blocking E\u2019, \u2018C had a detrimental effect on E\u2019, \u2018C had a surprisingly small effect on E\u2019 and so on. With a few exceptions, causal mapping analysts do not even try to formally encode this rich and unsystematic range of causal nuance, relying instead simply on the lowest common denominator: A link from X to Y means simply that someone claims that X somehow causally influences or influenced Y. There is one exception: Many causal mapping approaches do accommodate information about the polarity of links, marking each link as either positive or negative, for example, the claim \u2018the recession led to unemployment\u2019 could be coded as a negative link from \u2018the recession\u2019 to \u2018employment\u2019. In general, causal maps usually encode a belief about partial causal influences of C on E and only in special cases do they encode total or exclusive causation such that C entirely determines E. This also means that encoding a claim does not require us to make any judgement about the quality of the evidence or the ability of the source to judge that this link was causal (although it may be very useful to do so). Figure 1. Combining two separate single source causal maps into a multi source map: an illustrative example. !A diagram of a diagram Description automatically generated Third, causal mapping often handles large numbers of causal claims, sometimes many thousands. Handling large numbers of claims en masse in this way is made much easier because of the relatively unsophisticated nature of the way claims are coded (as discussed earlier). Related approaches in evaluation tend to bring more sophisticated tools to bear on a much smaller number of causal links. In process tracing, for example, researchers may produce diagrams depicting claims about causal links but tend to focus on testing the strength of a relatively small number of specific \u2018high stakes\u2019 causal links, whether through verbal reasoning, application of Boolean logic or Bayesian updating (Befani and Stedman Bryce, 2017). Fourth, causal maps may originate from one or many sources, each reporting on one or many cases. In a causal map, the links all originate from one person or document a \u2018single source\u2019 or \u2018individual\u2019 or \u2018idiographic\u2019 causal map, as in Axelrod\u2019s original work (Axelrod, 1976). But we can also draw causal maps that incorporate information from a variety of different sources, as illustrated in Figure 1. The simplest causal maps refer to only one context and contain information from only one source (which may be the consensus view of several people, treated as speaking with a single voice). Various forms of systems mapping such as PSM could be understood as a special case of causal mapping in this sense. There are many other variants. One source might give differentiated information about different cases or contexts, or many sources might give information about just one context, as when different water systems experts each give their (possibly differing) opinion about the same water catchment area, for example. Figure 2. From text to causal mapping via coding, an illustrative example. Another frequent type of causal map is drawn from many sources, each reporting on their own situation or context, such as their perception of drivers of change in their own lives. In coding and analysis of this sort of data, one source equals one case and one context; these can subsequently be aggregated across many sources who, for example, all share a similar context. Fifth, causal maps do not necessarily specify a clear system boundary. The boundaries of a causal map are usually defined more loosely, partly by data collection but also by the sources themselves. Indeed, some systems proponents would say that the term \u2018systems diagram\u2019 simply signals a readiness to use systems approaches (Williams, 2022). Finally, causal mapping, especially in management sciences and operations research, has nearly always been at least as interested in process as in the result. There is often a focus on the process of reaching consensus as part of the task of solving a business problem, rather than on the universal accuracy or validity of the final map. Janus: Causal mappers face in two directions It is worth concluding this section by highlighting how, like Janus, the causal mapper looks in two directions at once: sometimes interpreting maps as perceptions of causation but also often wanting to make the leap to inferences about actual causation. As Laukkanen and Wang (2016: 3) point out, while conceptually poles apart, in practice, the two functions can be hard to distinguish, particularly without sufficient explanation about source information and how this has been analysed. We see the job of the causal mapper as being primarily to collect and accurately visualise evidence from different sources, often leaving it to others (or to themselves wearing a different hat) to draw conclusions about what doing so reveals about the real world. This second interpretative step goes beyond causal mapping per se (Copestake, 2021; Copestake et al., 2019a; Powell et al., 2023). Seen as models of the world, causal maps, like systems maps, are fallible but useful: We can use inference rules (which are explicitly set out in FCMs, SDs, BBNs and CLDs and are implicit in other related approaches) to make deductions about the world. Seen as models of individuals\u2019 causal beliefs, we can arguably use analogous rules, perhaps also including rules from epistemic logic, to make deductions about what individuals ought to believe. However, we argue that evaluators can break the Janus dilemma and make the best use of causal maps in evaluation by considering causal maps not primarily as models of either beliefs or facts but as repositories of causal evidence. We can use more or less explicit rules of deduction, not to make inferences about beliefs, nor directly about the world, but to organise evidence: to ask and answer questions such as: \u2022 Is there any evidence that X influences Z? \u2022 . . . directly, or indirectly? \u2022 . . . if so, how much? \u2022 Is there more or less evidence for any path from X to Z compared to any path from W to Z? \u2022 How many sources mentioned a path from X to Z? \u2022 . . . of these, how many sources were reliable? We also argue that this is a good way of understanding what evaluators are already doing: gathering and assembling data from different sources about causal connections in order to weigh up the evidence for pathways of particular interest, like the pathways from an intervention to an outcome. Causal mapping in evaluation: Three tasks Causal mapping offers ways to organise, combine, present and make deductions from a large number of relatively unstructured causal claims \u2013 the sort of data that are often collected in evaluations. Different approaches to these three tasks are discussed in turn in the following sections. Task 1: Gathering narrative data How to collect causal claims from which to draw causal maps? There are a wide variety of options, including in depth individual interviews (Ackermann and Eden, 2004), reuse of open ended questions in structured surveys (Jackson and Trochim, 2002), literature reviews (in which \u2018sources\u2019 can be documents rather than individuals) and archival or secondary material within which pre existing causal claims are already made (Copestake, 2020). Other approaches aim to build consensus by using structured collaborative processes, including Delphi studies and PSM (Penn and Barbrook Johnson, 2019). Guidelines for causal mapping may include procedures for collecting primary data, with forms of elicitation including back chaining (\u2018what influenced what?\u2019) and forward chaining (what resulted, or could result, from this?) When gathering primary data, the way in which questions are asked influences the meaning of the maps and their links. For example, in the QuIP, (Copestake et al., 2019b) respondents are asked to identify causes of changes, then causes of the causes and so on. This means that most of the factors are not expressed directly as variables that may go up or down (e.g. \u2018harvest\u2019, \u2018hunger\u2019) but already as changes in something, such as \u2018an improved harvest\u2019 or \u2018reduced hunger\u2019). This has implications for how positive and negative statements are combined, as discussed in the following section. With primary data collection, we can distinguish between relatively closed and open approaches and whether respondents are forced to choose between pre selected optional answers or can formulate their own (see Table 2). Interviewers may also be guided by a chaining algorithm; for example, they may be instructed to iteratively ask questions like \u2018You mentioned X, please could you tell me what were the main factors that influenced X or led to it happening.\u2019 Table 2. Different approaches within primary data collection for causal mapping, with example questions. | Admissible answers Scope of questions | Explicit : factors are explicitly identified | Implicit : factors are not explicitly named | | | | | | Closed: questions with a predetermined focus | Which factors in this list influenced this particular event? | What influenced this particular event? | | Open: a freer discussion | Identify the biggest change you experienced in relation to X, and list three factors that influenced it | Tell me what has changed for you in the last x years | Task 2: Coding causal claims or causal qualitative data analysis Some approaches such as that suggested by Markiczy and Goldberg (1995) directly elicit causal links from their sources, perhaps by asking respondents to suggest causal links between a predetermined list of causal factors, and thus, after finishing Task 1, are already in a position to create causal maps. More explicitly, qualitative approaches are faced with Task 2: encoding causal claims in the form of explicit causal links and factors. This task is similar to ordinary qualitative data analysis (QDA), whether done manually or using tools like NVivo, Dedoose and AtlasTI. However, these tools are designed to capture general concepts, rather than claimed causal links between concepts, which is what we need for causal mapping. QDA for causal mapping also starts with a corpus of narrative data, but it does not create causal links between independent concepts that might already have been coded using ordinary non causal thematic analyses. Rather, in causal QDA, the primary act of coding is to highlight a specific quote from within a statement and identify the causal claim made by simultaneously identifying a pair of causal factors: an \u2018influence factor\u2019 and a \u2018consequence factor\u2019. The causal factors only exist as one or other end of a causal link and have no meaning on their own. Each claim forms a link in the visual representation of the causal map. The Axelrod school had its own coding manual describing how to highlight areas of text expressing causal connections and code them as links between causal factors, originally inspired by evaluative assertion analysis (Osgood et al., 1956). Manual causal coding of text data, like ordinary thematic coding, requires a considerable investment of time and expertise to do well. We now use natural language processing to at least partially automate this; however, the process is essentially the same, and discussion of this is beyond the scope of the present article. Where do the labels for the causal factors come from? As with ordinary QDA and thematic analysis (Braun and Clarke, 2006), approaches vary in the extent to which they are purely exploratory or seek to confirm prior theory (Copestake, 2014). Exploratory coding entails trying to identify different causal claims embedded in what people say, creating factor labels inductively and iteratively from the narrative data. Different respondents will not, of course, always use precisely the same phrases, and it is a creative challenge to create and curate this list of causal factors. For example, if Alice says \u2018Feeling good about the future is one thing that increases your wellbeing\u2019, is this element \u2018Feeling good about the future\u2019 the same as \u2018Being confident about tomorrow\u2019 which Bob mentioned earlier? Should we encode them both as the same thing, and if so, what shall we call it? We might choose \u2018Positive view of future\u2019, but how well does this cover both cases? Laukkanen (1994) discusses strategies for finding common vocabularies. As in ordinary QDA, analysts will usually find themselves generating an ever growing list of factors and will need to continually consider how to consolidate it \u2013 sometimes using strategies such as hierarchical coding or \u2018nesting\u2019 factors (as discussed in the following section). The alternative to exploratory coding is confirmatory coding, which employs an agreed code book, derived from a ToC and/or from prior studies. QuIP studies mostly use exploratory coding but sometimes supplement labels with additional codes derived from a project\u2019s ToC, for example, \u2018attribution coding\u2019 helps to signify which factors explicitly refer to a specific intervention being evaluated (Copestake et al., 2019b: 257). However, careful sequencing matters here because pre set codes may frame or bias how the coder sees the data (Copestake et al., 2019a). Again, the positionality of the coder matters just as much when doing causal coding as it does for any other form of qualitative data coding. Combining Tasks 1 and 2 Tasks 1 and 2 result in a coded data set of causal claims, each of which consists of (at the very least) the labels for a pair of causal factors. Those using a more explicit elicitation approach have been able to skip Task 2. Task 3: Answering evaluation questions Causal maps help us to assemble evidence for the causal processes at work in specified domains, including the influence of activities being evaluated. They can also help expose differences between the evidence given by different sources and differences between the analysed data and theories of change derived from other sources, including those officially espoused by the commissioner of the evaluation (Powell et al., 2023). The identification of differences in understanding can then feed into further enquiry, analysis and action concerning why people have different views, what the implications of this are and how these might be addressed. Focusing on causal claims is of course only one way of answering evaluation questions from a corpus of text data. But it is productive because many evaluation questions are at least partly about causation and causal contribution, and we have found that causal mapping points to possible answers to these questions relatively rapidly compared to more generic QDA approaches. Answering questions about efficiency, effectiveness, impact and sustainability, for example, all depend on identifying the causal effects of a specific intervention, be they perceived as positive or negative, intended or unintended (OECD, 2010). Even \u2018relevance\u2019 can have a causal interpretation in the sense that an intervention is relevant if it is doing the right thing: Whether it is likely to help to address the needs of stakeholders is at least partly a judgement about its causal powers. For a data set comprising hundreds or thousands of links, an unfiltered global map of all the links is a bewildering and useless \u2018hairball\u2019 that includes everything but highlights nothing. One way to simplify is to derive from the global map several smaller maps that focus on different features of the data. For example, maps may selectively forward chain the multiple consequences of a single cause \u2013 including those activities being evaluated: effects of causes (Goertz and Mahoney, 2006) \u2013 or trace back to the multiple contributory causes of an anticipated or highly valued outcome or consequence: causes of effects. A series of simpler causal maps, each selected transparently to address a specific question, generally adds more value to an evaluation than a complicated, if comprehensive, single map that is hard to interpret. The downside of this is that selectivity in what is mapped and is not mapped from a single database opens up the possibility of deliberate bias in selection, including omitting to show negative stories. Sets of individual links with the same influence and consequence factor (co terminal links) are usually represented bundled together as a single line, often with thickness of the line indicating the number of citations, and/or with a label showing the number of links in the bundle. The map has not fundamentally changed, but the visualisation is much simpler. Another way to simplify a global causal map is to produce an overview map showing only the most frequently mentioned factors and/or links. Care should be taken if this leads to omitting potentially important but infrequently mentioned evidence about, for example, an unintended consequence of an intervention. Another common way to simplify is to combine sets of very similar factors into one. For example, if hierarchical coding has been used, it is possible (with caveats) to \u2018roll up\u2019 lower level factors (such as health behaviour; hand washing and health behaviour; boiling water) into their higher level parents (health behaviour), rerouting links to and from the lower level factors to the parent (Bana e Costa et al., 1999). Large causal maps can also be analysed quantitatively, including by tabulating which factors are mentioned most often, identifying which are most centrally connected or calculating indicators of overall map density, such as the ratio of links to factors (Klintwall et al., 2023; Nadkarni and Narayanan, 2005). We are wary of the value of summarising maps in this way, not least because results are highly sensitive to the granularity of coding. For example, although a specific factor such as \u2018improved health\u2019 might have been mentioned most often, if two subsidiary factors had been used instead (such as \u2018improved child health\u2019 and \u2018improved adult health\u2019), these two separate factors would not have scored so highly. Causal mapping has some limitations. First, the credibility of the causal arguments which can be derived from a map is limited by the credibility of the original data sources. We see the job of causal mapping as collecting, organising and synthesising a large number of claims about what causes what; drawing conclusions about what this actually reveals about the world is a final step that goes beyond causal mapping per se. In specific cases, establishing explicit and context specific rules of inference may help to make this final step. For example, it might be agreed that a reasonable threshold of evidence that C influenced E is that (i) a specified number or proportion of respondents independently mentioned the link; (ii) the connection can be plausibly explained theoretically and (iii) researchers anticipated confirmation bias and other potential sources of bias and took adequate steps to mitigate against them. See Tegarden et al. (2016) for a discussion of the role of anonymity in causal mapping in organisations. A second limitation of causal mapping is the difficulty it has in systematically capturing the strength or type of causal influence. It is relatively rare in open conversation for people to indicate in a consistent way the magnitude of the effect of C on E, or whether C was a necessary or sufficient condition for E or precisely how certain they are about the connection. There is of course scope for framing questions to encourage people to ascribe weights to their answers, which can then be incorporated into the way maps are constructed. But imposed precision risks turning into spurious precision, and stronger framing of questions may distract from other issues and nuances that more open ended questioning might otherwise have elicited. Figure 3. An illustrative example of a very simple causal map. !A diagram of a diagram Description automatically generated An illustrative example A positive feature of causal maps, illustrated by Figure 3, is that they capture a lot of information in a way that is quick and easy to understand. This example reveals that Source S provided a narrative that connects the intervention to improved feeling of wellbeing as a direct consequence of taking more exercise and via the effect of this on their health. This source also suggests a positive feedback loop, with more exercise making them more physically fit and encouraging even more exercise. The information from Source T is more fragmented; there are two causal statements claiming that improved feeling of wellbeing can result from more exercise and improved health, although T does not link the two causally, nor make any causal link back to the intervention. In addition, T suggests that an additional factor, \u2018more confidence in the future\u2019, also contributes to improved feeling of wellbeing. The two sources of evidence do agree on certain points; there is scope for generalisation beyond either individual source (and can be scaled up from here), both in assessing the multiple outcomes of the intervention and in understanding what explains improved feeling of wellbeing. Generalisability is strengthened when a link is reported by different sources in different contexts. We believe that within causal mapping, we should never make the mistake of thinking that stronger evidence for a causal link is evidence that the causal link is strong; only that there is more evidence for it. The example also reveals some weaknesses of causal maps. First, there is ambiguity about the precise meaning of the labels and the extent to which their use is conceptually equivalent between the two sources. There is also ambiguity about whether they are referring to their own personal experience (and if so, over what period) or speaking in more general terms. Furthermore, the diagram sacrifices details, including how the statements shown relate to the wider context within which each source is situated. To mitigate this, an important feature of any causal mapping procedure is how easily it permits the user to trace back from the diagram to the underlying transcripts and key information about the source (e.g. gender, age, location etc.). Where this is possible, the diagram can be regarded in part as an index or contents page \u2013 an efficient route to searching the full database to pull out all the data relating to a specific factor or causal link. Even with a simple example like this, we can answer many questions by visually examining the paths. But analysis of larger data sets might be simplified by selecting only all the paths between a selected cause and consequence to produce what Bougon et al (1977) called an \u2018etiograph\u2019. Eden et al. (1992) go so far as to collapse some causal paths into individual links, simply removing intervening factors. Our causal map software uses the \u2018maximum flow/minimum cut\u2019 algorithm (Erickson, 2019), which quantifies the robustness of longer paths from C to E by calculating the minimum number of causal claims, which would have to be invalidated or lost to remove any possible causal pathway between them. This is simply the idea that the strength of an argument is dependent on the strength of its weakest link, extended to apply to an interconnected network rather than a single chain. In other words, we can express a path through a causal map as a possible argument: An argument can be constructed that C causally influences D, and then E. This also provides ways to formally address questions such as \u2018how robust is the evidence for the influence of C on E, compared to evidence for the influence of B on E?\u2019. As our causal evidence will often be of varying quality and reliability, we are advised to also construct and compare paths that consist only of evidence from the most reliable sources. We prefer to talk about possible arguments here because this sidesteps the (also interesting) question of whether any individual source made any such argument in its entirety, with all its constituent links.9 In many circumstances, evidence for a causal path derived from different sources and contexts can be considered to strengthen the argument, whereas heaps of evidence from the same source will not. To address this kind of issue directly, we can use a complementary measure \u2018source thread count\u2019 as a measure of the strength of the argument from C to E: the number of sources, each of which mentions any complete path from C to E. Where there are many sources, it may also be useful to identify variation in which sub groups support different possible arguments. For example, Markiczy and Goldberg (1995) use dimension reduction techniques on the table of all links reported by all sources to identify clusters of sources so that the members of each cluster are maximally similar to one another in terms of the links they report, and so that the clusters taken as a whole are maximally different from one another. These groups can also be cross tabulated with existing metadata, to interpret them as, for example, young city dwellers versus older rural residents. The transitivity trap Transitivity is perhaps the single most important challenge for causal mapping. Consider the following example. If source P [pig farmer] states \u2018I received cash grant compensation for pig diseases [G], so I had more cash [C]\u2019, and source W [wheat farmer] states \u2018I had more cash [C], so I bought more seeds [S]\u2019, can we then deduce that pig diseases lead to more cash which leads to more seed (G \u00e0 C \u00e0S), and therefore G \u00e0 S (there is evidence for an indirect effect of G on S, i.e. that cash grants for pig diseases lead to people buying more seeds)? The answer is of course that we cannot because the first part only makes sense for pig farmers, and the second part only makes sense for wheat farmers. In general, from G \u00e0 C (in context P) and C \u00e0 S (in context W), we can only conclude that G \u00e0 S in the intersection of the contexts P and W. Correctly making inferences about indirect effects is the key benefit but also the key challenge for any approach which uses causal diagrams or maps, including quantitative approaches (Bollen, 1987). Conclusion Constructing causal maps can help evaluators use narrative evidence from multiple sources to answer questions about the evidence for the influence of interventions on outcomes (direct and indirect; intended and unintended) along with the evidence for influence of contextual factors. Second, causal maps also enable evaluators to sort, search, summarise and analyse narrative evidence more easily and systematically and to select suitable examples and quotations to illustrate key observations and arguments. Third, done correctly, it can enhance the credibility of causal findings and conclusions by making the process by which they were generated more systematic and transparent, leaving them open to peer review. Fourth, causal maps produce graphics that can cut through the complexity of language and grammar by visualising the relatively simple cognitive connections that inform our everyday thinking and speech. In addressing core questions which evaluators are tasked with answering, causal mapping also draws on a long if fragmented literature; it can be integrated or combined with a range of methodological approaches and methods, including realist evaluation (Pawson and Tilley, 1997), outcome harvesting (Wilson Grau and Britt, 2012), process tracing (Bennett and Checkel, 2015), contribution analysis (Mayne, 2001), most significant change (Dart and Davies, 2003) and the QuIP (Copestake et al., 2019a). What has been lacking, we suggest, is greater clarity and consistency about what causal mapping means for evaluators. Key to doing so is recognising head on the ambiguity of much narrative causal data, particularly when confronted with large bodies of data collected in disparate ways. Evaluators must contend with messiness: imprecise system boundaries, differing specification of claimed causal influences and lack of clear or consistent information about what case or group of cases claims refer to. Causal mapping can contend with all this ambiguity rather than shying away from it. It can make use of messy operational data, treating urgent, unexpected and unstructured information at face value. This is made possible by distinguishing clearly between two analytical steps in evaluation: The first is to gather, understand and assemble causal evidence from different sources (those in a position to have useful evidence about relevant causal links and chains) to construct, compare and contrast the evidence for and against different possible causal pathways. By focusing on this task, causal mapping lays a more reliable foundation for the second, often critical, task of using the assembled data to make judgements about what is in fact really happening. This avoids the confusion and ambiguity that often arises when evaluators seek to address both steps simultaneously by constraining what data are collected to fit a prior view of reality which other stakeholders may or may not share. Focusing the methodological lens on causal mapping as a discrete, central evaluation task opens promising avenues for further research. First, there is scope for causal meta analysis of existing evaluation reports (see Powell (2020) for an example). Second, there is scope for more research into the task of integrating causal mapping with wider debates about the interface between fact and value (Powell, 2019) \u2013 evaluation as social science and as applied moral philosophy (Schwandt and Gates, 2021). Third, there is scope for delving more deeply into the process and logic of causal mapping itself, including how to manage directly conflicting causal claims, how to assess and improve inter rater reliability of coding and how to automate coding using machine learning. In our effort to recommend causal mapping to evaluators, we have relied heavily on the extensive causal mapping literature. In this article, we have also briefly outlined some modest contributions of our own. First, we outline a way to understand document processing for causal mapping as \u2018causal QDA\u2019, in which the tags are entire causal links. We mentioned our barefoot approach to causal QDA, which sticks close to the original text and unusually does not rest on an assumption that causal factors are variables and which therefore does not distinguish between positive and negative links. Second, we show evaluators a way out of the Janus dilemma (is a causal map a model of the world? Or is it a model only of beliefs?) by understanding causal maps as repositories of evidence for interconnected causal links, out of which it is possible to construct and compare arguments for causal pathways of interest. We also suggest the maximum flow/minimum cut algorithm as a way to calculate and compare the robustness of an argument. Third, we highlight transitivity (deducing C\u00e0E from C\u00e0D and D\u00e0E) as a key strength as well as a potential danger (the transitivity trap) within causal mapping. The risks of drawing spurious conclusions from incompletely specified data highlight the continuing importance of experienced evaluators who understand and relate findings to the context surrounding the data and refer back constantly to the original data. Causal mapping is a tool which can help to speed up the process and lift the lid on the black box of qualitative analysis, but it is no silver bullet; the findings must still be processed, digested and interpreted. Finally, we suggest that all theory based evaluation, insofar as it involves assembling (and drawing conclusions from) evidence about causal connections within a programme model, can be understood as a kind of causal mapping. Notes 1. An informal guide to our approach to causal mapping is available at: https://guide.causalmap.app/ 2. See https://www.causalmap.app/previousprojects and https://bathsdr.org/resources for example reports using causal mapping. 3. See Karl and Weick (2001: 311) for further discussion, which acknowledges some variation in how different authors distinguish between cognitive maps in general and causal maps in particular. 4. Our own approach (https://guide.causalmap.app/coding opposites.html) diverges from this, for reasons which are beyond the scope of this article. 5. Axelrod (1976) was particularly interested in using cognitive maps to evaluate whether a decision maker\u2019s causal theory was relevant and rational from their own perspective. To achieve this, he emphasised that \u2018the options, the goals, the ultimate utility, and the relevant intervening concepts should all be included in the cognitive map of a decision maker. Moreover, the evaluative theory of decision making that is employed should be sufficiently sophisticated as not to beg the important questions involved in reaching a complex policy decision\u2019 Axelrod (1976: 7). 6. We use the phrases \u2018influence factor\u2019 and \u2018consequence factor\u2019 rather than \u2018cause\u2019 and \u2018effect\u2019 when we want to avert the danger that a factor might be construed in a deterministic way as the cause or the effect of something else. 7. In practice, it is convenient to allow the analyst to code some non causal claims too, such as \u2018there was a good harvest this year\u2019. This amounts to creating a factor without a causal link and is especially useful to cover discussion of factors which may also appear elsewhere as part of causal claims (e.g. \u2018thanks to the new seeds there was a good harvest this year\u2019). 8. These risks can be mitigated by agreeing in advance on the forms of maps to be produced and by making the full causal database available for peer review. But being overly prescriptive in this also carries risks of adding to cost and length of reports. 9. The idea of evaluation as argument goes back at least to House (1977) and see also Gates and Dyson (2017). \u00b7 Steve Powell is a freelance evaluator and Director of Causal Map Ltd. He has been working as an evaluator for 25 years, mainly in international development. \u00b7 James Copestake is a Professor in the Department of Social & Policy Sciences with research interests in the political economy of development finance, and author of the Qualitative Impact Protocol. \u00b7 Fiona Remnant is Director of non profit evaluation consultancy Bath Social & Development Research and of software company Causal Map Ltd. Bath Social & Development Research specialises in qualitative social impact assessment using the Qualitative Impact Protocol. References Ackermann F and Alexander J (2016) Researching complex projects: Using causal mapping to take a systems perspective. International Journal of Project Management 34(6): 891\u2013901. Ackermann F and Eden C (2004) Using causal mapping: Individual and group; traditional and new. In: Pidd (ed.) Systems Modelling: Theory and Practice. Chichester: Wiley, pp. 127\u201345. Ackermann F and Eden C (2011) Using causal mapping to support information systems development. In: Armstrong DJ, Nakayama VK and Narayanan VK (eds) Causal Mapping for Research in Information Technology. Hershey, PA: IGI Global, pp. 263\u201383. Ackermann F, Howick S, Quigley J, et al. (2014) Systemic risk elicitation: Using causal maps to engage stakeholders and build a comprehensive view of risks. European Journal of Operational Research 238(1): 290\u20139. Ackermann F, Jones M, Sweeney M, et al. (1996) Decision explorer: User guide. Vol. Version 3. Axelrod R (1976) Structure of Decision: The Cognitive Maps of Political Elites. Princeton, NJ: Princeton University Press. Bana e Costa C, Ensslin L, Corn\u00eaa \u00c9, et al. (1999) Decision support systems in action: Integrated application in a multicriteria decision aid process. European Journal of Operational Research 113: 315\u201335. Barbrook Johnson P and Penn A (2021) Participatory systems mapping for complex energy policy evaluation. Evaluation 27(1): 57\u201379. Barbrook Johnson P and Penn AS (2022a) Introduction. In: Barbrook Johnson P and Penn AS (eds) Systems Mapping: How to Build and Use Causal Models of Systems. Cham: Springer, pp. 1\u2013 19. Barbrook Johnson P and Penn AS (2022b) Systems Mapping: How to Build and Use Causal Models of Systems. Cham: Springer. Available at: https://link.springer.com/10.1007/978 3 031 01919 7 (accessed 14 May 2023). Befani B and Stedman Bryce G (2017) Process tracing and Bayesian updating for impact evaluation. Evaluation 23(1): 42\u201360. Bennett A and Checkel JT (2015) Process Tracing. Cambridge: Cambridge University Press. Bollen KA (1987) Total, direct, and indirect effects in structural equation models. Sociological Methodology 17: 37\u201369. Bougon M, Weick K and Binkhorst D (1977) Cognition in organizations: An analysis of the Utrecht Jazz Orchestra. Administrative Science Quarterly 22: 606\u201339. Braun V and Clarke V (2006) Using thematic analysis in psychology. Qualitative Research in Psychology 3(2): 77\u2013101. Cartwright N (2020) Middle range theory: Without it what could anyone do? Theoria 35(3): 269\u2013323. Chaib Draa B and Desharnais J (1998) A relational model of cognitive maps. International Journal of Human Computer Studies 49(2): 181\u2013200. Clarkson GP and Hodgkinson GP (2005) Introducing CognizerTM: A comprehensive computer package for the elicitation and analysis of cause maps. Organizational Research Methods 8(3): 317\u201341. Copestake J (2014) Credible impact evaluation in complex contexts: Confirmatory and exploratory approaches. Evaluation 20(4): 412\u201327. Copestake J (2021) Case selection for robust generalisation: Lessons from QuIP impact evaluation studies. Development in Practice 31: 150\u2013160. Copestake J (2020) The Veil of ignorance Process Tracing (VPT) methodology. Qualitative & Multi Method Research 18(2): 19\u201325. Copestake J, Davies G and Remnant F (2019a) Generating credible evidence of social impact using the Qualitative Impact Protocol (QuIP): The challenge of positionality in data coding and analysis. In: Clift BC, Gore J, Bekker S, et al. (eds) Myths, Methods, and Messiness: Insights for Qualitative Research Analysis. Bath: University of Bath, p. 17. Copestake J, Morsink M and Remnant F (2019b) Attributing Development Impact. Rugby: Practical Action. Craven L (2020) Improving the health, wellbeing, and chronic disease management of the Arabic speaking community. Culture Well Project, Asthma Australia. Available at: https://asthma.org.au/wp content/uploads/2020/02/Culture Well Data Report Arabic Feb 2020 PRINT.pdf Craven LK (2017) System effects: A hybrid methodology for exploring the determinants of food in/ security. Annals of the American Association of Geographers 107(5): 1011\u201327. Dart J and Davies R (2003) A dialogical, story based evaluation tool: The most significant change technique. American Journal of Evaluation 24(2): 137\u201355. Davies RJ (2018) Representing theories of change: A technical challenge with evaluation consequences. Journal of Development Effectiveness 10(4): 438\u2013461. Eden C (1992) On the nature of cognitive maps. Journal of Management Studies 29(3): 261\u20135. Eden C, Ackermann F and Cropper S (1992) The analysis of cause maps. Journal of Management Studies 29(3): 309\u201324. Eden C, Jones S and Sims D (1979) Thinking in Organisations. London: Palgrave Macmillan. Fiol CM and Huff AS (1992) Maps for managers: Where are we? Where do we go from here? Journal of Management Studies 29(3): 267\u201385. Forrester JW (1971) Counterintuitive behavior of social systems. Theory and Decision 2: 109\u201340. Gates E and Dyson L (2017) Implications of the changing conversation about causality for evaluators. American Journal of Evaluation 38(1): 29\u201346. Goertz G and Mahoney J (2006) A Tale of Two Cultures: Qualitative and Quantitative Research in the Social Sciences. Princeton, NJ: Princeton University Press. Hayward J, Morton S, Johnstone M, et al. (2020) Tools and analytic techniques to synthesise community knowledge in CBPR using computer mediated participatory system modelling. npj Digital Medicine 3(1): 1\u20136. Hodgkinson GP, Maule AJ and Bown NJ (2004) Causal cognitive mapping in the organizational strategy field: A comparison of alternative elicitation procedures. Organizational Research Methods 7(1): 3\u201326. House ER (1977) The Logic of Evaluative Argument. Los Angeles, CA: Center for the Study of Evaluation, UCLA Graduate School of Education. Huff AS (1990) Mapping Strategic Thought. Hoboken, NJ: John Wiley & Sons. Jackson KM and Trochim WMK (2002) Concept mapping as an alternative approach for the analysis of open ended survey responses. Organizational Research Methods 5(4): 307\u201336. Karl E and Weick KE (2001) Making Sense of the Organization. Malden, MA: Blackwell Business. Kelly G (1955) Personal construct theory. In: Beneath the Mask; An Introduction to Theories of Personality. Hoboken, NJ: John Wiley & Sons. Khan MS and Quaddus M (2004) Group decision support using fuzzy cognitive maps for causal reasoning. Group Decision and Negotiation 13(5): 463\u201380. Klintwall L, Bellander M and Cervin M (2023) Perceived causal problem networks: Reliability, central problems, and clinical utility for depression. Assessment 30(1): 73\u201383. Kosko B (1986) Fuzzy cognitive maps. International Journal of Man Machine Studies 24(1): 65\u201375. Laukkanen M (1994) Comparative cause mapping of organizational cognitions. Organization Science (3): 322\u201343. Laukkanen M (2012) Comparative causal mapping and CMAP3 software in qualitative studies. Forum Qualitative Sozialforschung / Forum: Qualitative Social Research. Qualitative Research 13(2). DOI: 10.17169/fqs 13.2.1846. Laukkanen M and Eriksson P (2013) New designs and software for cognitive causal mapping. Qualitative Research in Organizations and Management: An International Journal 8(2): 122\u201347. Laukkanen M and Wang M (2016) Comparative Causal Mapping: The CMAP3 Method. London: Routledge. Markiczy L and Goldberg J (1995) A method for eliciting and comparing causal maps. Journal of Management 21(2): 305\u201333. Maxwell J (2004a) Causal explanation, qualitative research, and scientific inquiry in education. Educational Researcher 33(2): 3\u201311. Maxwell J (2004b) Using qualitative methods for causal explanation. Field Methods 16(3): 243\u201364. Mayne J (2001) Addressing attribution through contribution analysis: Using performance measures sensibly. The Canadian Journal of Program Evaluation 16(1): 1\u201324. Montibeller G, Belton V, Ackermann F, et al. (2008) Reasoning maps for decision aid: An integrated approach for problem structuring and multi criteria evaluation. Journal of the Operational Research Society 59(5): 575\u201389. Nadkarni S and Narayanan VK (2005) Validity of the structural properties of text based causal maps: An empirical assessment. Organizational Research Methods 8(1): 9\u201340. Narayanan VK (2005) Causal mapping: An historical overview. In: Armstrong DJ, Nakayama VK and Narayanan VK (eds) Causal Mapping for Research in Information Technology. Hershey, PA: IGI Global, pp. 1\u201319. OECD (2010) DAC quality standards for development evaluation. Available at: http://www.oecd.org/ development/evaluation/qualitystandardsfordevelopmentevaluation.htm (accessed 24 June 2023). Osgood C, Saporta S and Nunnally J (1956) Evaluative assertion analysis. Literatura 3: 47\u2013102. Pawson R and Tilley N (1997) Realistic Evaluation. Thousand Oaks, CA: Sage. Pearl J (2000) Causality: Models, Reasoning and Inference. Cambridge, MA: Cambridge University Press. Available at: http://journals.cambridge.org/production/action/cjoGetFulltext?fulltex tid=153246 (accessed 6 January 2015). Pearl J and Mackenzie D (2018) The Book of Why: The New Science of Cause and Effect. London: Hachette UK. Penn A and Barbrook Johnson P (2019) Participatory systems mapping: A practical guide. Available at: https://www.cecan.ac.uk/wp content/uploads/2020/09/PSM Workshop method.pdf Powell S (2018) The book of why the new science of cause and effect. Pearl, Judea, and Dana Mackenzie. 2018. Hachette UK. Journal of MultiDisciplinary Evaluation 13: 47\u201354. Powell S (2019) Theories of change: Making value explicit. Journal of Multidisciplinary Evaluation 15(32): 53\u20134. Powell S (2020) Federation wide meta evaluation of Nepal earthquake response. IFRC. Available at: https://www.ifrc.org/media/13554 Powell S, Larquemin A, Copestake J, et al. (2023) Does our theory match your theory? Theories of change and causal maps in Ghana. In: Simeone L, Drabble D, Morelli N, et al. (eds) Strategic Thinking, Design and the Theory of Change. A Framework for Designing Impactful and Transformational Social Interventions. Cheltenham: Edward Elgar Publishing, pp. 232\u201350. Rodrigues TC, Montibeller G, Oliveira MD, et al. (2017) Modelling multicriteria value interactions with reasoning maps. European Journal of Operational Research 258(3): 1054\u201371. Schmitt J (2020) The causal mechanism claim in evaluation: Does the prophecy fulfill? New Directions for Evaluation 2020(167): 11\u201326. Schwandt TA and Gates EF (2021) Evaluating and Valuing in Social Research. New York: Guilford Publications. Sedlacko M, Martinuzzi A, R\u00f8pke I, et al. (2014) Participatory systems mapping for sustainable consumption: Discussion of a method promoting systemic insights. Ecological Economics 106: 33\u201343. Taber R (1991) Knowledge processing with fuzzy cognitive maps. Expert Systems with Applications 2(1): 83\u20137. Tegarden D, Tegarden L, Smith W, et al. (2016) De fusing organizational power using anonymity and cognitive factions in a participative strategic planning setting. Group Decision and Negotiation 25(1): 1\u201329. Tolman EC (1948) Cognitive maps in rats and men. Psychological Review 55(4): 189. Weick KE (1995) Sensemaking in Organizations. Thousand Oaks, CA: Sage. Wilkinson H, Hills D, Penn A, et al. (2021) Building a system based theory of change using participatory systems mapping. Evaluation 27(1): 80\u2013101. Williams B (2022) System diagrams: A practical guide. Available at: https://bobwilliams.gumroad. com/l/systemdiagrams (accessed 13 September 2022). Wilson Grau R and Britt H (2012) Outcome Harvesting. Cairo: Ford Foundation. Wrightson MT (1976) The documentary coding method. In: Axelrod R (ed.) Structure of Decision: The Cognitive Maps of Political Elites. Princeton, NJ: Princeton University Press, pp. 291\u2013332."}, {"title": "eval2025", "path": "/000 reference articles/eval2025.html", "text": "An AI powered workflow for collecting and understanding stories at scale Abstract This paper presents an AI assisted causal mapping pipeline for gathering and analysing stakeholder perspectives at scale. Evidence relevant to constructing a program theory, as well as evidence for the causal influences flowing through it, are both collected at the same time, without the evaluator needing to possess a prior theory. The method uses an AI interviewer to conduct interviews, automated coding to identify causal claims in the transcripts, and causal mapping to synthesise and visualise results. The authors tested this approach by interviewing participants about problems facing the USA. Results indicate the method can efficiently collect and process qualitative data, producing useful causal maps that capture respondents' views. The paper discusses the potential of this approach for evaluation, enabling rapid, large scale qualitative analysis. It also notes limitations and ethical concerns, emphasising the need for human oversight and verification. Motivation and background To evaluate a program, the evaluator can use Contribution Analysis (CA) (Mayne, 2012, 2015). We start with a program logic or Theory of Change (ToC), consisting of possible pathways from interventions to outcomes, and collect existing or new evidence for each link. However evaluators can often not assume that the ToC underpinning a program aligns with the realities on the ground, or they may uncover outcomes not anticipated in the original program design see Koleros & Mayne (2019). We have argued (Powell, Copestake, et al., 2023, p. 114) for a generalisation of CA in which evidence relevant to constructing a program theory, as well as evidence for the causal influences flowing through it, are both collected at the same time, without the evaluator (necessarily) having a prior theory. In this sense, following Mayne, \u201cprogram theory\u201d need not be something that any person necessarily possessed or articulated at the time, but is something which can be approximated and improved during the evaluation process. (Re )constructing program theory empirically in this way is an essentially open ended, qualitative problem. Closed data collection methods are not suitable because we cannot measure what we do not yet know. Open ended, qualitative methods to (re )construct a theory are notoriously time consuming and are usually heavily influenced by researcher positionality (Copestake et al., 2019). Powell, Copestake, et al (2023, p. 108) present this task as gathering and synthesising evidence about \"what influenced what\", evidence which is simultaneously about theory or structure and contribution. Each piece of evidence may be of differing quality and reliability and about different sections of a longer pathway, or multiple interlocking pathways, and may come from different sources who see and value different things. Causal mapping as part of the solution One way to meet the challenge of simultaneously assessing theory and contribution is by using causal mapping (Ackermann and Eden, 2004; Axelrod, 1976; Eden et al., 1992; Hodgkinson and Clarkson, n.d.; Laukkanen and Wang, 2016; Powell, Copestake, et al., 2023) the collection, coding and visualisation of interconnected causal claims. Causal mapping can be seen as unifying visual and text based approaches and involves identifying causal claims within a set of texts. Each piece of text which contains information that a source S claims that X causally influences Y is coded: represented as a directed link or arrow in a network, connecting node X to node Y. Along with each link, the information about the origin of this claim, the source S, is also noted. Causal mapping provides a structured approach of synthesising (some of) the meaning of large quantities of text, from interviews or documents, providing a relatively generic way to extract and uncover emerging meaning. We can see causal mapping as a form of qualitative data analysis in which the fundamental act of coding requires creating not a single tag or theme but an ordered pair of tags, the cause and the effect. This evidence may A) be collected systematically from relatively homogenous sources, as in QuIP (Copestake et al., 2019), or B) be a mix, consisting, for example, of some systematic interviews, some official data, and some reports from a monitoring system. In this article, we focus only on Option A. If we broaden our view beyond any specific results chain, causal mapping can also be useful to evaluators who need to know how groups of people see the world, what these views of the world have in common and what distinguishes them, perhaps to make informed recommendations for program improvement (Copestake et al., 2019). We sometimes use the phrase \u201ccausal landscapes\u201d to describe the object of causal maps, to emphasise that we are most interested in the contents and layout of the broader world in which stakeholders understand themselves to be living. We believe that causal mapping should carefully distinguish between evidence for a causal link and the causal link itself. While causal mapping can help the evaluator to identify, code and synthesise the evidence for causal connections, the evaluative judgement about whether one thing causally influences another is left to the evaluator (Causal mapping | Better Evaluation, 2024). Constructing causal maps by gathering and coding individual interview transcripts (rather than by consensus in groups (Ackermann and Eden, 2004; Barbrook Johnson and Penn, 2022) can be a challenge for evaluators due to the complexity of the task and the resource intensive and researcher dependent nature of human led coding. The use of AI driven tools is rapidly increasing among evaluators (Bohni Nielsen et al., 2024). AI can facilitate tasks by, for example, enabling virtual platforms to engage hard to reach populations or augment human capabilities by processing large datasets, auto coding qualitative inputs, or optimising data analysis (Eloundou et al., 2023). Using causal mapping to gather evidence about structure/theory and contribution simultaneously Our suggestion comprises the following steps (following Tasks 1 3 according to Powell, Copestake, et al. (2023, p. 108 112): 1. Gathering data by interviewing stakeholders about key issues of mutual interest (for example, outcomes) and asking what drives these issues, and how they are interrelated with the drivers. For example, we can ask about outcomes and causes of outcomes and causes of causes. (We use the term \u201ccausal\u201d here in the loosest sense: we make causal connections every day using ordinary language when we say that one thing contributes to or drives or influences another, or makes, or might make, something else happen.) For applications using Option B (above), this step will look somewhat different, but we will not cover Option B here. 2. Code causal claims; we can then use causal mapping rules to identify causal claims within transcripts of these interviews. Each claim is a link between one cause or \u201cinfluence\u201d factor and one effect or \u201cconsequence\u201d factor. This will result in many individual causal maps, one per source/stakeholder. 3. Synthesise the individual causal maps into a causal network, showing common and diverging views, and then query the network to answer evaluation questions. Scaling the approach Gathering and processing sufficient data for this kind of approach has been a time consuming task. Recent advances in AI raise the question: can we use AI to gather and process this kind of information at scale? Previous work on each step We are not aware of any prior work on our entire workflow, i.e. using AI interviewers to gather data, construct causal maps and synthesise these to help answer evaluation questions. Here we will refer to prior work on the three individual steps. Geiecke and Jaravel (2024) provide an end to end workflow with a single, general large language model (LLM) interviewer similar to ours and a suggestion for automated analysis, but their analysis approach is a global one involving the identification of themes across all transcripts and then identifying whether each theme is present in each transcript. Step 1: Using an AI interviewer to gather causal information at scale Data gathering with AI powered chatbots has become popular in various businesses and industries. Chatbots have been used to gather customer data, feedback and to provide automated responses to frequently asked questions (Yuen, 2022) and in evaluation for tasks like gathering feedback (Bruce et al., 2024; Nielsen, 2023) or conducting \u201claddering\u201d interviews to investigate respondents\u2019 values (Rietz and Maedche, 2022). Chopra and Haaland (2023) report a recent use case in social research, suggesting that chat automations can be a valid and effective way to gather open ended information from respondents. However, their approach involves a sophisticated set up with multiple bots which are hard coded into a team to monitor and steer the progress of the interview. Our experience has found hard coding can be effective but difficult to adjust for different kinds of interviews. In this study, we leave the control of the interview, as well as developing and delivering the responses and new questions, all to a single automated interviewer. Geiecke and Jaravel (2024) present an approach similar to our own, using a single LLM agent to conduct useful social research interviews. The term \u201cchatbot\u201d for this kind of broader use of genAI to conduct natural seeming conversations seems no longer appropriate, and we prefer the term \u201cAI interviewer\u201d. In the world of machine learning, a clear distinction can be made between supervised and unsupervised approaches (Ziulu et al., 2024). Using genAI to conduct interviews and code texts blurs this boundary. In our case, we developed our semi generic instructions for interviewing, giving the AI instructions on how to behave, and how to make follow up questions based on the interview objectives. Once the data collection is done, we create a separate genAI prompt to code causal links as a trial and error process, monitoring the quality of the coding post hoc. We did not have an explicitly stated ground truth about exactly how the interview should look or which causal claims were \u201creally\u201d present within each text passage or how their causes and effects should be labelled, as we believe neither of these questions have a definitive answer; rather, we monitored AI\u2019s responses coding post hoc, iterating the prompt over many cycles to improve its performance. \"Prompt engineering\" (Ferretti, 2023) like this can be considered a kind of supervision because it steers the AI\u2019s responses in a desired way. Once the prompt was finalised, the interview AI was left to conduct interviews without further supervision. This prompt can remain broadly the same across different studies. However, the response of the AI can be highly sensitive to small differences in the \"prompt\" and other settings (Jang and Lukasiewicz, 2023). Small adjustments made for specific studies, such as adjusting the instructions to focus better on research objectives, remain a vital point of human intervention. Step 2: using automated causal mapping to code causal information at scale Making sense of texts by assigning codes or topics to text sections (or even entire documents) can be called thematic analysis (Braun et al., 2020; Braun and Clarke, 2006) or Qualitative Data Analysis (QDA) (Lacey and Luff, 2001). Approaches to automating this process have moved on from topic modelling (Cintron and Montrosse Moorhead, 2021) based on counting and clustering the words in the texts (Blei et al., 2003) to sentiment analysis (Roy and Rambo Hernandez, 2021) and procedures which use LLMs to capture the meaning of longer sections of text (Sia et al., 2020; Ziulu et al., 2024). Our task is more specific: identifying not just general meanings but specifically causal relationships. Coding texts by hand for causal mapping has until recently been time consuming work for trained analysts. Thus it has remained a relatively niche approach. The limited sample size means it is difficult to make generalisations or comparisons between subgroups or across timepoints, reducing its utility for program monitoring. Earlier language models (Devlin et al., 2019) and other machine based techniques have been used to identify causal relationships expressed in text (Dunietz, 2018; Dunietz et al., 2017; Rory Hooper et al., 2023; Jiang et al., 2023) for an overview see Yang et al. (2022). However, these were highly specialised procedures which required \"training\" the models. The advent of large language models and genAI makes this process much easier because we can rely on the model\u2019s inherent understanding of causality and directly ask the model to \u201cidentify causal claims\u201d rather than having to define exactly what this means. The task of identifying causal claims is simpler than the broader problem of identifying themes in general, as with thematic analysis, because the task \u201cidentify all the causal claims in this text\u201d is more specific than \u201cidentify all the themes in this text\u201d. The latter approach would require a pre reading of the text to know what kind of themes we might want to look for, which would leave the AI with too much freedom to make judgments for us and would also be more likely to expose any underlying model biases. Evaluators (Davies, 2023; Ferretti, 2023) have recently been demonstrating the possibilities of AI in evaluation, for example with asking AIs for summaries or global syntheses of texts including interviews (see Wachinger et al., 2024 and Mason and Montrosse\u2010Moorhead, 2023). It is even possible to ask an AI to synthesise the main causal links within a text and produce a diagram directly (Graham, 2023). However, when doing this, evaluators have to take care not to leave fundamental evaluative decisions such as \u201cwithin this text, what are the most important claims\u201d to the LLM. Evaluators should be wary of transferring the responsibility for making evaluative judgements to an unknown third party, the \u201cblack box\u201d of the AI (Choudhary et al., 2022). One way to navigate this complexity is to systematically break down larger, weakly specified tasks into multiple, smaller, better specified tasks and also to clearly distinguish where AI adds value and where human insight is needed. In this sense, we prefer approaches which use the power of genAI more transparently, as a low level coding assistant in the tradition of QDA or thematic analysis who follows detailed instructions, leaving the evaluator with the responsibility of making evaluative judgements. This involves establishing coding guidelines designed to extract causal information from the documents with little guidance. Jalali and Akhavan (2024) use one such approach. They first instruct the AI to construct a codebook (a list of salient causal factors) based on the whole text and then use this codebook to identify and code links. While they report good results, we see this as still leaving too much freedom for the AI to make its own global \u201cjudgement\u201d based on processing the whole text about what are the salient factors and therefore also being more exposed to bias inherent in the AI\u2019s \u201cworldview\u201d. There are different ways to create factor labels for the influence and consequence factors making up of each causal link. They may be: specified in advance deductively in the form of a codebook based on pre existing theory; based on a preliminary assessment of the text to be coded, as with Jalali and Akhavan; according to a codebook which is developed iteratively during coding (Laukkanen, 1994) and QuIP (Copestake et al., 2019); created \u201cIn vivo\u201d in whatever form is most suited to each coding. This means that there will be many different labels for the causes and effects, many of which are likely to overlap in meaning. We call this approach to causal coding \u201cradical zero shot\u201d: no codebook or examples are provided, and there is no mechanism to ensure consistent labels across the dataset. This coding procedure maximises granularity but means that to build synthesis maps, it will be necessary to subsequently, retrospectively, cluster labels into sets with similar meaning and provide appropriate labels for the clusters. This is the approach we prefer and use in this paper. As with creating the instruction (\u201cprompt\u201d) for the AI interviewer, developing an instruction to identify causal links was a trial and error process, involving monitoring the quality of the coding post hoc, making adjustments post hoc as we analysed the quality of the coding and identified errors or gaps. Step 3: using automated causal mapping to help answer evaluation questions The extensive causal mapping literature provides many examples of its use to answer evaluation questions (see Powell, Copestake, et al., 2023, p. 110), for example: Getting an overview of respondents\u2019 \"causal landscape\". This can be useful for orientation or for particular tasks like triaging masses of information to identify key outcomes and possible causal pathways when planning an Outcome Harvesting (Wilson Grau and Britt, 2012) or Process Tracing (Befani and Stedman Bryce, 2017) project. Weighing up evidence about contribution: in particular, tracing back and comparing the possibly multiple contributory causes of an important outcome or consequence (Goertz and Mahoney, 2006), or examining effects of causes. Reporting key metrics of the causal network, for example, to reveal which factors are most central in the whole network or to identify feedback loops. Asking whether the empirical ToC matches the plan (Powell, Larquemin, et al., 2023, p. 7). Making comparisons between groups or across timepoints. We have integrated the research questions into the method steps below and set out more detailed criteria for evaluating the research questions within the results section. Method: the \u201cAI assisted causal mapping pipeline\u201d We present a procedure which harnesses the power of AI to facilitate and augment evaluation practice (Eloundou et al., 2023) in three ways: firstly to carry out large numbers of automated, qualitative, online interviews, secondly to automatically code the transcripts and thirdly, to present overview causal maps. Step 1: Conducting the chat interviews This paper presents results from a proof of concept analogue study. We employed online workers as respondents, recruited via Amazon\u2019s MTurk platform (Shank, 2016). We decided to investigate respondents\u2019 ideas about problems facing the USA, as this generic theme was likely to elicit opinions from randomly chosen participants. This unsophisticated way of recruiting respondents means that the results cannot be generalised to a wider population in this case. We had no specific evaluative questions in mind; We aimed to demonstrate a method which can be easily adapted to a specific research question. A short semi structured interview guideline was designed on the theme of \"What are the important current problems facing the USA and what are the (immediate and underlying) reasons for those problems?\". We aimed to construct an overall collective \u201cToC\u201d around problems in the USA. As it does not encompass a specific intervention this theory is not an example of a program theory. This interview guideline was implemented via an online interview \"AI interviewer\" called \"Qualia\", which uses the OpenAI Application Programming Interface (API) to control the AI\u2019s behaviour. Qualia is designed to elicit stories from multiple individual respondents, in an AI driven chat format. Individual respondents are sent a link to an interview on a specific topic and, after consenting, are greeted by the interviewer. Rather than following a set list of questions, the interviewer is instructed to adapt its responses and follow up questions depending on the respondents' answers, circling back to link responses and asking for more information as appropriate, focusing on the interview's objective mentioned above. These behaviours are based on the instructions written by the authors. The respondents, who had the level of \u201cMaster\u201d on Amazon's MTurk service, each completed an interview. The Amazon workers were given up to 19 minutes to complete the interview. We repeated this interview at three different timepoints in September, October and November 2023, inviting approximately N=50 respondents each time. The data from the three timepoints was pooled. The Research Question for Step 1 is: can an automated interview bot successfully gather causal information at scale? Step 2: Coding the interviews Step 2a: Constructing a guideline Once the interviews were completed, we wrote instructions to guide the qualitative causal coding of the transcripts, in a radical zero shot style: without giving a codebook or any examples. The assistant was told not to give a summary or overview but to list each and every causal link or chain of causal links and to ignore hypothetical connections (for example, \u201cif we had X we would get Z\u201d). We told the AI to produce codes or labels following this template: 'general concept; specific concept'. We gave no examples, but expected the AI to produce labels like: \u201ceconomic stress; no money to pay bills\u201d. We call the combination of both parts a (factor) label. The assistant was told also to provide a corresponding verbatim quote for each causal chain, to ensure that every claim could be verified. Codings without a quote which matched the original text were subsequently rejected, thus reducing the potential for \u201challucination\u201d. Step 2b: Coding The final instructions were human readable and could have been given to a human assistant. Instead, we gave these instructions to the online app \"Causal Map\", which used the GPT 4 OpenAI API. As the transcripts were quite long (each around a page of A4 in length), each was submitted separately. The \u201ctemperature\u201d (the amount of \u201ccreativity\u201d) was set to zero to improve reproducibility. The Causal Map app managed the housekeeping of keeping track of combining the instructions with the transcripts, watching out for any failed requests and repeating them, saving the causal links identified by the AI, etc. Step 2c: Clustering The coding procedure resulted in many different labels for the causes and effects, many of which overlap in meaning. Even the general concepts (e.g. \u201ceconomic stress\u201d) were quite varied. The procedure for clustering these labels (including both the general and specific parts of the label) into common groups with their labels was a three step process based on assigning to each of the original labels an embedding. An embedding is a numerical encoding of the meaning of each label (Chen et al., 2023) in the form of a point in a space, such that two labels with similar meaning are close in this space. For any two such vectors, a measure cosine similarity can be calculated representing the approximate similarity in meaning between the labels which they encode: 1. Inductive clustering . First, we grouped the labels into clusters of similar labels using the hclust() function from the stats package of base R (R Core Team, 2015). 2. Labelling. We then asked an AI to find distinct labels for each cluster. We also manually inspected these labels with regard to the original labels within each cluster and adjusted some of them. 3. Deductive clustering. We then discarded the original clustering, created embeddings for the new labels, and formed a new set of clusters, one for each of the new labels, assigning each original label to one of the new labels, the one to which it was most similar, providing the similarity was at least higher than a given threshold. This additional deductive step ensures that each member of each new cluster is sufficiently close in meaning to the new cluster label, rather than just to the other members of the cluster. After each sub step, we checked the AI\u2019s results to ensure that the instructions were being followed correctly and, if they weren't, the instructions were tweaked or rewritten and tested again to ensure quality and consistency. The Research Question for Step 2 is: can automated causal mapping successfully code causal information at scale? Step 3: Making useful syntheses of causal mapping data to answer evaluation questions Standard filters (details on request) can be applied to the resulting dataset of causal claims to create overview causal maps as a qualitative summary of the respondents' \"causal landscapes\". The primary aim is to construct a simple map with a not overwhelming number of links and factors which captures a large percentage of the information given by the respondents. In addition, network metrics like centrality can be used to identify the factors which are most central within the network. To weigh up the evidence for the contributions made to a specific factor, we can list the evidence (the specific quotes from specific respondents) for direct and indirect links leading to it. The Research Question for Step 3 is: can automated causal mapping help answer evaluation questions? Results The sub headings within each question form our criteria for answering that question. Question 1: can an AI interviewer gather causal information at scale? Efficiency As we were still experimenting with the process, it took us around 8 hours to write, test, deploy and monitor the interviews. We spent around $40 on API fees, including both tests and real interviews. The time and cost involved were significantly less than what it would have taken for humans to create an interview guideline and interview the same number of participants. Validity This is a difficult question to answer fully. However, in the interview prompt, we instruct the AI to summarise the conversation at the end of the interview and ask the respondent to verify its accuracy. We can use these answers to make a rough assessment of how valid the original summaries were: if the interviewee expresses no dissatisfaction, we can assume that the interviewer successfully elicited valid information. The final section of all 163 interviews was analysed. We classified each interview into 3 groups: 1. No summary provided; 2. The respondent explicitly expressed dissatisfaction and/or asked for changes in the summary; 3. The respondent finished the interview and did not explicitly express dissatisfaction nor ask for changes in the summary. 78.5% of the respondents ( group 3 ) didn\u2019t ask for changes in the summary, implying at least no dissatisfaction with what the AI produced (128 out of 163 interviews). Only in 7 interviews (4.29%) did the interviewee ask the AI to change or correct something in the summary, and/or the respondent explicitly expressed dissatisfaction ( group 2 ). Of these, three then explicitly expressed satisfaction with the revised summary offered by the AI. The other 25 interviews (15.3%) were not summarised ( group 1 ), mainly due to the participants breaking off before the end of the interview. We used a much simpler architecture to manage the interview process than Chopra and Haaland (2023), however, our interviews were much shorter than theirs (their average interview length was about 30 minutes), raising the question of whether longer interviews might need more elaborate management architecture. Question 2: Can automated causal mapping code causal information at scale? Efficiency It took around 5 hours to write and test the coding instructions and validate the results. The cost of using the API was around $20. Recall Recall can be defined as the extent to which the AI finds \u201call\u201d the causal links (Resnik and Lin, 2010). We made a separate assessment of the number of links \u201creally\u201d present within each interview, a \u201cground truth\u201d of 1154 links. In comparison, the automated coding identified 1024 links, or 89%. However this is before assessing which of those codings were correct: the precision of the links, as follows. Precision Precision can be defined as the proportion of the identified links which were accurate/correct (Resnik and Lin, 2010). To define \u201ccorrect\u201d we used the following informal criteria, which were assessed for each link by the second author: 1. The cause and effect in each link correctly name phenomena which are named in the text; 2. The coding represents an actual causal claim within the text (rather than, for example, merely events listed in sequence); 3. The coding represents a factual claim rather than a wish or hypothetical statement. 4. The coding is in the correct direction (cause to effect). We gave each causal link a 0 2 score on the four criteria of precision as detailed in the Supplementary Material. 65% of the links had a perfect score, and 72% dropped only one point (a \u201cnot sure\u201d on only one criterion). The errors we identified seem to take place approximately at random, except that there were more errors with causal claims which human analysts themselves judged to be difficult to code. A more systematic assessment of the coding process on a real life dataset had similar results and is currently in press (redacted). Question 3: can automated causal mapping help answer evaluation questions? Can an overall causal map be generated which includes much of the information? The map in Figure 1 is filtered to show only the top 11 factors (in terms of the number of respondents mentioning them); links mentioned by only one source are also removed, meaning many less frequently mentioned factors and links are not shown. We introduce a measure which we call coding coverage : given any map based on any recoding or filtering of the original data, what percentage of the original codings are included? There are balances to be struck: a map with more factors will usually have higher coverage but will be harder to understand and less useful. More homogeneity in sample and theme usually mean higher coverage. Very granular clustering will mean lower coverage or a larger map. The first result can be seen in Figure 1. This map contains only 11 factors but covers 42% of the raw causal claims. Insert Figure 1 around here. Most (113 out of 136) sources have contributed at least some citations to this summary map. The numbers on the factors and links (and the sizes of the factors and the widths of the links) represent the number of sources mentioning each. Factors with darker backgrounds have proportionately more incoming than outgoing links: they have greater \u201coutcome ness\u201d. At this coarse level of \u201cgranularity\u201d, many of the factors are bundles of cause effect stories, as shown by the \u201cself loops\u201d such as the 10 sources that mentioned links between different environment/climate change issues. In this map, it is mostly not possible to distinguish between constituent factors with different valence or sentiment. For example, \u201cmilitary strengthening\u201d and \u201cmilitary weakening\u201d are two codes which have been included under \u201cInternational conflict\u201d. Indeed they are not so far from one another in the overall space of embeddings, something which is quite hard to understand from a positivistic, Cartesian point of view but which is perhaps more familiar to those more used to thinking in terms of \"themes\" than in terms of \u201cvariables\u201d. Face validity Does the overall causal map present a plausible picture of the most important factors and how they influence one another (in the opinion of respondents)? Yes, even in the absence of a particular research focus, this causal map has a lot to tell us about the causal worlds of the respondents. \u201cIt\u2019s the economy, stupid\u201d: economic stress is mentioned by the largest number of sources and is central to most of the narratives. Covid 19 appears as a pure driver of economic stress. Ability to answer other evaluation questions Regarding the differences between timepoints, there were significant differences for several of the links. For example, of the five sources that mentioned the link from Political conflict to International conflict overall, all of them were from the third time point, which is unsurprising considering the situation in Israel/Palestine at that timepoint. In this analogue study, we did not have any additional information e.g. about the sociodemographic characteristics of the respondents which would have enabled us to look at differences between subgroups. In a more realistic evaluation context, it would be possible to further investigate narratives about the causes and effects of specific factors of interest. Discussion Question for Step 1 can an AI interviewer successfully gather causal information at scale? : Our AI interviewer was able to conduct multiple interviews with no researcher intervention at a low cost, reproducing the results of Chopra and Haaland (2023) and Geiecke and Jaravel (2024). The interview transcripts read quite naturally and the process seems to have been acceptable to the interviewees. Question for Step 2 can automated causal mapping successfully code causal information? : Automated coding was able to identify causal claims made by respondents. The coding was noisy, with 35% dropping at least one quality point, but with no evidence of systematic errors . This level of precision is adequate for sketching out \u201ccausal landscapes\u201d but would not be for high stakes evaluations without additional manual correction. The accuracy can also be substantially improved by getting the AI to revise its work, (see redacted). This procedure still involves the researchers making significant high level decisions in the formulation of the coding instructions as well as, before analysis, in clustering similar factor labels into groups. We believe this coding approach using genAI represents a significant improvement over the more hard coded approaches for identifying causal relationships expressed in text (Dunietz, 2018; Dunietz et al., 2017; Rory Hooper et al., 2023; Jiang et al., 2023; Yang et al., 2022), and provides a more detailed, section by section coding which relies less on using AI as a black box to identify themes for initial coding (Jalali and Akhavan, 2024) or to identify a global map (Graham, 2023). Question for Step 3 can automated causal mapping help answer evaluation questions? : An overview map was produced which included over 40% of the causal claims identified within the transcripts, using just 11 relatively broad factor labels. The most central factor with the highest number of citations was Economic stress, which is a plausible result, with plausible connections to other factors. We can use the map to identify and weigh up the evidence for contributions from and to individual factors. For example, the major contributions to Economic stress are Government policy and Covid 19, as well as \u201cself loops\u201d mentioned by 46 sources, i.e. where one aspect of Economic stress was seen as causing another. All such results depend on the (not automated) decisions made during the clustering process: how many clusters to use, whether to intervene in labelling, etc. This situation is closely parallel to decisions facing a statistician who has to identify variables for, say, structural equation modelling (Goertz, 2020: 136 ff). Comparison of citation frequency across timepoints was able to show that some links were mentioned significantly more than others, illustrating how this kind of map could be used to explore changes in systems (or in mental models of systems) over time. Caveats Ethics, bias and validity This kind of AI processing is not suitable for dealing with sensitive data because information from the interviews passes to OpenAI\u2019s servers, even though it is no longer used for training models (OpenAI, 2024). Head et al. (2023) and Reid (2023) raise concerns about bias and the importance of equity in AI applications for evaluation, which have led to questions about the validity of AI generated findings (Azzam, 2023). The way the AI sees the world, the salient features it identifies, the words it uses to identify them, and its understanding of causation are certainly wrapped up in a hegemonic worldview (Bender et al., 2021). Those groups most likely to be disadvantaged by this worldview are approximately the same who have least say in how these technologies are developed and employed. AI is developing quickly: new models and techniques become available every month. However, we believe that any tools which genuinely add to knowledge should use procedures which are broken down into workflows consisting of simple individual steps so that humans can understand and check what is happening. Interviewing Researchers should carefully consider whether the interview subject matter is compatible with this kind of approach. For example, the AI may miss subtle cues or struggle to provide appropriate support to respondents expressing distress (Chopra and Haaland, 2023; Ray, 2023). We recommend that interview guidelines are tested and refined by human interviewers before being automated. No automated interview can substitute for the contextual information which a human evaluator can gain by talking directly to a respondent, ideally face to face and in a relevant context. There is likely to be a differential response rate in this kind of interview: some people are less likely to respond to an AI driven interview than others, and this propensity may not be random. Causal mapping Causal mapping is not at all suited for estimating the strength of causal effects: it can reveal the strength of the evidence for the influence of X on Y but this is not to be confused with the strength of the effect itself. There can be strong evidence for a weak link and vice versa. Autocoding The work of the AI coder and clustering algorithms are not error free. The coding of individual high stakes causal links should be checked. In particular, there is a danger of accepting inaccurate results which look plausible. This approach does not nurture substantive, large scale theory building of the kind expected, for example in grounded theory (Glaser and Strauss, 1967). However, it can do smaller scale theory building in the sense of capturing theories implicit in individuals\u2019 responses. This pipeline relieves researchers of much of the work involved in coding but it is not fully autonomous. The human evaluator is responsible for applying the techniques in a trustworthy way and for drawing valid conclusions. Potential Qualitative approach: These procedures approach the stakeholder stories as far as possible without preconceived templates, to remain open to emerging and unexpected changes in respondents\u2019 causal landscapes. Scalability and reach: The AI\u2019s ability to communicate in many languages presents an opportunity to reach more places and people, subject to internet access and the AI\u2019s fluency in less common languages, and to include representative samples of populations. The interview and coding processes are machine driven and use zero temperature, so this approach should be mostly reproducible. Reproducibility opens the possibility of comparing results across groups, places and timepoints. The low cost of coding large amounts of information means that it is much easier to develop, compare and discard hypotheses and coding approaches, something which qualitative researchers have previously been understandably reluctant to do. Qualitative causality: These procedures have the potential to help evaluators answer evaluation questions which are often causal in nature, like: understanding stakeholders' mental models; judging whether \"their\" ToC matches \"ours\"; investigating \u201chow things work\u201d for different subgroups of stakeholders; tracing impact from mentions of \"our\" intervention to outcomes of interest; triaging the key outcomes in stakeholders\u2019 perspectives. In summary, this kind of semi automated pipeline opens up possibilities for monitoring, evaluation and social research which were unimaginable just three years ago and are well suited to today\u2019s challenging, complex problems like climate change and political and social polarisation. Previously, only quantitative research claimed to produce generalisable knowledge about social phenomena validly and at scale, by turning meaning into numbers. Now perhaps qualitative research will eclipse quantitative research by bypassing quantification and dealing with meaning directly, in somewhat generalisable ways. Further work We have tried to demonstrate a semi automated workflow with which evaluators can capture stakeholders\u2019 emergent views of the structure of a problem or program at the same time as capturing their beliefs about the contributions made to factors of interest by other factors. We have presented this approach via a proxy application but have since applied it in real life research. Many challenges remain, from improving the behaviour of the automated interviewer through improving the accuracy of the causal coding process to dealing better with valence (for example distinguishing between \u201cemployment\u201d, \u201cemployment issues\u201d and \u201cunemployment\u201d). Perhaps most urgently needed are ways to better understand and counter how LLMs may reproduce hegemonic worldviews (Head et al., 2023; Reid, 2023). References Ackermann F and Eden C (2004) Using Causal mapping: individual and group; traditional and new. Systems modelling: Theory and practice . Wiley, Chichester: 127\u2013145. Amazon Inc. (2016) Simplified Masters Qualifications. Available at: https://blog.mturk.com/simplified masters qualifications 137d77647d1c (accessed 4 December 2023). Axelrod R (1976) The Analysis of Cognitive Maps. In: Structure of Decision : The Cognitive Maps of Political Elites . Azzam T (2023) Artificial intelligence and validity. New Directions for Evaluation 2023(178\u2013179): 85\u201395. Barbrook Johnson P and Penn AS (2022) Systems Mapping: How to Build and Use Causal Models of Systems . Cham: Springer International Publishing. Available at: https://link.springer.com/10.1007/978 3 031 01919 7 (accessed 14 May 2023). Befani B and Stedman Bryce G (2017) Process Tracing and Bayesian Updating for impact evaluation. Evaluation 23(1): 42\u201360. Bender EM, Gebru T, McMillan Major A, et al. (2021) On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c. In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency , Virtual Event Canada, 3 March 2021, pp. 610\u2013623. ACM. Available at: https://dl.acm.org/doi/10.1145/3442188.3445922 (accessed 14 November 2023). Blei DM, Ng AY and Jordan MI (2003) Latent dirichlet allocation. Journal of machine Learning research 3(Jan): 993\u20131022. Bohni Nielsen S, Mazzeo rinaldi F and Petersson G (2024) Evaluation in the Era of Artificial Intelligence., pp. 1\u201312. Braun V and Clarke V (2006) Using thematic analysis in psychology. Qual. Res. Psychol. 3(2): 77\u2013101. Braun V, Clarke V, Boulton E, et al. (2020) The online survey as a qualitative research tool. International Journal of Social Research Methodology 00(00). Routledge: 1\u201314. Bruce K, Gandhi VJ and Vandelanotte J (2024) Emerging Technology and Evaluation in International Development. In: Artificial Intelligence and Evaluation . Routledge. Causal mapping | Better Evaluation (2024). Available at: https://www.betterevaluation.org/methods approaches/methods/causal mapping (accessed 3 August 2024). Chen S, Zhang H, Chen T, et al. (2023) Sub Sentence Encoder: Contrastive Learning of Propositional Semantic Representations. arXiv:2311.04335. arXiv. Available at: http://arxiv.org/abs/2311.04335 (accessed 14 November 2023). Chopra F and Haaland I (2023) Conducting Qualitative Interviews with AI.: 72. Choudhary S, Chatterjee N and Saha SK (2022) Interpretation of Black Box NLP Models: A Survey. arXiv preprint arXiv:2203.17081. Epub ahead of print 31 March 2022. Cintron D and Montrosse Moorhead B (2021) Integrating Big Data Into Evaluation: R Code for Topic Identification and Modeling. American Journal of Evaluation 43: 109821402110316. Copeland AH (1951) A reasonable social welfare function . mimeo, 1951. University of Michigan. Copestake J, DAVIES G and REMNANT F (2019) Generating credible evidence of social impact using the Qualitative Impact Protocol (QuIP): The challenge of positionality in data coding and analysis. Myths, Methods, and Messiness: Insights for Qualitative Research Analysis : 17. Davies R (2023) Evaluating thematic coding and text summarisation work done by artificial intelligence (LLM). In: Rick On the Road . Available at: http://mandenews.blogspot.com/2023/08/evaluating thematic coding and text.html (accessed 13 October 2023). Devlin J, Chang M W, Lee K, et al. (2019) BERT: Pre training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805. arXiv. Available at: http://arxiv.org/abs/1810.04805 (accessed 2 December 2023). Dunietz J (2018) Annotating and Automatically Tagging Constructions of Causal Language . PhD Thesis. Brandeis. Dunietz J, Levin L and Carbonell J (2017) The BECauSE Corpus 2.0: Annotating Causality and Overlapping Relations. In: Proceedings of the 11th Linguistic Annotation Workshop , Valencia, Spain, 2017, pp. 95\u2013104. Association for Computational Linguistics. Available at: http://aclweb.org/anthology/W17 0812 (accessed 14 November 2023). Eden C, Ackermann F and Cropper S (1992) The Analysis of Cause Maps. Journal of Management Studies 29(3): 309\u2013324. Eloundou T, Manning S, Mishkin P, et al. (2023) GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. arXiv:2303.10130. arXiv. Available at: http://arxiv.org/abs/2303.10130 (accessed 10 February 2025). Ferretti S (2023) Hacking by the prompt: Innovative ways to utilize ChatGPT for evaluators. New Directions for Evaluation 2023(178\u2013179): 73\u201384. Geiecke F and Jaravel X (2024) Conversations at Scale: Robust AI led Interviews with a Simple Open Source Platform. 4974382, SSRN Scholarly Paper. Rochester, NY: Social Science Research Network. Available at: https://papers.ssrn.com/abstract=4974382 (accessed 25 November 2024). Glaser BG and Strauss AL (1967) The Discovery of Grounded Theory: Strategies for Qualitative Research . Aldine de Gruyter. Goertz G (2020) Social Science Concepts and Measurement . New and completely revised edition. Princeton: Princeton University Press. Goertz G and Mahoney J (2006) A Tale of Two Cultures: Qualitative and Quantitative Research in the Social Sciences . Princeton University Press. Available at: 12345. Graham C (2023) Using ChatGPT for foresight: Futures wheel. In: Medium . Available at: https://medium.com/@christian.graham 49279/using chatgpt for foresight futures wheel 8e79eecfe86b (accessed 2 December 2023). Head CB, Jasper P, McConnachie M, et al. (2023) Large language model applications for evaluation: Opportunities and ethical implications. New Directions for Evaluation 2023(178\u2013179): 33\u201346. Hodgkinson GP and Clarkson GP (n.d.) What Have We Learned from Almost 30 Years of Research on Causal Mapping?: 4. Jalali MS and Akhavan A (2024) Integrating AI language models in qualitative research: Replicating interview data analysis with ChatGPT. System Dynamics Review 40(3): e1772. Jang ME and Lukasiewicz T (2023) Consistency Analysis of ChatGPT. arXiv preprint arXiv:2303.06273 . Epub ahead of print 2023. DOI: https://doi.org/10.48550/arXiv.2303.06273. Jiang H, Ge L, Gao Y, et al. (2023) Large Language Model for Causal Decision Making. arXiv:2312.17122. arXiv. Available at: http://arxiv.org/abs/2312.17122 (accessed 16 January 2024). Koleros A and Mayne J (2019) Using actor based theories of change to conduct robust contribution analysis in complex settings. Canadian Journal of Program Evaluation 33(3). Lacey A and Luff D (2001) Qualitative Data Analysis . Trent focus Sheffield. Available at: https://www.academia.edu/download/61606002/9 Qualitative Data Analysis Revision 200920191225 129738 301p8i.pdf (accessed 27 February 2024). Laukkanen M (1994) Comparative Cause Mapping of Organizational Cognitions. Organization Science 5(3): 322\u2013343. Laukkanen M and Wang M (2016) Comparative Causal Mapping: The CMAP3 Method . Routledge. Mason S and Montrosse\u2010Moorhead B (2023) Editors\u2019 notes. New Directions for Evaluation 2023(178\u2013179): 7\u201310. Mayne J (2012) Making causal claims. Epub ahead of print 2012. Mayne J (2015) Useful Theory of Change Models. Canadian Journal of Program Evaluation 30(2). Nielsen SB (2023) Disrupting evaluation? Emerging technologies and their implications for the evaluation industry. New Directions for Evaluation 2023(178\u2013179): 47\u201357. OpenAI (2024) Enterprise privacy at OpenAI. In: Enterprise privacy at OpenAI . Available at: https://openai.com/enterprise privacy/. Powell S, Copestake J and Remnant F (2023) Causal Mapping for Evaluators. Evaluation . Epub ahead of print 2023. Powell S, Larquemin A, Copestake J, et al. (2023) Does our theory match your theory? Theories of change and causal maps in Ghana. In: Simeone L, Drabble D, Morelli N, et al. (eds) Strategic Thinking, Design and the Theory of Change. A Framework for Designing Impactful and Transformational Social Interventions . Edward Elgar. R Core Team (2015) R: A language and environment for statistical computing,. R Foundation for Statistical Computing, Vienna, Austria. Ray PP (2023) ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber Physical Systems 3. Reid AM (2023) Vision for an equitable AI world: The role of evaluation and evaluators to incite change. New Directions for Evaluation 2023(178\u2013179): 111\u2013121. Resnik P and Lin J (2010) Evaluation of NLP systems. The handbook of computational linguistics and natural language processing . Wiley Online Library: 271\u2013295. Rietz T and Maedche A (2022) Ladderbot a Conversational Agent for Human Like Online Laddering Interviews. SSRN Electronic Journal . Epub ahead of print 1 January 2022. DOI: 10.2139/ssrn.4062500. Rory Hooper, Nihit Goyal, Kornelis Blok, et al. (2023) A semi automated approach to policy relevant evidence synthesis: Combining natural language processing, causal mapping, and graph analytics for public policy. Available at: https://www.researchsquare.com (accessed 14 November 2023). Roy A and Rambo Hernandez K (2021) There\u2019s So Much to Do and Not Enough Time to Do It! A Case for Sentiment Analysis to Derive Meaning From Open Text Using Student Reflections of Engineering Activities. American Journal of Evaluation 41: 1\u201317. Shank DB (2016) Using Crowdsourcing Websites for Sociological Research: The Case of Amazon Mechanical Turk. Am. Sociol. 47(1): 47\u201355. Sia S, Dalmia A and Mielke SJ (2020) Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too! arXiv:2004.14914. arXiv. Available at: http://arxiv.org/abs/2004.14914 (accessed 26 February 2024). Wachinger J, B\u00e4rnighausen K, Sch\u00e4fer LN, et al. (2024) Prompts, Pearls, Imperfections: Comparing ChatGPT and a Human Researcher in Qualitative Data Analysis. Qualitative Health Research : 10497323241244669. Wilson Grau R and Britt H (2012) Outcome harvesting. Cairo: Ford Foundation . Epub ahead of print 2012. Yang J, Han SC and Poon J (2022) A survey on extraction of causal relations from natural language text. Knowledge and Information Systems 64(5): 1161\u20131186. Yuen M (2022) Chatbot market in 2022: Stats, trends, and companies in the growing AI chatbot industry. In: Insider Intelligence . Available at: https://www.insiderintelligence.com/insights/chatbot market stats trends/. Ziulu V, Anuj H, Hagh A, et al. (2024) Extracting Meaning from Textual Data for Evaluation: Lessons from Recent Practice at the Independent Evaluation Group of the World Bank. In: Artificial Intelligence and Evaluation . Routledge. List of figure captions Figure 1: A high level overview causal map. Causal factors are automatically clustered as described in the Supplementary Material."}, {"title": "trash", "path": "/000 reference articles/trash.html", "text": "!An AI powered workflow for collecting and understanding stories at scale !Abstract !015 Causal mapping in evaluation/Reconstructing program theory empirically !Causal mapping as a way to reconstruct program theory empirically !It is possible to gather evidence at scale about program theory and contribution simultaneously three steps !Steps in scaling the automated causal mapping workflow !Previous work on each step !Step 1 existing work Using an AI interviewer to gather causal information at scale !Step 2 existing work using automated causal mapping to code causal information at scale !Step 3 existing work using causal mapping to help answer evaluation questions !Steps in the \u201cAI assisted causal mapping pipeline\u201d !Results Discussion Question for Step 1 can an AI interviewer successfully gather causal information at scale? : Our AI interviewer was able to conduct multiple interviews with no researcher intervention at a low cost, reproducing the results of Chopra and Haaland (2023) and Geiecke and Jaravel (2024). The interview transcripts read quite naturally and the process seems to have been acceptable to the interviewees. Question for Step 2 can automated causal mapping successfully code causal information? : Automated coding was able to identify causal claims made by respondents. The coding was noisy, with 35% dropping at least one quality point, but with no evidence of systematic errors . This level of precision is adequate for sketching out \u201ccausal landscapes\u201d but would not be for high stakes evaluations without additional manual correction. The accuracy can also be substantially improved by getting the AI to revise its work, (see redacted). This procedure still involves the researchers making significant high level decisions in the formulation of the coding instructions as well as, before analysis, in clustering similar factor labels into groups. We believe this coding approach using genAI represents a significant improvement over the more hard coded approaches for identifying causal relationships expressed in text (Dunietz, 2018; Dunietz et al., 2017; Rory Hooper et al., 2023; Jiang et al., 2023; Yang et al., 2022), and provides a more detailed, section by section coding which relies less on using AI as a black box to identify themes for initial coding (Jalali and Akhavan, 2024) or to identify a global map (Graham, 2023). Question for Step 3 can automated causal mapping help answer evaluation questions? : An overview map was produced which included over 40% of the causal claims identified within the transcripts, using just 11 relatively broad factor labels. The most central factor with the highest number of citations was Economic stress, which is a plausible result, with plausible connections to other factors. We can use the map to identify and weigh up the evidence for contributions from and to individual factors. For example, the major contributions to Economic stress are Government policy and Covid 19, as well as \u201cself loops\u201d mentioned by 46 sources, i.e. where one aspect of Economic stress was seen as causing another. All such results depend on the (not automated) decisions made during the clustering process: how many clusters to use, whether to intervene in labelling, etc. This situation is closely parallel to decisions facing a statistician who has to identify variables for, say, structural equation modelling (Goertz, 2020: 136 ff). Comparison of citation frequency across timepoints was able to show that some links were mentioned significantly more than others, illustrating how this kind of map could be used to explore changes in systems (or in mental models of systems) over time. Caveats Ethics, bias and validity This kind of AI processing is not suitable for dealing with sensitive data because information from the interviews passes to OpenAI\u2019s servers, even though it is no longer used for training models (OpenAI, 2024). Head et al. (2023) and Reid (2023) raise concerns about bias and the importance of equity in AI applications for evaluation, which have led to questions about the validity of AI generated findings (Azzam, 2023). The way the AI sees the world, the salient features it identifies, the words it uses to identify them, and its understanding of causation are certainly wrapped up in a hegemonic worldview (Bender et al., 2021). Those groups most likely to be disadvantaged by this worldview are approximately the same who have least say in how these technologies are developed and employed. AI is developing quickly: new models and techniques become available every month. However, we believe that any tools which genuinely add to knowledge should use procedures which are broken down into workflows consisting of simple individual steps so that humans can understand and check what is happening. Interviewing Researchers should carefully consider whether the interview subject matter is compatible with this kind of approach. For example, the AI may miss subtle cues or struggle to provide appropriate support to respondents expressing distress (Chopra and Haaland, 2023; Ray, 2023). We recommend that interview guidelines are tested and refined by human interviewers before being automated. No automated interview can substitute for the contextual information which a human evaluator can gain by talking directly to a respondent, ideally face to face and in a relevant context. There is likely to be a differential response rate in this kind of interview: some people are less likely to respond to an AI driven interview than others, and this propensity may not be random. Causal mapping Causal mapping is not at all suited for estimating the strength of causal effects: it can reveal the strength of the evidence for the influence of X on Y but this is not to be confused with the strength of the effect itself. There can be strong evidence for a weak link and vice versa. Autocoding The work of the AI coder and clustering algorithms are not error free. The coding of individual high stakes causal links should be checked. In particular, there is a danger of accepting inaccurate results which look plausible. This approach does not nurture substantive, large scale theory building of the kind expected, for example in grounded theory (Glaser and Strauss, 1967). However, it can do smaller scale theory building in the sense of capturing theories implicit in individuals\u2019 responses. This pipeline relieves researchers of much of the work involved in coding but it is not fully autonomous. The human evaluator is responsible for applying the techniques in a trustworthy way and for drawing valid conclusions. Potential Qualitative approach: These procedures approach the stakeholder stories as far as possible without preconceived templates, to remain open to emerging and unexpected changes in respondents\u2019 causal landscapes. Scalability and reach: The AI\u2019s ability to communicate in many languages presents an opportunity to reach more places and people, subject to internet access and the AI\u2019s fluency in less common languages, and to include representative samples of populations. The interview and coding processes are machine driven and use zero temperature, so this approach should be mostly reproducible. Reproducibility opens the possibility of comparing results across groups, places and timepoints. The low cost of coding large amounts of information means that it is much easier to develop, compare and discard hypotheses and coding approaches, something which qualitative researchers have previously been understandably reluctant to do. Qualitative causality: These procedures have the potential to help evaluators answer evaluation questions which are often causal in nature, like: understanding stakeholders' mental models; judging whether \"their\" ToC matches \"ours\"; investigating \u201chow things work\u201d for different subgroups of stakeholders; tracing impact from mentions of \"our\" intervention to outcomes of interest; triaging the key outcomes in stakeholders\u2019 perspectives. In summary, this kind of semi automated pipeline opens up possibilities for monitoring, evaluation and social research which were unimaginable just three years ago and are well suited to today\u2019s challenging, complex problems like climate change and political and social polarisation. Previously, only quantitative research claimed to produce generalisable knowledge about social phenomena validly and at scale, by turning meaning into numbers. Now perhaps qualitative research will eclipse quantitative research by bypassing quantification and dealing with meaning directly, in somewhat generalisable ways. Further work We have tried to demonstrate a semi automated workflow with which evaluators can capture stakeholders\u2019 emergent views of the structure of a problem or program at the same time as capturing their beliefs about the contributions made to factors of interest by other factors. We have presented this approach via a proxy application but have since applied it in real life research. Many challenges remain, from improving the behaviour of the automated interviewer through improving the accuracy of the causal coding process to dealing better with valence (for example distinguishing between \u201cemployment\u201d, \u201cemployment issues\u201d and \u201cunemployment\u201d). Perhaps most urgently needed are ways to better understand and counter how LLMs may reproduce hegemonic worldviews (Head et al., 2023; Reid, 2023). !References !List of figure captions"}, {"title": "! Knowledge graphs", "path": "/005 unpublished/! Knowledge graphs.html", "text": "publish: false coming soon"}, {"title": "! meaning but not epistemology", "path": "/005 unpublished/! meaning but not epistemology.html", "text": "publish: false !400 Being realist about causation !!400 Being realist about causation Causal realism invites us to say that things have the causal power to affect other things. We can learn about causal powers (the powers that types of things have to influence other things) via constructing or observing some kind of counterfactual, but also via other routes; and maybe counterfactual arguments logically follow from facts about causal powers. The meaning of \"X caused Y\" maybe implies something about a counterfactual: broadly speaking, that Y would not have happened if X had not happened and everything else had stayed exactly the same. (Philosophers love to argue over the details.) Counterfactual statements follow from causal claims, they are part of their meaning. Counterfactual evidence can update (strengthen or weaken) the evidence for causal hypotheses. Counterfactual evidence is NOT the only route to updating the evidence for causal hypotheses. Counterfactuals: essential to the meaning of causation, optional for knowledge of it."}, {"title": "! Questions you can answer", "path": "/005 unpublished/! Questions you can answer.html", "text": "Looking at Marina and Tom's causal pathways \"Questions\" (for the CP training below) they never really talk about the fundamental property and challenge of a causal map which is transitivity or tracing. From a b and b c concluding, (maybe, but when and how?), a c. They just never address the question. But if you werent interested in this fundamental property & challenge, why would you use a map or talk about a pathway?? Causal questions that fit within this overarching question may focus on exploring causal pathways or on assessing impact. Apgar and Aston categorize them as follows and provide the following examples: Causal Process Questions: What specific processes led to the observed outcomes? How did the intervention trigger change? What intermediate mechanisms connect the intervention to outcomes? Causal Context Questions: How do different contexts modify the causal mechanism? What contextual conditions enable or inhibit causal effects? How do local factors interact with intervention strategies? Emergent Causality Questions: How do unintended consequences emerge? What unexpected causal pathways developed? How do multiple factors interact to produce outcomes? Comparative Causal Questions: Which intervention strategy was most effective in creating change? How do different approaches compare in terms of causal impact? What are the differential effects of alternative interventions? Equity Focused Questions: How did causal effects differ across social groups? What variations in impact existed? Did the intervention affect different populations differently? Systemic Causality Questions: How did the intervention contribute to systemic changes? What broader transformations were triggered? How do multiple levels of the system interact causally? Impact Contribution and Inquiry Questions: To what extent did the intervention cause the observed changes? What difference did the program make compared to doing nothing? How much of the observed outcome can be directly attributed to the intervention? From eval24 However, we argue that evaluators can break the Janus dilemma and make the best use of causal maps in evaluation by considering causal maps not primarily as models of either beliefs or facts but as repositories of causal evidence. We can use more or less explicit rules of deduction, not to make inferences about beliefs, nor directly about the world, but to organise evidence: to ask and answer questions such as: \u2022 Is there any evidence that X influences Z? \u2022 . . . directly, or indirectly? \u2022 . . . if so, how much? \u2022 Is there more or less evidence for any path from X to Z compared to any path from W to Z? \u2022 How many sources mentioned a path from X to Z? \u2022 . . . of these, how many sources were reliable? We also argue that this is a good way of understanding what evaluators are already doing: gathering and assembling data from different sources about causal connections in order to weigh up the evidence for pathways of particular interest, like the pathways from an intervention to an outcome."}, {"title": "!350 is there a hidden vocabulary", "path": "/005 unpublished/!350 is there a hidden vocabulary.html", "text": "publish: false Prof Beach at ees: no, there is no very useful mid level vocabulary for causation (how to do causal mapping, even at scale) So I was just at the European evaluation Society meeting in helimony last week and heard an interesting presentation from Professor Beach, from our horse in Denmark. And he was calling for a or discussing the possibility of a common vocabulary for the A series of change and program theory. And I think this is reminds me of Nancy Cartwright, whom he frequently sighted with her visual vocabulary, which includes elements like blockers, and also Gary Gertz has is fascinated by the question of what different elements what One might need in a explanatory map, but I'm quite sure that the answer to all of this is that it isn't going to work, and if there was such a possible common set of elements one could use to build up many different kinds of program theory in a way that would cover most of the relevant use cases, then we would have discovered one, probably hundreds of years ago, and certainly by now. Can also think of Rick Davies work on theory of change. I we have something like an alternative vision, which says that the only elements we need or are ever really going to have, are Simply boxes for the elements of the causal map, which we call factors, although we have to point out that factors can also be the ends of causal chains, and not just explanatory factors. Is factors and lines between them, where the lines simply mean an undifferentiated influence, or in our case, when we're talking about causal maps that somebody believes or claims there is an undifferentiated influence. This means we don't have or hold out any hope for any kinds of special symbols or categories with which we might differentiate the kinds of causal influence, like necessary or sufficient conditions or packaging factors into packages, or designating functions like moderators or inhibitors. In fact, we are so radical as to say that we have to abandon the idea that the nodes are somehow naming variables which can somehow go up or down or take different values, and that the arrows, therefore, at least can or should express At least a positive or negative direction of influence and or, more ambitiously, a Strength of influence. For us, the boxes are simply a bare propositions and the arrows are simply bare causal influence. But you might say, What an extraordinary, impoverished universe you live in. But that's not the case, because what we do is what we believe actual human speakers do, which is, in the most cases, to make the labels do all the work. And make the links do none of the work. This obviously simplifies things, because you only have the labels to worry about, not the links. So for example, people say things like, but because I don't have the money. I'm not going on holiday this year. So we can see this as an undifferentiated causal link between causal factors which are relatively sophisticated, namely, I the absence of something on the left, or possibly the absence of a sufficient amount of something on the left, and its influence on a future expectation or lack of future expectation. So evidence. This makes transcribing what people say in terms of causal influences very simple, because we don't have to worry about anything like necessity or sufficiency or anything else, and all the work is done by the labels the question then, of course, becomes, then means that the usual problem of, how do we organize our labels more generally in order to be able to gain a knowledge by saying that two or more labels are somehow related, which is the fundamental problem of qualitative text analysis and it's This function to which we give the job of combining for example, this person's child is ill with that person's child is ill, and subsuming them under a Common label. And in the same way, we have the problem of somehow combining the information about this person's going on holiday this year and this person, this other person isn't going on holiday this year. And somehow being able to present them as in one way, the same idea, but with differing I somehow differing parameters, somehow we need to, on the one hand, accept them as related or even instances of the same thing, while not losing the information that they are saying something different about this thing, the fact is that although humans sometimes do actually express causal influences in the form of an expression of how one variable influences another with a certain polarity and strength. These cases are really rare, and in the vast majority of human speech and writing, we express a relatively undifferentiated causal influence, but have a hair raising uncountable universe of different kinds of causal factor labels, which are a I also be somehow related or not related to one another. And I'm saying both that this barefoot style is the way that people, in fact, do express themselves, but also it's how we should try to understand and code what's going on."}, {"title": "!400 Being realist about causation", "path": "/005 unpublished/!400 Being realist about causation.html", "text": "publish: false Nice quote Andi Fugard Yes and I think we can go further, if we are realist about causation. We can say that narrative accounts are full of claims about causal powers, that X had the power to affect Y, and X did exercise that power and Y was affected (perhaps in this particular case in spite of or with the assistance of other things). Causal realism invites us to say that things have the causal power to affect other things. We can learn about causal powers via constructing or observing pseudo counterfactuals, but also via other routes; and maybe counterfactual arguments logically follow from facts about causal powers. But the meaning of \"X caused Y\" can't be reduced to a counterfactual statement about co occurrences. The weird thing is that most physical and natural scientists think about causation in a realist way, but in the social sciences we tell ourselves not to because it isn't scientific (!) The meaning of \"X caused Y\" maybe implies something about a counterfactual: broadly speaking, that Y would not have happened if X had not happened and everything else had stayed exactly the same. (Philosophers love to argue over the details.) But the meaning of \"X caused Y\" can't be reduced to a statistical, counterfactual statement about co occurrences. It says that the co occurrences are true but they happen because X has the power to cause Y , and X happened."}, {"title": "!500 nuance", "path": "/005 unpublished/!500 nuance.html", "text": "publish: false it's all very well saying ah but quant doesn't capture these rich nuanced stories. How much time do you have? start multi column: ID 2583 Agreeing with most of the above, and a special applause for Florencia and Tom's piece. I like the way they address the process rather than the type of data or analysis. Coming here from a modest acquaintance with psychotherapy effectiveness research, where I\u2019ve observed similar debates between quantitative and qualitative schools. I think it\u2019s unfair to claim this problem with nuance is solely the fault of quantitative researchers for being insufficiently nuanced. For decades \u2014 at least 30 or 40 years \u2014 it\u2019s been widely accepted that the key question isn\u2019t simply \u201cwhat works?\u201d but rather \u201cwhat works, for whom, in what circumstances, and why?\u201d And I'm assuming that any colleagues with their salt in the quant evaluation of development effectiveness think the same. So in therapy research, they acknowledge the complex interplay of variables, such as the pathology, the context of the treatment, the person delivering it, and other factors. No one cares (any more) about the main effect, like overall CBT is the best or psychodynamic is bullshit. To be fair, the basic concept of presenting scientific results as a main effect with nuanced caveats \u2014 like interaction effects and subgroup analyses \u2014 is a brilliant move. Maybe it's just how our brains work. It allows scientific findings to be communicated to the public in a digestible way. For instance, people might remember that CBT is effective for anxiety but then qualify that with considerations like comorbidities, age group, or cognitive functioning. You can't remember or work with hundreds of interaction effects and in this kind of research there are really very many of them. This method simplifies communication: starting with a general takeaway before diving into specific contexts and probabilities of success. While this is an oversimplification built on numerous assumptions, it is still an effective way to derive meaning from data. Of course, in some datasets, you might in fact find no main effect but significant interactions \u2014 therapy that doesn\u2019t work overall but is effective for specific groups and possibly harmful to others. This complexity is harder to remember and to base policies on. And even experts can struggle with higher order interactions. This challenge isn\u2019t unique to quantitative results. Qualitative research faces similar barriers: how do you communicate nuanced findings to a policymaker who demands simple bullet points, wherever they come from? If the response is \u201cit depends,\u201d the message often fails to resonate. Therefore, claiming that qualitative research inherently occupies the \u201cnuance corner\u201d or just communicates better is misleading. column break I'm guessing that plenty of quant people must have uttered a wry grunt when Paulson and Tilly came up with \u201cwhat works for whom, in what circumstances.\u201d They really did not invent this way of thinking. The real issue, as Tom and others have already said, surely isn\u2019t the type of data or analysis but how much we organise our world in terms of high stakes questions and binary, all or nothing propositions. For example, the Ofsted scandal in the UK highlighted the dangers of reducing school performance to oversimplified ratings like pass or fail. It took tragic events, like teachers\u2019 suicides, to prompt reforms. Yet, history repeats itself. Months later, the incoming Labour government boasts about publishing league tables for hospitals \u2014 another flawed reduction of complex evaluative judgements into a single metric, whether the league table comes from something \u201csoft and fuzzy\u201d like QCA or hierarchical card sorting \u2014 or some other favourite qualitative approach. It is the reductionism, whether from qualitative or quantitative approaches, that is problematic. The problem is how much we expect information to be aggregated as it moves up the chain of decision making. By the time it reaches the highest levels, it\u2019s reduced to a couple of traffic light icons for a single minister. That oversimplified output then gets propagated back down as blunt directives, such as in the extreme case, \u201cClose this school\u201d or Put this hospital into special measures.\u201d I'm caricaturing here and this isn't something I know a lot about, but common sense would surely go with more distributed, localized, and adaptive management approaches being more trusting of teachers, headteachers, nurses doctors, local education and health authorities, empowering them with the resources and support to make informed decisions within their contexts. And as often, the optimum is somewhere in between, between the big picture and the local context. end multi column nuance and high stakes decisions"}, {"title": "!700 Retraceable, reconstructable, operationalisable", "path": "/005 unpublished/!700 Retraceable, reconstructable, operationalisable.html", "text": "publish: false 600 bricolage the best criterion for rigour: replayability, retrospective reconstructability: even if it wouldn't have been obvious a priori how to answer this question, afterwards you can see how these conclusions were arrived at and can retrace the steps yourself. No method, not even RCTs, gives you a free pass to not do evaluative thinking. Because they're always embedded in the judgement of the evaluator that this is the right thing to do and I've checked all loopholes. In the same way our reasoning when doing bricolage is evaluative reasoning. The real life application even of what seems like a monolithic tool like an RCT involves at different levels dozens of evaluative judgements about what to accept and what not to accept, whether this questionnaire or econometric measure is up to scratch, whether to trust this researcher, etc., whether this quantity of missing data is acceptable, etc. This appeal to evaluative reasoning is in some sense a transcendent one: you can never completely transfer responsibility to an algorithm. Because you always at least bear responsibility for using this particular algorithm in this particular way. At the same time algorithms can be incredibly helpful in breaking down our reasoning into transparent steps. Nachvollziehbarkeit also means it fits in your head Bricolage, rigour and Nachvollziehbarkeit in a partially evaluative summary of the Strategic Plans of 191 Red Cross / Red Crescent societies globally. Steve Powell, \ufeffMon, May 20, 2024\ufeff But what I find weaker in this paper is the just the generic weakness of the constructivist response to classic Cambellian validity: The bomb that's just dropped is suddenly we do have ways thanks to generative AI of making qualitative judgements (about qualitative constructions, about people's meanings, people's beliefs and understandings) which are in some sense reproducible and intersubjective and reliable. The methods aren't essentially any different from before, but they can be done quickly, reproducibly and at scale and in a way which vastly reduces noise due to the individual researcher. (Our chosen AI is of course also in some sense biassed, but it's the same bias each time, so we can compare results across timepoints and groups in a way which was never possible before: we can claim our methods are reliable ). As a footnote here are two members of the extended family of the concept of validity:"}, {"title": "!Humans are the best detectors of causation", "path": "/005 unpublished/!Humans are the best detectors of causation.html", "text": "Humans are the best detectors of causation We can thank Judea Pearl for promoting the insight that if you want to thrive in this world, you have to understand causality natively. We humans make causal connections from an early age. We wouldn't survive long if we didn't. GPT 3.5 just about understood causation. GPT 4 and more recent models understand causal connections within text very well. Our understanding of the world is drenched with causal understanding: information and hypotheses about how things work (mostly accurate enough, sometimes not). It's really hard for us to not think causally: the concept of correlation is much harder to understand than the concept of causation. Why are humans the best detectors of causation? 1. Evolutionary Adaptation: Human brains have evolved specifically to detect and act on causal relationships. Survival depends on recognizing which actions lead to which outcomes\u2014e.g., which plants are safe to eat, which animals are dangerous, and how to use tools. This evolutionary pressure has made causal reasoning a core part of human cognition. 2. Intuitive Causal Models: From infancy, humans build mental models of the world that are fundamentally causal. Children naturally ask \"why\" questions and seek explanations, not just associations. This ability to infer hidden causes and predict effects is unique and robust compared to current artificial systems. 3. Generalization and Flexibility: Humans can generalize causal knowledge across domains. For example, understanding that pushing causes movement can be applied to objects, social situations, and abstract concepts. This flexibility allows humans to detect causation even in novel or ambiguous situations. 4. Counterfactual Reasoning: Humans routinely engage in counterfactual thinking\u2014imagining what would happen if things were different. This is a hallmark of causal reasoning and is essential for planning, learning from mistakes, and scientific discovery. 5. Distinguishing Correlation from Causation: While humans sometimes make errors (e.g., seeing causation where there is only correlation), we are still far better than machines at using context, background knowledge, and intervention to distinguish true causal relationships. 6. Social and Cultural Transmission: Human societies accumulate and transmit causal knowledge across generations through language, stories, and education. This collective causal understanding is a foundation of science, technology, and culture. Human Strengths in Causal Inference Causal inference is the process of determining whether and how one event or variable brings about another. Some writers mistakenly assume that only a controlled experiment can \"really\" provide a route to causal inference. Humans excel at causal inference not only through counterfactual reasoning, but also through a variety of other strategies: Observation and Pattern Recognition: Humans are adept at noticing regularities and anomalies in their environment. We naturally look for patterns\u2014such as temporal precedence (A happens before B), co occurrence, and changes following interventions\u2014that suggest causal relationships. Even without formal training, people intuitively apply principles like \"no effect without a cause\" and \"causes precede effects.\" Intervention and Experimentation: Humans frequently test their causal hypotheses by intervening in the world\u2014changing variables and observing outcomes. This hands on experimentation, whether in childhood play or scientific research, is a powerful tool for distinguishing causation from mere correlation. Use of Multiple Sources of Evidence: Humans combine different types of evidence\u2014temporal order, statistical regularities, mechanistic explanations, and observed interventions\u2014to make robust causal inferences. We can weigh conflicting evidence, consider alternative explanations, and update our beliefs as new information arises. Comparing Hypotheses: Humans excel at comparing and refining competing causal hypotheses, a skill central to both realist evaluation and process tracing. In realist evaluation, people naturally ask not just \u201cDid it work?\u201d but \u201cWhat works, for whom, in what circumstances, and why?\u201d This involves generating multiple candidate explanations (mechanisms), considering how they interact with context, and systematically weighing evidence for and against each hypothesis. Similarly, in process tracing\u2014a method often used in social science and history\u2014humans are adept at assembling detailed sequences of events and looking for \u201ccausal process observations\u201d that support or refute specific causal pathways. We can identify critical pieces of evidence (so called \u201csmoking guns\u201d or \u201choop tests\u201d) and use them to discriminate between rival explanations. This ability to juggle multiple hypotheses, integrate diverse types of evidence, and reason through complex causal chains is a hallmark of human causal inference. It allows us to move beyond simple associations and develop nuanced, context sensitive explanations for how and why outcomes occur. Contextual and Background Knowledge: Unlike machines, humans bring a vast store of background knowledge and context to bear on causal questions. This allows us to rule out spurious correlations and recognize plausible causal mechanisms, even in complex or ambiguous situations. Social Learning and Testimony: Humans also infer causation from the experiences and reports of others, integrating social and cultural knowledge into their causal models. This collective inference process accelerates learning and helps avoid individual errors. In sum, humans are not just passive recipients of causal information; we are active causal detectives, constantly inferring, testing, and refining our understanding of how the world works. This multifaceted approach to causal inference is what makes us the best detectors of causation. Comparison to Machines Recent AI models are now great at recognizing causal language patterns. However they are still worse than humans at reading between the lines and inferring a more connected causal story from a less explicit text. Most significantly, humans don't only have to infer causation from carefully prepared sets of texts, but live, on the hoof, while trying to get the kids to school and texting. Conclusion Causal reasoning is not just a feature of human cognition\u2014it is its backbone. Our ability to detect, infer, and act on causal relationships is what allows us to navigate, survive, and thrive in a complex world. While machines are improving, humans remain the best detectors of causation, both individually and collectively. See also: 400 realist causation"}, {"title": "!qual causal models", "path": "/005 unpublished/!qual causal models.html", "text": "So when you do an evaluation, what's the product? What do you get? Obviously, you can think of the evaluation report, which might answer predetermined questions, but it may also include material that goes beyond the specific questions we were tasked with answering\u2014for example, to address unanticipated issues or simply to describe or contextualize. Another important output is relational: hopefully, people have come together in a way that helps to expand learning and perhaps develop projects or relationships. But today I want to talk about something different. A statistical analysis not only answers questions but gives you the whole model can a qual analysis do that too? When you do quantitative research, you might have specific research questions, but often one of the major outputs is a statistical model of the phenomenon. (There might be an effort to go beyond the data and hope that the model generalizes more widely, but that's not my focus here.) In the simplest case, the model might represent a suspected causal relationship\u2014say, between the amount of screen time in the evening and difficulty falling asleep. At the very least, the model allows us to look at a case in the dataset and say: on this day, this person looked at a screen for three hours and rated, let's say, a difficulty of four out of five falling asleep on some self rating scale. Because we have the model, we can explain that : yes, this is quite a high level of difficulty, and it's explained at least partly by a high level of screen time, at least in this individual case. The model might also enable us to make predictions , like: people who, at least in this context, spend more than three hours on screens in the evening are, on average, going to experience a higher level of difficulty falling asleep. A more sophisticated model will probably capture more variables, and many models\u2014like directed acyclic graphs\u2014link up these kinds of connections into a causal network, so you can explain or even predict how tweaking one variable will affect another variable downstream of it. Of course, there are other kinds of statistical models apart from causal models, but if you're reading this, you love causal models, don't you? There's a relatively small but extremely well funded section of evaluation activity based around this kind of statistical causal model, with randomized control trials (RCTs) as one facet. Ideally, an RCT is tasked not primarily with producing a model, but with answering a specific question, like: which is the better of these two interventions? Or: does this treatment work better than a placebo? But these are calculations conducted on the underlying model, which, from the point of view of workflow, is the major output of the work. To generalise or not to generalise There are two ways you might want to use that kind of model. One is to answer further questions about the same dataset\u2014for example, to ask whether a particular subset (say, people over 70) differ in how screen time influences difficulty falling asleep, compared to other subgroups. If it's a sophisticated model, it might allow us to generalize beyond this specific context, perhaps by including more general variables like attention style or eye movement speed, which might help explain and predict behavior in other contexts. Most quantitative researchers make a big deal about generalizing the model beyond the specific use case or context. In fact, the whole point of the study is normally to do that, and there's a whole armoury of tools, concepts, and arguments about how and under what conditions you can generalize a model to other people, other years, or even other countries with different kinds of screens, etc. Can qualitative research do that? The majority of evaluations aren't like that, although they might include a specific quantitative question somewhere in their terms of reference. In most cases, we think of the research output as a report in which the original (possibly modified) list of questions is answered, with additional narrative to summarize and link these sections. Now, going beyond strictly quantitative paradigms, some evaluation projects will also include what we might call qualitative modelling . For example, if we're using QCA (Qualitative Comparative Analysis), apart from answering specific questions, we've likely also produced QCA style tables, which could help us answer other questions beyond those we were actually tasked with. You might see those tables as annexes to the report. The same goes for causal loop diagrams and other techniques, which are essentially quantitative models but with a more restricted set of numbers. For example, in causal loop diagrams, we might model a variable like inflation with a number from 1 through 0 to +1, and do the same for variables like unemployment or military threats, building models of the relationships between these things using simplified numbers. What I want to argue here is: any halfway decent evaluation, which at least implicitly gathers qualitative information about how things within the evaluation influence one another, can be considered as constructing a qualitative causal model. This is irrespective of the specific methods used\u2014even if it doesn't include something explicitly called causal pathways analysis or causal mapping. Qualitative models as products: theory, model, or something else? In quantitative research, the idea of a \"model\" as a product is well established: you build a model, and then you can query it to answer new questions\u2014even ones you didn't anticipate at the start. But what about qualitative research? Can the result of a qualitative analysis be a model in this sense, rather than just a set of answers to specific questions or a summary? Of course (some) qualitative research produces models. Just don't call them that. Some qualitative researchers do indeed conceptualize their results as models. For example, grounded theory often produces a theoretical model that explains the underlying processes or relationships within the data. These models can be revisited and \"queried\" to generate new insights beyond the initial research questions. However, many qualitative researchers are more comfortable with the term \"theory\" rather than \"model.\" \"Theory\" aligns more closely with the interpretive and conceptual nature of qualitative work, emphasizing explanation and understanding rather than prediction or parameterization. Still, the distinction is often more about language than substance: both models and theories can serve as frameworks for making sense of data and for generating new questions. How are qualitative models used? In qualitative research, especially in fields like grounded theory and narrative inquiry, the focus is less on \"prediction\" and more on generating understanding or insight. Researchers talk about \"theorizing\" from the data\u2014developing concepts and frameworks that explain the phenomena under study. Once a theory or model is developed, it can be revisited, interrogated, and applied to new data or different contexts. This iterative process allows for continual refinement and deeper insight. Importantly, a qualitative model or theory can also be used to answer new questions about the same dataset. For example, after developing a grounded theory, researchers (or others) can return to the theory and use it as a lens to interpret further cases, refine concepts, or generate new insights\u2014without having to re examine all the original data. This practice is sometimes referred to as \"secondary analysis\" or \"theoretical application,\" where the theory or model functions as a standalone analytical tool. In causal mapping, for instance, the model might consist of a network of causal links derived from qualitative data. Even if there are no quantitative parameters on the links, the model can still be queried: \"Is there evidence for a causal pathway from A to B?\" or \"What are the main factors influencing outcome X?\" This allows the model to be used flexibly, supporting both anticipated and unanticipated lines of inquiry. Why does this matter? Thinking of qualitative research outputs as models (or theories) that can be queried and reused has several advantages: Transparency: It makes explicit the structure of the findings and how they relate to the data. Reusability: Others can use the model/theory to answer new questions or apply it in new contexts. Iterative learning: The model can be refined and expanded as new data or perspectives emerge. Bridging paradigms: It helps bridge the gap between qualitative and quantitative traditions, showing that both can produce structured, interrogable outputs. In summary, while qualitative researchers may prefer the language of \"theory\" over \"model,\" the idea is the same: a well constructed qualitative analysis can produce a framework that is more than just a set of answers\u2014it is a model of the phenomenon, one that can be queried, shared, and built upon."}, {"title": "!Transitivity as the fundamental property", "path": "/005 unpublished/!Transitivity as the fundamental property.html", "text": "Causal pathways: when is a pathway not just a link? Thanks guys, there's lots to like here and lots to agree with. Helping to find what causal hypotheses to focus on... What do you have to say about pathways as opposed to individual links/mechanisms? If we called this approach \"quality and rigour in causal mechanisms evaluation\" would that miss anything? Disclaimer: for me, the logic around how links might combine into pathways and what that means for evaluation, that's the most exciting part. e.g. how might this intervention influence an outcome which might be multiple steps downstream of it? Megan Shoji on cp coffee chat today 3/7/2025, getting ppl to rate the evidence for the whole pathway vs evidence for particular hotspots. We've been honoured to be part of the Causal Pathways initiative since 2021, it's been really great to network with these evaluation experts, and we've learned and benefitted a lot. As someone interested not only in the content of causal pathways work but also its logical underpinnings, I'm really glad to have Tom Aston and Marina Apgar's \"How do we define and support quality and rigor in Causal Pathways evaluation?\" (Apgar & Aston, 2025)as a go to guide for this: Alongside (Lynn et al., 2021) I think it's the main Causal Pathways text. BUT I think there is still something essential which is missing from Causal Pathways thinking as it currently stands. Why do we talk about causal pathways (and causal networks and diagrams and maps) rather than just listing individual links : separate, hypothesised, cause/effect relationships? What is it about the logic of causal networks which we are leveraging, or at least implying, when talking about causal pathways? Do we have guidance on how to assemble evidence for A C on the basis of evidence for A B and B C? Assembling evidence to make evaluative judgements about individual links One thing we do often do is query the network to answer questions like: what are the various influences on factor F. or what are its various consequences. Evaluation literature Occasionally we also mention network measures like centrality. Looking at Marina and Tom's causal pathways \"Questions\" (for the CP training below) they never really talk about the fundamental property and challenge of a causal map which is transitivity or tracing . From a b and b c concluding, (maybe, but when and how?), a c. They just never address the question. But if you weren't interested in this fundamental property & challenge, why would you use a map or talk about a pathway??Causal questions that fit within this overarching question may focus on exploring causal pathways or on assessing impact. Apgar and Aston categorize them as follows and provide the following examples: Aston and Apgar list \"all possible types of causal questions.\" (Apgar & Aston, 2025, p. 16) I've marked with \u2705 those which I believe can be reasonably answered through a \"single link\" lens Causal Process Questions: \u2705 What specific processes led to the observed outcomes? \u2705 How did the intervention trigger change? \u2705 What intermediate mechanisms connect the intervention to outcomes? Causal Context Questions: \u2705 How do different contexts modify the causal mechanism? \u2705 What contextual conditions enable or inhibit causal effects? \u2705 How do local factors interact with intervention strategies? Emergent Causality Questions: \u2796How do unintended consequences emerge? \u2753What unexpected causal pathways developed? \u2705 How do multiple factors interact to produce outcomes? Comparative Causal Questions: \u2753 Which intervention strategy was most effective in creating change? \u2753 How do different approaches compare in terms of causal impact? \u2753 What are the differential effects of alternative interventions? Equity Focused Questions: \u2796 How did causal effects differ across social groups? \u2796 What variations in impact existed? \u2796 Did the intervention affect different populations differently? Systemic Causality Questions: \u2796 How did the intervention contribute to systemic changes? \u2796 What broader transformations were triggered? \u2796 How do multiple levels of the system interact causally? Impact Contribution and Inquiry Questions: \u2796 To what extent did the intervention cause the observed changes? \u2796 What difference did the program make compared to doing nothing? \u2796 How much of the observed outcome can be directly attributed to the intervention? \"The practice of critical reasoning in causal analysis involves: Identifying causal hypotheses and interrogating each step within a causal chain\" Transitivity, Janus There are at least three problems of transitivity which we need to think about Given that A influences B and B influences C, does A influence C? Given that P believes that A influences B and P believes that B influences C, does P believe that A influence C? Given that someone believes that A influences B and someone else P believes that B influences C, does someone (who? we? the people?) believe that A influence C?"}, {"title": "bricolage", "path": "/005 unpublished/600 bricolage.html", "text": "!700 Retraceable, reconstructable, operationalisable Yes! we hear a lot about how one could or should mix and match our methods. But there isn't much on HOW to do that. That's why Tom and Marina's Bricolage paper really hits the spot. Yes I really agree that there is no compelling reason not to include quantitative methods under the bricolage umbrella. We would be likely to get much better uptake if we didn't present this as being something confined to the qualitative box. I think the underlying problem is this: I think the reason this problem is hard is because answering it requires a general theory of what makes an evaluation workflow valid. That's a tall order. If part of the answer is \"most evaluations need to include most of a special list of specific evaluation functions\", we need to argue why this should be so, and where this list of functions come from. In the absence of a general theory, (Scriven's general logic of evaluation?) we can perhaps just appeal to our obligation as evaluators to use \"evaluative reasoning\" in every step of the recipe: Here we would have to start talking about how some general theory of evaluation would have to bravely grab the thistle and address the need for a concept like \"validity\" in qualitative methods. I don't think it's enough to replace the concept with \"rigour\" or to hope it can be approached with a mix of other related criteria like transparency or participation. \"Rigour\" begs the question, because you can rigorously apply an obviously inappropriate (and invalid) method. What our client in the end wants to ask is: ok, your workflow was inclusive and transparent and rigorously applied, but does it actually provide a valid answer to my original question? You said [for example] that my intervention didn't significantly affect the outcome, but how can you be reasonably sure that if it had affected it, you would have detected and reported that, and that you'd only report it as ineffective when it really was ineffective? But, this discussion would be a long one. We would have to address the fact that in many cases beyond simple numerical measurement applications, we can't define validity as something like \"corresponding to the facts\" because the facts in a sense don't exist until we construct them (intersubjectively, collectively) through our workflow; and then we would have to show how anyone else could now follow our workflow and reconstruct broadly similar answers. Often we have to start by describing the extent to which there is not agreement about what the question means and/or how to arrive at an answer to it, and then propose / construct a way to do this in a way which can be accepted so that stakeholders say \"well now we agree enough what these questions mean and how to answer them\". For example we have broken down the task into smaller pieces, established sufficiently unambiguous rubrics for success, etc. Anyway ... Bricolage should also address the fact that we often don't arrive at our final workflow until the evaluation is nearly finished, that we often need time to pause and reflect and talk to people and tweak and reassemble, and that the workflow we arrive at is something we might not have been able to imagine when we started: and yet nevertheless we can now publish our workflow in a way that others could in principle reconstruct. One more question to address: Is Bricolage specifically for impact evaluation? I don't really see why it should be. Marina"}, {"title": "OH plus Attitudes", "path": "/005 unpublished/650 OH plus Attitudes.html", "text": "Strengthening OH with causal mapping I read this by Michelle Garred, Conny Hoitink, Jeph Mathias and other colleagues with much interest. Outcome Harvesting Plus Attitude Change. Ripple Peace Research & Consulting . Retrieved 21 April 2025, from https://ripple peace.net/outcome harvesting plus attitude change/ Attitude can cover areas including perceptions, emotions, opinions, knowledge, assumptions, beliefs, values Attitude Changes are internal and therefore invisible. This distinguishes them from behavior changes, which are external and therefore observable. A Behavior Change Statement is one unit of data within an OH+AC data set. It includes not only the description of a behavior change but also its significance, contribution details and related attitude change. (In mainstream OH, this is called an Outcome Statement.) Behavior Change. (An Attitude Change is not considered an outcome because it is not observable.) AC data can add significant value to OH in two situations: \u2022 Where attitude change is essential to achieving program results \u2013 because, in such situations, considering attitudes in evaluation is a key aspect of understanding how social change happens. \u2022 Where attitudes are central to the worldview of the program implementers or social actors \u2013 because, in such situations, considering attitudes can help to enhance cultural responsiveness and multicultural validity. In Systems theory pioneer Donella Meadows defines mental models broadly, including assumptions, values and beliefs. In other words, \u201cmental models\u201d in systems theory hold roughly the same meaning as \u201cattitudes\u201d in OH+AC (and in social psychology). Meadows argues that mental models are not just any part of the system, but actually the most influential part of the system. Mental models may be the most strategic place to intervene within a system in order to change it.1 UT I WAS PUZZLED BY Sample OH+AC Questions for Progress Tracking or Evaluation ....\u2022 What significant ACs have been identified? To what extent do they indicate progress toward the program\u2019s long term aims? THIS HAS CONSEQUENCES C. However, OH+AC carries an added requirement to communicate with some or all of the social actors influenced by the program, because they are the only ones who can verify or clarify the ACs that they have experienced. This can In OH+AC, you will add ACs as an additional component of the BC Statement. This changes absolutely nothing in terms of the centrality of BCs as the anchor of the BC Statement. Ho"}, {"title": "What is causal mapping", "path": "/010 Causal mapping/0.10 What is causal mapping.html", "text": "From Better Evaluation. Causal mapping helps make sense of the causal claims (about \"what causes what\") that people make in interviews, conversations, and documents. This data is coded, combined, and displayed in the form of maps. These maps show individuals' and groups' mental models and can support further investigation of causal connections. Causal mapping is designed for the analysis and visualisation of qualitative data about causal links. It can be used to test an existing theory of change or create collective empirical theories of change about how a program works based on stakeholders\u2019 experiences. People\u2019s narratives and reflections about their experiences provide qualitative data that can be coded and displayed as maps to present the cognitive structures (mental models) of individuals and groups and to support further exploration to understand actual causal connections. These causal maps can help to answer questions about what people think happened and what they think caused this by building links between different factors, such as different kinds of outcomes and inputs. Mapping the chains of results and their linkages builds pictures of causal pathways showing the intermediate steps and connections between them."}, {"title": "Causal mapping has been used for over 50 years in many disciplines", "path": "/010 Causal mapping/0.130 Causal mapping has been used for over 50 years in many disciplines.html", "text": "From Powell, Copestake, et al. (2023 Causal mapping \u2013 diagramming beliefs about what causes what \u2013 has been used since the 1970s across a range of disciplines from management science to ecology. The idea of wanting to understand the behaviour of actors in terms of their internal maps of the world can be traced back further to field theory (Tolman, 1948) which influenced Kelly\u2019s \u2018personal construct theory\u2019 (Kelly, 1955). A seminal contribution was made by Robert Axelrod in political science, with the book The Structure of Decision (Axelrod, 1976). Causal mapping is largely based on \u2018concept mapping\u2019 and \u2018cognitive mapping\u2019, and sometimes the three terms are used interchangeably, although \u2018causal mapping\u2019 strictly involves maps that only include explicit causal links, rather than, for example, relationships like \u2018membership\u2019.3 Axelrod\u2019s book presents a comprehensive idiographic approach to how individuals make decisions which he himself mostly refers to as \u2018cognitive mapping\u2019 (although his definition makes it clear that all links are causal). An appendix to the book (Wrightson, 1976) gives details about how to code causal links. Bougon et al. (1977) applied a similar approach to a study of the Utrecht Jazz Orchestra as an organisational unit, eliciting \u2018cause maps\u2019 from several individual members and amalgamating them. One strand of literature about causal mapping can be located within the wider literature on sensemaking in organisations pioneered by Weick (1995), and applications within organisations were present almost from the start. By 1990, there were many different applications of similar ideas, including an edited book (Huff, 1990) that offered a unitary approach to \u2018concept mapping\u2019 in the United States. Most authors (Ackermann and Alexander, 2016: 892; Clarkson and Hodgkinson, 2005: 319; Fiol and Huff, 1992: 268; Laukkanen, 2012: 2; Narayanan, 2005: 2) use a broadly similar definition of a causal map: A causal map is a diagram, or graphical structure, in which nodes (which we call factors) are joined by directed edges or arrows (which we call links), so that a link from factor C to factor E means that someone (P) believes that C in some sense causally influences E. There is a constructive ambiguity (Eden, 1992) about what a collective map is a map of: While maps constructed as a consensus within a group can plausibly be claimed to map \u2018what the group thinks\u2019, this is more problematic for maps constructed post hoc by synthesising individual maps. We found no significant deviations from this basic definition of a causal map across all the variants of causal mapping reviewed in the following sections, with the caveat that there is variation in how explicit different authors are in describing causal links as representing bare causation as opposed to beliefs about causation. In the following decades, Eden et al. (1992) applied the approach to understanding and supporting decision making in organisations, increasingly using the phrase \u2018causal mapping\u2019 rather than \u2018cognitive mapping\u2019, and they subsequently extended the application of causal maps to fields as varied as risk elicitation and information systems development (Ackermann and Eden, 2011; Ackermann et al., 2014), also developing a series of software packages beginning with Decision Explorer (Ackermann et al., 1996). There is now a wealth of literature on using causal mapping for decision support in organisations (including sophisticated approaches to formalise decision support (Montibeller et al., 2008) and even to rank options (Rodrigues et al., 2017)). Laukkanen (1994, 2012; Laukkanen and Eriksson, 2013) also wrote extensively on causal mapping and developed a software programme called CMAP3 for processing both idiographic and comparative causal maps by importing, combining and analysing factors and links attributed to one or more sources. A broadly similar approach was taken by Clarkson and Hodgkinson (2005) with their Cognizer approach and software."}, {"title": "Causal mapping distinguishes evidence from facts and does not make causal inferences", "path": "/010 Causal mapping/011 Causal mapping distinguishes evidence from facts and does not make ca-7f2975.html", "text": "From Better Evaluation. Causal mapping distinguishes carefully between evidence for a causal link and the causal link itself. It does not provide any specific way to make causal inferences from one to the other. Causal mapping can help the evaluator to identify, code, simplify and synthesise the evidence for causal connections, but the evaluative step to make a judgement about whether one thing in fact causally influences another is left to the evaluator. This method is not useful if each piece of evidence is not clearly identified with a source. Examples As explained on the Causal Mapping website: \" A global causal map resulting from a research project can contain a large number of links and causal factors. By applying filters and other algorithms, a causal map can be queried in different ways to answer different questions, for example to simplify it, to trace specific causal paths, to identify significantly different sub maps for different groups of sources, etc. With certain assumptions, it is possible to ask and answer questions like 'which is the largest influence' or 'which is the most positive effect'. \" The figure below shows a map from the application Causal Map, showing coded causal statements for a project that provided farmers with agricultural training and advice in order to increase crop yields. The map has been filtered to show only outcomes downstream of the influence factor \u2018Agricultural training and advice\u2019. Numbers shown indicate how many times the links were made across all interviews. !Causal map from the Causal Map App showing text boxes connected from left to right by arrows Source: BDSR, 2021, p 4"}, {"title": "When to use and not to use causal mapping", "path": "/010 Causal mapping/012 When to use and not to use causal mapping.html", "text": "From Better Evaluation. When to use causal mapping Causal mapping is useful when seeking to understand the causal pathways influencing the outcomes of programs operating in complex settings. It helps make sense of a program and its context in stakeholders\u2019 own words. This includes providing ways to make sense of and organise the different, but sometimes overlapping, labels that different groups use to describe the causal factors that are important to them. Causal mapping is particularly useful for evaluations that focus on learning to inform program improvement as visual representation of causal links between context, activities and outcomes can help to facilitate the sharing and collaborative use of findings. Causal mapping can be used during a program lifespan to inform adaptive management and as part of a final evaluation. Causal mapping can be used to help make sense of large amounts of qualitative data. Using this method requires expertise in coding and analysis of qualitative data. If you\u2026 have a relatively large amount of narrative data need help to synthesise a large number of links have information from more than one source (for example respondents, documents) are interested in differences between the sources and groups of sources you don\u2019t know the contents or boundaries of the map want to capture what your sources actually say, systematically and transparently When not to use causal mapping Causal mapping is less frequently used to analyse quantitative data or to do precise mathematical modelling, e.g. of future states of a system under certain conditions. If you \u2026 don\u2019t place high value on the views of the sources only have a relatively small map which you can manage with traditional tools for drawing network diagrams (e.g. PowerPoint, kumu.io etc.) need to analyse quantitative data and/or need to do precise mathematical modelling, e.g. of future states of a system under certain conditions would like to sketch out a plan (e.g. Theory of Change or similar) without much reference to the different sources underpinning each link"}, {"title": "History of the Causal Map app", "path": "/010 Causal mapping/013 History of the Causal Map app.html", "text": "From Powell, Copestake, et al. (2023 During 20 years of conducting evaluations, the lead author became persuaded of the central importance of collecting and being able to aggregate causal propositions embedded in written and spoken data. Further interest in causal mapping arose from discussions with the other authors who had developed a qualitative impact evaluation protocol (the QuIP), which relies on causal mapping for analysis of narrative data. This prompted further expansion of the search for literature and software that could assist in systematically constructing causal maps as a way of presenting the outcome of such impact evaluation studies. A main product of this action research has been the design of new software and detailed guidelines for causal mapping, which have already been used in many evaluations around the world."}, {"title": "Causal mapping approaches differ in application, construction, analysis and how they deal with multiple sources", "path": "/010 Causal mapping/0130.0 Causal mapping approaches differ in application, construction, ana-39069f.html", "text": "See also 0.130 Causal mapping has been used for over 50 years in many disciplines 0132 Causal mappers believe that humans are good at thinking in terms of causal nuggets 0132 Causal mappers believe that humans are hardwired to perceive causation From Powell, Copestake, et al. (2023 Table 1 shows some highlights from the extensive literature on causal mapping. Many of the key ideas were already in place by the end of the 1970s. The subsequent literature covers a variety of specific techniques to elicit maps from documents, individuals, sets of individuals and groups, with or without software support, following protocols from the purely open ended to those which use strictly pre defined lists of factors and links (see Hodgkinson et al (2004) for a comparison of methods), and with aims ranging from strictly idiographic (understanding individuals in specific contexts as Axelrod did) to more nomothetic, such as Tegarden et al. (2016). Table 1. Major milestones in the development of the evaluation tool \u2018causal mapping\u2019 \u2013 the collection, coding and visualisation of interconnected causal claims. | Reference | Main Application of Causal Mapping | Mode of Construction | Dealing with Multiple Sources | Analysis Procedures | | | | | | | | Axelrod, 1976 | Understand and critique decision making | Coding documents | Mainly idiographic | Compute polarity of indirect effects in some cases. | | Bougon et al., 1977 | Understand how organisations are constructed and can be influenced. | Semi structured interview to identify a fixed list of factors (\u201cvariables\u201d); respondents indicate links and polarity. | Compare individual maps and combine into global \u201caverage\u201d map. | Identify variables with high outdegree/indegree; construct \u2018etiograph\u2019 to show multiple paths; discuss respondent influence over variables. | | (Ackermann & Eden, 2004, 2011; Eden, 1992; Eden et al., 1979, 1992) | Decision support and problem solving in organisations. Maps as useful tools rather than research about reality. | Open interviewing based on Kelly\u2019s Personal Construct Theory; direct group map construction (1988). | Compare individual maps and analyse group maps directly. | Structural measures, isolated clusters, hierarchical trees, loops; simplify maps by collapsing X\u2192Y\u2192Z into X\u2192Z. | | (Laukkanen, 1994, 2012; Laukkanen & Eriksson, 2013; Laukkanen & Wang, 2016) | Explicitly cognitive, to improve knowledge and understanding in management | Systematic comparative method with semi structured interviewing: anchor topics, causes/effects, standardise factor names; comprehensive coverage of map construction. | Comparative study of individual maps, combine data into database. | Display combined maps for subgroups (e.g., all local managers). |"}, {"title": "Causal mappers believe that humans are good at thinking in terms of causal nuggets", "path": "/010 Causal mapping/0132 Causal mappers believe that humans are good at thinking in terms of -c87577.html", "text": "See also 0.130 Causal mapping has been used for over 50 years in many disciplines From Powell, Copestake, et al. (2023 Renewed interest in causal mapping may also be reinforced by the \u2018causal revolution\u2019 in quantitative data science initiated by Judea Pearl (Pearl, 2000; Pearl and Mackenzie, 2018), which has fundamentally challenged the almost total taboo placed on making or assessing explicit causal claims, which was dominant in statistics for much of the twentieth century (Powell, 2018), and this has in turn helped rekindle interest in explicitly addressing causation using qualitative methods. Causal mapping and most related approaches share the basic idea that causal knowledge \u2013 whether generalised or about a specific case or context \u2013 can be at least partially captured in small, relatively portable \u2018nuggets\u2019 of information (Powell, 2018: 52). These can be assembled into larger models of how things worked, or might work, in some cases. More ambitiously, they may contribute to constructing \u2018middle level theory\u2019 theory, useful for understanding causal processes in other contexts, without necessarily reaching the level of overarching scientific laws (Cartwright, 2020). Causal nuggets are also related to the mechanisms that help to explain how people behave in different contexts (Pawson and Tilley, 1997; Schmitt, 2020). These can be thought of as causal schema and linked to the hypothesis that human knowledge is stored in chunks that are activated and combined with others in relevant circumstances. This would suggest that we humans do not have a comprehensive set of causal maps in our heads at any one time, but we do have a set of more basic components and the ability to assemble them when the situation calls for it, including when prompted by a researcher."}, {"title": "Causal mappers believe that humans are hardwired to perceive causation", "path": "/010 Causal mapping/0132 Causal mappers believe that humans are hardwired to perceive causation.html", "text": "See also 0.130 Causal mapping has been used for over 50 years in many disciplines 0132 Causal mappers believe that humans are good at thinking in terms of causal nuggets From Powell, Copestake, et al. (2023 This approach suggests that our everyday causal understanding is as primary as our perception of, say, colour and arises from more than empirical observations of associations between objects or events; our ability to infer causation goes beyond and is not primarily based on noting correlations. And for all its complexity and intuitive brilliance, it is also just as fallible as our perception of colour or size. This reaffirms our practice as evaluators of taking the causal claims and opinions of humans (experts and non experts) seriously (Maxwell, 2004a, 2004b); indeed, this kind of information is the bread and butter of most evaluations."}, {"title": "Causal mapping is part of the qualitative branch of the new causal revolution", "path": "/010 Causal mapping/0132 Causal mapping is part of the qualitative branch of the new causal r-d0ad71.html", "text": "See also 0.130 Causal mapping has been used for over 50 years in many disciplines From Powell, Copestake, et al. (2023 Renewed interest in causal mapping may also be reinforced by the \u2018causal revolution\u2019 in quantitative data science initiated by Judea Pearl (Pearl, 2000; Pearl and Mackenzie, 2018), which has fundamentally challenged the almost total taboo placed on making or assessing explicit causal claims, which was dominant in statistics for much of the twentieth century (Powell, 2018), and this has in turn helped rekindle interest in explicitly addressing causation using qualitative methods."}, {"title": "Six ways causal mapping differs from related approaches", "path": "/010 Causal mapping/0133 Six ways causal mapping differs from related approaches.html", "text": "From Powell, Copestake, et al. (2023) Most evaluators are probably more familiar with related approaches under the term \u2018systems mapping\u2019, recently covered by Barbrook Johnson and Penn (2022). They provide an overview table of relevant methods on pp. 169 ff. \u2013 fuzzy cognitive maps (FCM), participatory systems mapping (PSM), Bayesian belief networks (BBN), causal loop diagramming (CLD), systems dynamics (SD) and theory of change (ToC) \u2013 which will be briefly mentioned here. SD, CLDs, FCMs and BBNs are all ways to encode information about networks of interconnected causal links and follow formal inference rules to make deductions based on them, for example, to calculate the strength of indirect effects or to predict behaviour over time. The oldest of the three methods, SD (Forrester, 1971), models flows of a substance (for example, of energy or money) within a network over time, whereas the other three methods model \u2018bare\u2019 causal connections between network elements. SD uses general mathematical functions to model the connections and explicitly models non linear relationships. CLDs are related but mathematically simpler, modelling causal effects in a semi quantitative way. FCMs might seem to be of more interest for causal mapping; Kosko\u2019s original article on FCM (Kosko, 1986) takes Axelrod\u2019s work as its starting point. This tradition (Chaib Draa and Desharnais, 1998; Khan and Quaddus, 2004; Taber, 1991) was originally introduced to model causal reasoning (Kosko, 1986: 65): If person or group P believes the set of causal propositions making up a map M, the model attempts to predict the strength with which they could or should also believe some other propositions, for example, about indirect effects and how they might change over time. In practice, however, FCM is less interested in cognition than in making predictions about the world. The difference between FCM and the other three methods is more about the fuzzy logic used to make the predictions rather than about the cognitive nature of the data. BBNs are also designed to make causal inferences by doing calculations with data about causal connections. While FCMs make essentially qualitative predictions such as \u2018increasing\u2019 and \u2018decreasing\u2019, BBNs use directed acyclic graphs (networks without loops) to make quantitative predictions about the probability of events, particularly about the probability that one event was the cause of another. All four approaches are primarily ways to make predictions about causal effects within a network of factors, and (despite the words \u2018cognitive\u2019 and \u2018belief\u2019 in the names of two of the four) the relative lack of interest in who is doing the reasoning sets FCM, BBNs and SD apart from causal mapping as outlined earlier. In the last few years, PSM has featured in several publications in evaluation journals and guides (Barbrook Johnson and Penn, 2021; Hayward et al., 2020; Sedlacko et al., 2014; Wilkinson et al., 2021), alongside mapping of \u2018systems effects\u2019 (Craven, 2020). Indeed, Craven\u2019s work (see also Craven, 2017) can be considered causal mapping with a particular emphasis on systems aspects. Barbrook Johnson and Penn (2022) explicitly exclude causal maps from their overview of systems mapping because they are arguably included via FCM and because they \u2018sometimes emphasise developing representations of individual mental models rather than representations of systems\u2019 (p. 11). Nevertheless, PSM is closer to the tradition of causal mapping (and of more direct interest to evaluators) than the previous four approaches because it is a more concrete and pragmatic intervention to construct a map with specific group of stakeholders to support decisions. A devotee of causal mapping could claim that approaches like PSM are just variants of what they have been doing for the last 50 years, just as a devotee of systems mapping might consider causal mapping as a form of PSM. Finally, logic models and ToC can be considered causal maps in which they make assertions about past or future causal links that one or more stakeholders believe to be important. They are also political artefacts that aim to justify and inform action by establishing an agreed synthesis of multiple perceptions of change and may also gain legitimacy by being the product of an agreed process of participatory planning and co design. They do not, however, normally retain information about which stakeholder(s) believe which claim. Reflecting on logic models and theories of change provides one entry point for thinking more carefully both about who actually makes these claims and about the symbols and rules employed to construct them (Davies, 2018). We think it is useful to distinguish this tradition of causal mapping from related activities in six ways, as set out in the following section. None of these distinctions are definitive, and many are shared with other approaches. To systems people who want to say that causal mapping is just systems mapping and to causal mappers who want to say that systems mapping is just causal mapping (and we have heard both arguments many times), we can only say, perhaps we should all just get to know each other first. First, the raw material for causal maps comprises claims about, perceptions of or evidence for causal links. Causal maps are primarily epistemic, meaning that their constituent parts are about beliefs or evidence, not facts; yet their logic tends to be parallel to, and based upon, the logic of non epistemic systems maps and similar diagrams that are broadly used across a range of sciences. Some systems mapping techniques are also sometimes concerned with stakeholder beliefs; causal mapping does this more systematically. Second, causal maps tend to be unsophisticated about the types of causal connection they encode. To explain this, we should note that causal claims in ordinary language are expressed in an endless variety of ways: \u2018C made E happen\u2019, \u2018C influenced E\u2019, \u2018C may have been necessary for E\u2019, \u2018C was one factor blocking E\u2019, \u2018C had a detrimental effect on E\u2019, \u2018C had a surprisingly small effect on E\u2019 and so on. With a few exceptions, causal mapping analysts do not even try to formally encode this rich and unsystematic range of causal nuance, relying instead simply on the lowest common denominator: A link from X to Y means simply that someone claims that X somehow causally influences or influenced Y. There is one exception: Many causal mapping approaches do accommodate information about the polarity of links, marking each link as either positive or negative, for example, the claim \u2018the recession led to unemployment\u2019 could be coded as a negative link from \u2018the recession\u2019 to \u2018employment\u2019. In general, causal maps usually encode a belief about partial causal influences of C on E and only in special cases do they encode total or exclusive causation such that C entirely determines E. This also means that encoding a claim does not require us to make any judgement about the quality of the evidence or the ability of the source to judge that this link was causal (although it may be very useful to do so). Figure 1. Combining two separate single source causal maps into a multi source map: an illustrative example. !A diagram of a diagram Description automatically generated Third, causal mapping often handles large numbers of causal claims, sometimes many thousands. Handling large numbers of claims en masse in this way is made much easier because of the relatively unsophisticated nature of the way claims are coded (as discussed earlier). Related approaches in evaluation tend to bring more sophisticated tools to bear on a much smaller number of causal links. In process tracing, for example, researchers may produce diagrams depicting claims about causal links but tend to focus on testing the strength of a relatively small number of specific \u2018high stakes\u2019 causal links, whether through verbal reasoning, application of Boolean logic or Bayesian updating (Befani and Stedman Bryce, 2017). Fourth, causal maps may originate from one or many sources, each reporting on one or many cases. In a causal map, the links all originate from one person or document a \u2018single source\u2019 or \u2018individual\u2019 or \u2018idiographic\u2019 causal map, as in Axelrod\u2019s original work (Axelrod, 1976). But we can also draw causal maps that incorporate information from a variety of different sources, as illustrated in Figure 1. The simplest causal maps refer to only one context and contain information from only one source (which may be the consensus view of several people, treated as speaking with a single voice). Various forms of systems mapping such as PSM could be understood as a special case of causal mapping in this sense. There are many other variants. One source might give differentiated information about different cases or contexts, or many sources might give information about just one context, as when different water systems experts each give their (possibly differing) opinion about the same water catchment area, for example. Figure 2. From text to causal mapping via coding, an illustrative example. Another frequent type of causal map is drawn from many sources, each reporting on their own situation or context, such as their perception of drivers of change in their own lives. In coding and analysis of this sort of data, one source equals one case and one context; these can subsequently be aggregated across many sources who, for example, all share a similar context. Fifth, causal maps do not necessarily specify a clear system boundary. The boundaries of a causal map are usually defined more loosely, partly by data collection but also by the sources themselves. Indeed, some systems proponents would say that the term \u2018systems diagram\u2019 simply signals a readiness to use systems approaches (Williams, 2022). Finally, causal mapping, especially in management sciences and operations research, has nearly always been at least as interested in process as in the result. There is often a focus on the process of reaching consensus as part of the task of solving a business problem, rather than on the universal accuracy or validity of the final map."}, {"title": "What is the difference between outcome harvesting, contribution analysis and causal mapping", "path": "/010 Causal mapping/01331 What is the difference between outcome harvesting, contribution ana-9cd115.html", "text": "Outcome harvesting is about collecting and explaining a (hopefully long and substantial) list of intended and unintended outcomes after the fact, and identifying how the programme contributed to the outcomes. But it's just like a list of cause effect relationships. It can be a challenge to understand how those causes and effects overlap with one another or influence one another. Contribution analysis is about testing and refining a theory of change to build a credible case for contribution. It's focused on a specific theory of change and on the contributions to outcomes. Causal mapping is quite similar to both. It is not an evaluation method in its own right, but more of a tool which can assist with either. it's about visualizing and interrogating the whole web of causes and effects. It explicitly addresses the challenge of overlap and influence between causes and effects. It doesn't rely on a fixed theory of change but on the factors and links between them which are actually mentioned in documents and interviews. How causal mapping can help with Outcome Harvesting: causal mapping can input a pile of Outcome Harvesting data and link them all together into the form of a larger web. How causal mapping can help with Contribution Analysis: And it can assist in contribution analysis by helping to assemble all the relevant evidence along some causal pathway or pathways from intervention to outcome. Contribution Analysis can then focus on what it is best at, namely weighing up different explanations for an outcome and how much our intervention really contributed to it."}, {"title": "Janus - Causal mappers face in two directions", "path": "/010 Causal mapping/0134 Janus - Causal mappers face in two directions.html", "text": "From Powell, Copestake, et al. (2023) ..., like Janus, the causal mapper looks in two directions at once: sometimes interpreting maps as perceptions of causation but also often wanting to make the leap to inferences about actual causation. As Laukkanen and Wang (2016: 3) point out, while conceptually poles apart, in practice, the two functions can be hard to distinguish, particularly without sufficient explanation about source information and how this has been analysed. We see the job of the causal mapper as being primarily to collect and accurately visualise evidence from different sources, often leaving it to others (or to themselves wearing a different hat) to draw conclusions about what doing so reveals about the real world. This second interpretative step goes beyond causal mapping per se (Copestake, 2021; Copestake et al., 2019a; Powell et al., 2023). 0135 Epistemic logic ! Questions you can answer"}, {"title": "Epistemic logic", "path": "/010 Causal mapping/0135 Epistemic logic.html", "text": "From Powell, Copestake, et al. (2023) Seen as models of the world, causal maps, like systems maps, are fallible but useful: We can use inference rules (which are explicitly set out in FCMs, SDs, BBNs and CLDs and are implicit in other related approaches) to make deductions about the world. Seen as models of individuals\u2019 causal beliefs, we can arguably use analogous rules, perhaps also including rules from epistemic logic, to make deductions about what individuals ought to believe."}, {"title": "Causal mapping in evaluation - Three tasks", "path": "/010 Causal mapping/0136 Causal mapping in evaluation - Three tasks.html", "text": "From Powell, Copestake, et al. (2023) Causal mapping offers ways to organise, combine, present and make deductions from a large number of relatively unstructured causal claims \u2013 the sort of data that are often collected in evaluations. Different approaches to these three tasks are discussed in turn in the following sections. 01361 Task 1 Gathering narrative data 0137 Task 2 Coding causal claims as causal qualitative data analysis 0138 Task 3 Answering questions, specifically evaluation questions"}, {"title": "Task 1 - Gathering narrative data", "path": "/010 Causal mapping/01361 Task 1 - Gathering narrative data.html", "text": "From Powell, Copestake, et al. (2023) How to collect causal claims from which to draw causal maps? There are a wide variety of options, including in depth individual interviews (Ackermann and Eden, 2004), reuse of open ended questions in structured surveys (Jackson and Trochim, 2002), literature reviews (in which \u2018sources\u2019 can be documents rather than individuals) and archival or secondary material within which pre existing causal claims are already made (Copestake, 2020). Other approaches aim to build consensus by using structured collaborative processes, including Delphi studies and PSM (Penn and Barbrook Johnson, 2019). Guidelines for causal mapping may include procedures for collecting primary data, with forms of elicitation including back chaining (\u2018what influenced what?\u2019) and forward chaining (what resulted, or could result, from this?) When gathering primary data, the way in which questions are asked influences the meaning of the maps and their links. For example, in the QuIP, (Copestake et al., 2019b) respondents are asked to identify causes of changes, then causes of the causes and so on. This means that most of the factors are not expressed directly as variables that may go up or down (e.g. \u2018harvest\u2019, \u2018hunger\u2019) but already as changes in something, such as \u2018an improved harvest\u2019 or \u2018reduced hunger\u2019). This has implications for how positive and negative statements are combined, as discussed in the following section. With primary data collection, we can distinguish between relatively closed and open approaches and whether respondents are forced to choose between pre selected optional answers or can formulate their own (see Table 2). Interviewers may also be guided by a chaining algorithm; for example, they may be instructed to iteratively ask questions like \u2018You mentioned X, please could you tell me what were the main factors that influenced X or led to it happening.\u2019 Table 2. Different approaches within primary data collection for causal mapping, with example questions. | Admissible answers / Scope of questions | Explicit : factors are explicitly identified | Implicit : factors are not explicitly named | | | | | | Closed: questions with a predetermined focus | Which factors in this list influenced this particular event? | What influenced this particular event? | | Open: a freer discussion | Identify the biggest change you experienced in relation to X, and list three factors that influenced it | Tell me what has changed for you in the last x years |"}, {"title": "Task 2 - Coding causal claims as causal qualitative data analysis", "path": "/010 Causal mapping/0137 Task 2 - Coding causal claims as causal qualitative data analysis.html", "text": "From Powell, Copestake, et al. (2023) Some approaches such as that suggested by Markiczy and Goldberg (1995) directly elicit causal links from their sources, perhaps by asking respondents to suggest causal links between a predetermined list of causal factors, and thus, after finishing Task 1, are already in a position to create causal maps. More explicitly, qualitative approaches are faced with Task 2: encoding causal claims in the form of explicit causal links and factors. This task is similar to ordinary qualitative data analysis (QDA), whether done manually or using tools like NVivo, Dedoose and AtlasTI. However, these tools are designed to capture general concepts, rather than claimed causal links between concepts, which is what we need for causal mapping. QDA for causal mapping also starts with a corpus of narrative data, but it does not create causal links between independent concepts that might already have been coded using ordinary non causal thematic analyses. Rather, in causal QDA, the primary act of coding is to highlight a specific quote from within a statement and identify the causal claim made by simultaneously identifying a pair of causal factors: an \u2018influence factor\u2019 and a \u2018consequence factor\u2019. The causal factors only exist as one or other end of a causal link and have no meaning on their own. Each claim forms a link in the visual representation of the causal map. The Axelrod school had its own coding manual describing how to highlight areas of text expressing causal connections and code them as links between causal factors, originally inspired by evaluative assertion analysis (Osgood et al., 1956). Manual causal coding of text data, like ordinary thematic coding, requires a considerable investment of time and expertise to do well. We now use natural language processing to at least partially automate this; however, the process is essentially the same, and discussion of this is beyond the scope of the present article. Where do the labels for the causal factors come from? As with ordinary QDA and thematic analysis (Braun and Clarke, 2006), approaches vary in the extent to which they are purely exploratory or seek to confirm prior theory (Copestake, 2014). Exploratory coding entails trying to identify different causal claims embedded in what people say, creating factor labels inductively and iteratively from the narrative data. Different respondents will not, of course, always use precisely the same phrases, and it is a creative challenge to create and curate this list of causal factors. For example, if Alice says \u2018Feeling good about the future is one thing that increases your wellbeing\u2019, is this element \u2018Feeling good about the future\u2019 the same as \u2018Being confident about tomorrow\u2019 which Bob mentioned earlier? Should we encode them both as the same thing, and if so, what shall we call it? We might choose \u2018Positive view of future\u2019, but how well does this cover both cases? Laukkanen (1994) discusses strategies for finding common vocabularies. As in ordinary QDA, analysts will usually find themselves generating an ever growing list of factors and will need to continually consider how to consolidate it \u2013 sometimes using strategies such as hierarchical coding or \u2018nesting\u2019 factors (as discussed in the following section). The alternative to exploratory coding is confirmatory coding, which employs an agreed code book, derived from a ToC and/or from prior studies. QuIP studies mostly use exploratory coding but sometimes supplement labels with additional codes derived from a project\u2019s ToC, for example, \u2018attribution coding\u2019 helps to signify which factors explicitly refer to a specific intervention being evaluated (Copestake et al., 2019b: 257). However, careful sequencing matters here because pre set codes may frame or bias how the coder sees the data (Copestake et al., 2019a). Again, the positionality of the coder matters just as much when doing causal coding as it does for any other form of qualitative data coding. Combining Tasks 1 and 2 Tasks 1 and 2 result in a coded data set of causal claims, each of which consists of (at the very least) the labels for a pair of causal factors. Those using a more explicit elicitation approach have been able to skip Task 2."}, {"title": "Task 3 - Answering questions, specifically evaluation questions", "path": "/010 Causal mapping/0138 Task 3 - Answering questions, specifically evaluation questions.html", "text": "From Powell, Copestake, et al. (2023) The extensive causal mapping literature provides many examples of its use to answer evaluation questions (see Powell, Copestake, et al., 2023, p. 110), for example: Getting an overview of respondents\u2019 \"causal landscape\". This can be useful for orientation or for particular tasks like triaging masses of information to identify key outcomes and possible causal pathways when planning an Outcome Harvesting (Wilson Grau and Britt, 2012) or Process Tracing (Befani and Stedman Bryce, 2017) project. Weighing up evidence about contribution: in particular, tracing back and comparing the possibly multiple contributory causes of an important outcome or consequence (Goertz and Mahoney, 2006), or examining effects of causes. Reporting key metrics of the causal network, for example, to reveal which factors are most central in the whole network or to identify feedback loops. Asking whether the empirical ToC matches the plan (Powell, Larquemin, et al., 2023, p. 7). Making comparisons between groups or across timepoints. !Answering EVALUATION questions One way to simplify is to derive from the global map several smaller maps that focus on different features of the data. For example, maps may selectively forward chain the multiple consequences of a single cause \u2013 including those activities being evaluated: effects of causes (Goertz and Mahoney, 2006) \u2013 or trace back to the multiple contributory causes of an anticipated or highly valued outcome or consequence: causes of effects. A series of simpler causal maps, each selected transparently to address a specific question, generally adds more value to an evaluation than a complicated, if comprehensive, single map that is hard to interpret. The downside of this is that selectivity in what is mapped and is not mapped from a single database opens up the possibility of deliberate bias in selection, including omitting to show negative stories. !Simplification co terminal link bundles Sets of individual links with the same influence and consequence factor (co terminal links) are usually represented bundled together as a single line, often with thickness of the line indicating the number of citations, and/or with a label showing the number of links in the bundle. The map has not fundamentally changed, but the visualisation is much simpler. !Simplification factor and link frequency Another way to simplify a global causal map is to produce an overview map showing only the most frequently mentioned factors and/or links. Care should be taken if this leads to omitting potentially important but infrequently mentioned evidence about, for example, an unintended consequence of an intervention. !Simplification hierarchical zooming Another common way to simplify is to combine sets of very similar factors into one. For example, if hierarchical coding has been used, it is possible (with caveats) to \u2018roll up\u2019 lower level factors (such as health behaviour; hand washing and health behaviour; boiling water) into their higher level parents (health behaviour), rerouting links to and from the lower level factors to the parent (Bana e Costa et al., 1999). !Reporting global and local network statistics Large causal maps can also be analysed quantitatively, including by tabulating which factors are mentioned most often, identifying which are most centrally connected or calculating indicators of overall map density, such as the ratio of links to factors (Klintwall et al., 2023; Nadkarni and Narayanan, 2005). We are wary of the value of summarising maps in this way, not least because results are highly sensitive to the granularity of coding. For example, although a specific factor such as \u2018improved health\u2019 might have been mentioned most often, if two subsidiary factors had been used instead (such as \u2018improved child health\u2019 and \u2018improved adult health\u2019), these two separate factors would not have scored so highly."}, {"title": "Limitations - data quality", "path": "/010 Causal mapping/01381 Limitations - data quality.html", "text": "From Powell, Copestake, et al. (2023) Causal mapping has some limitations. First, the credibility of the causal arguments which can be derived from a map is limited by the credibility of the original data sources. We see the job of causal mapping as collecting, organising and synthesising a large number of claims about what causes what; drawing conclusions about what this actually reveals about the world is a final step that goes beyond causal mapping per se. In specific cases, establishing explicit and context specific rules of inference may help to make this final step. For example, it might be agreed that a reasonable threshold of evidence that C influenced E is that (i) a specified number or proportion of respondents independently mentioned the link; (ii) the connection can be plausibly explained theoretically and (iii) researchers anticipated confirmation bias and other potential sources of bias and took adequate steps to mitigate against them. See Tegarden et al. (2016) for a discussion of the role of anonymity in causal mapping in organisations."}, {"title": "Dangers - granularity and generalisability", "path": "/010 Causal mapping/01382 Dangers - granularity and generalisability.html", "text": "From Powell, Copestake, et al. (2023) !A diagram of a diagram Description automatically generated An illustrative example A positive feature of causal maps, illustrated by Figure 3, is that they capture a lot of information in a way that is quick and easy to understand. This example reveals that Source S provided a narrative that connects the intervention to improved feeling of wellbeing as a direct consequence of taking more exercise and via the effect of this on their health. This source also suggests a positive feedback loop, with more exercise making them more physically fit and encouraging even more exercise. The information from Source T is more fragmented; there are two causal statements claiming that improved feeling of wellbeing can result from more exercise and improved health, although T does not link the two causally, nor make any causal link back to the intervention. In addition, T suggests that an additional factor, \u2018more confidence in the future\u2019, also contributes to improved feeling of wellbeing. The two sources of evidence do agree on certain points; there is scope for generalisation beyond either individual source (and can be scaled up from here), both in assessing the multiple outcomes of the intervention and in understanding what explains improved feeling of wellbeing. Generalisability is strengthened when a link is reported by different sources in different contexts. We believe that within causal mapping, we should never make the mistake of thinking that stronger evidence for a causal link is evidence that the causal link is strong; only that there is more evidence for it. The example also reveals some weaknesses of causal maps. First, there is ambiguity about the precise meaning of the labels and the extent to which their use is conceptually equivalent between the two sources. There is also ambiguity about whether they are referring to their own personal experience (and if so, over what period) or speaking in more general terms. Furthermore, the diagram sacrifices details, including how the statements shown relate to the wider context within which each source is situated. To mitigate this, an important feature of any causal mapping procedure is how easily it permits the user to trace back from the diagram to the underlying transcripts and key information about the source (e.g. gender, age, location etc.). Where this is possible, the diagram can be regarded in part as an index or contents page \u2013 an efficient route to searching the full database to pull out all the data relating to a specific factor or causal link."}, {"title": "Advantages of causal mapping - coping with messiness", "path": "/010 Causal mapping/Advantages of causal mapping - coping with messiness.html", "text": "From Powell, Copestake, et al. (2023) Key to doing so is recognising head on the ambiguity of much narrative causal data, particularly when confronted with large bodies of data collected in disparate ways. Evaluators must contend with messiness: imprecise system boundaries, differing specification of claimed causal influences and lack of clear or consistent information about what case or group of cases claims refer to. Causal mapping can contend with all this ambiguity rather than shying away from it. It can make use of messy operational data, treating urgent, unexpected and unstructured information at face value. This is made possible by distinguishing clearly between two analytical steps in evaluation: The first is to gather, understand and assemble causal evidence from different sources (those in a position to have useful evidence about relevant causal links and chains) to construct, compare and contrast the evidence for and against different possible causal pathways. By focusing on this task, causal mapping lays a more reliable foundation for the second, often critical, task of using the assembled data to make judgements about what is in fact really happening. This avoids the confusion and ambiguity that often arises when evaluators seek to address both steps simultaneously by constraining what data are collected to fit a prior view of reality which other stakeholders may or may not share."}, {"title": "Causal maps are knowledge graphs, but with wings", "path": "/010 Causal mapping/Causal maps are knowledge graphs, but with wings.html", "text": "What is a Knowledge Graph? \ud83e\udde0 A knowledge graph is like a giant mind map for a computer . It stores information not as text in a document, but as a network of interconnected facts. It's built from two main things: entities (the \"nodes,\" representing real world objects, people, or concepts like \"Paris\" or \"Photosynthesis\") and relationships (the \"edges,\" describing how these entities are connected, like \"is the capital of\" or \"is a process in\"). A single fact has three parts: (Subject) [Relationship] (Object) . For example: . Why are knowledge graphs specially useful in the age of AI? \ud83d\udca1 They create structure from chaos. AI can read through millions of pages of unstructured text (like news articles or scientific papers) and pull out these factual triplets. This turns a messy sea of words into an organized, queryable database of knowledge. They enable smarter searching and reasoning. Instead of just searching for keywords, you can ask complex questions that require understanding the relationships between things. For example, \"Which scientists who won a Nobel Prize also discovered an element?\" A computer can navigate the graph's connections to find the answer. They provide essential context. A knowledge graph helps an AI understand that \"Apple\" in a tech article is a company linked to \"Steve Jobs,\" not the fruit. By looking at its connections, the AI gets the right context, which is crucial for accurate understanding and analysis. ! Image from Wikipedia by Jayarathina Own work, CC BY SA 4.0. Why are Knowledge Graphs (KGs) so useful? A major benefit of KGs is we can then apply network logic like transitivity rules to answer meaningful questions. For example, if the relation is \"works in the same company as\", then if we know A is related to B and B is related to C, then we can conclude A is related to C (A is in the same company as C). Challenges with general purpose knowledge graphs The trick in constructing knowledge graphs is to know what relationship(s) to look for. \"belongs to? \" \"is capital of?\" \"challenges/undermines?\" This can be very difficult to decide. on the fly. Using network logic to answer queries can be difficult where each different type of relationship may have its own logic. It can be very tricky (though potentially rewarding and useful) to design custom queries to answer specific questions. Causal mapping gives knowledge graphs wings ! A causal map is just a knowledge graph in which there is only one kind of relation: \"causes\" or \"influences\". This means: It is much easier to scan and process text data as we already know what we are looking for. We focus on primarily on exactly the kind of information which is useful for monitoring and evaluation : what influences what? We can make use of pre existing logic and queries to help answer common evaluation questions almost \"out of the box\". Here we have a whole presentation on Questions you can answer with causal mapping which gives plenty of suggestions. Can only doing causal mapping answer all the questions you might want to ask about a text? Of course not. But it can help answer a lot of the most interesting and important ones. What about social network analysis? Yes, social networks can also be constructed as knowledge graphs with just one (or a small number of) relationships, such as \"works with\". So can we use causal mapping tools to construct general network graphs? You might ask if the reverse is also true: can you use causal mapping software like Causal Map to also do your AI supported knowledge graphing for you? The answer is yes! Concretely, in the new version of Causal Map, version 4, which is arriving very soon, you can manually code any type of link, not just causal, and you can also guide the AI to do this too."}, {"title": "Contribution criteria in advance", "path": "/010 Causal mapping/Contribution criteria in advance.html", "text": "https://www.linkedin.com/pulse/criteria good contribution hard steve powell pf72e/ I just read Julian King 's response to a blog by Drew Koleros and Michael Moses . Drew and Michael were saying, these complex evaluations are so hard ; we have to go beyond simply measuring against pre determined criteria, partly because everything changes. Julian responded by saying that to evaluate the value or worth of something, we can and should not just measure but establish criteria . Even in complex situations establishing then applying criteria isn't so hard, if you pick the right kind of criteria (not too specific and not too vague). I think he's arguing also that to be a genuine assessment of value, the criteria should be established in advance a key element of accountability which addresses the summative purpose of evaluation. I think it is this which Drew and Michael are saying is so hard. Here is the first part of Julian's example criterion for a public transport initiative aimed at nudging residents\u2019 attitudes to cars and driving. The transport system demonstrates a meaningful shift toward diverse, accessible, and sustainable mobility modes, reducing car dependency while increasing uptake and satisfaction with public and active transport, including rapid transit, walking and cycling. The optimal mix and sequencing of transport options is arrived at through genuine collaboration with stakeholders, and informed by sound analysis provided by transport experts. All good. But this example is all about assessing progress in outcome areas, wherever that comes from. The really hard part in evaluation is assessing a project's contribution to progress, which is (I think) what Drew and Michael are talking about. The criterion has to cover both the outcome and the contribution at the same time. That's the part that's hard. You'd think Thomas Aston 's contribution rubrics would solve this problem: together with stakeholders, set criteria for: the significance of the outcome, the level of the contribution and the strength of the evidence, .... and then do the assessment and combine the three scores. That's also great but it's designed for assessing contribution post hoc, as in Outcome Harvesting so it doesn't answer Julian's implicit accountability plea to establish criteria in advance. So an evaluation criterion, in advance, would have to look something like this: The initiative will show a meaningful and well evidenced contribution to the transport system taking a meaningful shift toward diverse, accessible, and sustainable mobility modes... That's not very elegant. Here's where evaluation criteria and progress criteria diverge. You could put Julian's example above the door of the initiative's office to remind staff what the aim is. Like a Vison in Outcome Mapping. I wouldn't the evaluation criterion above anyone's door.https://www.linkedin.com/pulse/criteria good contribution hard steve powell pf72e/"}, {"title": "Layout", "path": "/010 Causal mapping/Layout.html", "text": "Can you spot a complex system when you see one? Version 1 !Pasted image 20250910083533.png The network pictured above, even though it is quite small, looks pretty tangled. We're not going to fully understand it, so we'd better get out our tools for dealing with complexity? But wait, look at the boring, old fashioned hierarchy below. Version 2 ! Did you spot that they have exactly the same structure? Now it is easier to see that it is just a hierarchy. D, E and F have one contributor each, whereas G and H share I, J and K as contributors, and feed only into B, whereas D, E and F all feed into both B and C, which feed into A. Easy. Nothing which should be too hard to predict, no balancing feedback loops. \"Complex\" and \"System\" are very buzzy buzz words at the moment. We should check we don't throw them around too much without thinking. I'm just reading Moore, Parsons and Jessop in the American Journal of Evaluation. They quote Magee and de Weck (2004) who define complex systems as systems \"with numerous components and interconnections, interactions or interdependence that are difficult to describe, understand, predict, manage, design, and/or change.\" Well yes, kinda. But what if you find a system difficult to describe, etc, just because you didn't look hard enough? Yes, causal maps are just concept maps with only one type of connector, and that connector means \"... causes....\". Whereas concept maps can have any type of connector you like. Historically, causal maps come from concept maps. Laying out causal maps is a challenge! Most folks from the systems tradition like swirly circular layouts which make them look like everything is one big feedback loop. If there is a more linear structure, we recommend showing that linear structure. But how? Causal Map primarily uses the Graphviz DOT layout engine which does an amazing job of teasing out such a story if there is one. Generally speaking, the \"drivers\" will be on the left and the outcomes will be on the right, but at the same time trying to maintain readability and avoid the links crossing over factors or over each other. Which is always a compromise. For this reason we also usually use \"outcomeness\" colouring for the factor backgrounds, which represents the proportion of all the factor's links which are incoming links. So normally factors on the right are darker, except where Graphviz has had to reposition some of the factors for readability. So, does it look like a ToC? Of course that depends on what you expect a ToC to look like and famously there are no standards for that. As you imply, if the causal map is actually just neatly hierarchical, then our map will reflect that nicely and therefore \"look like a ToC\" but of course that's rarely the case. We often see that for reports, folks often take the original causal map and get a designer to redraw them anyway to match the report styling etc. In the upcoming version 4 of Causal Map, there is a more interactive style of map where it's possible to drag the factors around to put them just where you want them."}, {"title": "There has always been complexity", "path": "/010 Causal mapping/There has always been complexity.html", "text": ""}, {"title": "Answering EVALUATION questions (1)", "path": "/011 Algorithms and questions/Answering EVALUATION questions (1).html", "text": "Task 3: Answering evaluation questions Causal maps help us to assemble evidence for the causal processes at work in specified domains, including the influence of activities being evaluated. They can also help expose differences between the evidence given by different sources and differences between the analysed data and theories of change derived from other sources, including those officially espoused by the commissioner of the evaluation (Powell et al., 2023). The identification of differences in understanding can then feed into further enquiry, analysis and action concerning why people have different views, what the implications of this are and how these might be addressed. Focusing on causal claims is of course only one way of answering evaluation questions from a corpus of text data. But it is productive because many evaluation questions are at least partly about causation and causal contribution, and we have found that causal mapping points to possible answers to these questions relatively rapidly compared to more generic QDA approaches. Answering questions about efficiency, effectiveness, impact and sustainability, for example, all depend on identifying the causal effects of a specific intervention, be they perceived as positive or negative, intended or unintended (OECD, 2010). Even \u2018relevance\u2019 can have a causal interpretation in the sense that an intervention is relevant if it is doing the right thing: Whether it is likely to help to address the needs of stakeholders is at least partly a judgement about its causal powers. For a data set comprising hundreds or thousands of links, an unfiltered global map of all the links is a bewildering and useless \u2018hairball\u2019 that includes everything but highlights nothing."}, {"title": "Distinguishing between factors which make causal claims about different groups", "path": "/011 Algorithms and questions/Distinguishing between factors which make causal claims about different groups.html", "text": "Remember that whatever you do, Causal Map always knows where the claims come from , so you can always do things like \"show me only sources and links from group B\" or \"show me only links with information from group B\". Different ways you could differentiate the groups which the information applies to : Use different labels for each group, like \"Professionals give training\" vs \"Beneficiaries have improved confidence\". This makes most sense if you think the causal maps for each group are basically quite different with different contents. If you think the material overlaps for each group like \"Professionals have improved confidence\" vs \"Beneficiaries have improved confidence\", you could: Keep the labels separate Use hierarchical labels like \"Improved confidence; professionals\" so you can show/hide the group by zooming in or out Use tags in square brackets like \"Improved confidence [professionals]\" so you can temporarily remove the tags when required Use tags on the links (rather than on the factor labels) to show the group which the information refers to so you can show or hide those groups."}, {"title": "Etiograph - robustness", "path": "/011 Algorithms and questions/Etiograph - robustness.html", "text": "From Powell, Copestake, et al. (2023) Figure 3. An illustrative example of a very simple causal map. Even with a simple example like this, we can answer many questions by visually examining the paths. But analysis of larger data sets might be simplified by selecting only all the paths between a selected cause and consequence to produce what Bougon et al (1977) called an \u2018etiograph\u2019. Eden et al. (1992) go so far as to collapse some causal paths into individual links, simply removing intervening factors. Our causal map software uses the \u2018maximum flow/minimum cut\u2019 algorithm (Erickson, 2019), which quantifies the robustness of longer paths from C to E by calculating the minimum number of causal claims, which would have to be invalidated or lost to remove any possible causal pathway between them. This is simply the idea that the strength of an argument is dependent on the strength of its weakest link, extended to apply to an interconnected network rather than a single chain. In other words, we can express a path through a causal map as a possible argument: An argument can be constructed that C causally influences D, and then E. This also provides ways to formally address questions such as \u2018how robust is the evidence for the influence of C on E, compared to evidence for the influence of B on E?\u2019. As our causal evidence will often be of varying quality and reliability, we are advised to also construct and compare paths that consist only of evidence from the most reliable sources."}, {"title": "Reporting global and local network statistics", "path": "/011 Algorithms and questions/Reporting global and local network statistics.html", "text": "Reporting global and local network statistics"}, {"title": "Simplification - co-terminal link bundles", "path": "/011 Algorithms and questions/Simplification - co-terminal link bundles.html", "text": "Simplification co terminal link bundles"}, {"title": "Simplification - factor and link frequency", "path": "/011 Algorithms and questions/Simplification - factor and link frequency.html", "text": "Simplification factor and link frequency"}, {"title": "Simplification - hierarchical zooming", "path": "/011 Algorithms and questions/Simplification - hierarchical zooming.html", "text": "Simplification hierarchical zooming"}, {"title": "Source clustering", "path": "/011 Algorithms and questions/Source clustering.html", "text": "From Powell, Copestake, et al. (2023) Source thread tracing Where there are many sources, it may also be useful to identify variation in which sub groups support different possible arguments. For example, Markiczy and Goldberg (1995) use dimension reduction techniques on the table of all links reported by all sources to identify clusters of sources so that the members of each cluster are maximally similar to one another in terms of the links they report, and so that the clusters taken as a whole are maximally different from one another. These groups can also be cross tabulated with existing metadata, to interpret them as, for example, young city dwellers versus older rural residents."}, {"title": "Source thread tracing", "path": "/011 Algorithms and questions/Source thread tracing.html", "text": "From Powell, Copestake, et al. (2023) Etiograph robustness We prefer to talk about possible arguments here because this sidesteps the (also interesting) question of whether any individual source made any such argument in its entirety, with all its constituent links.9 In many circumstances, evidence for a causal path derived from different sources and contexts can be considered to strengthen the argument, whereas heaps of evidence from the same source will not. To address this kind of issue directly, we can use a complementary measure \u2018source thread count\u2019 as a measure of the strength of the argument from C to E: the number of sources, each of which mentions any complete path from C to E."}, {"title": "The transitivity trap", "path": "/011 Algorithms and questions/The transitivity trap.html", "text": "From Powell, Copestake, et al. (2023) 01382 Dangers granularity and generalisability Source clustering The transitivity trap Transitivity is perhaps the single most important challenge for causal mapping. Consider the following example. If source P [pig farmer] states \u2018I received cash grant compensation for pig diseases [G], so I had more cash [C]\u2019, and source W [wheat farmer] states \u2018I had more cash [C], so I bought more seeds [S]\u2019, can we then deduce that pig diseases lead to more cash which leads to more seed (G \u00e0 C \u00e0S), and therefore G \u00e0 S (there is evidence for an indirect effect of G on S, i.e. that cash grants for pig diseases lead to people buying more seeds)? The answer is of course that we cannot because the first part only makes sense for pig farmers, and the second part only makes sense for wheat farmers. In general, from G \u00e0 C (in context P) and C \u00e0 S (in context W), we can only conclude that G \u00e0 S in the intersection of the contexts P and W. Correctly making inferences about indirect effects is the key benefit but also the key challenge for any approach which uses causal diagrams or maps, including quantitative approaches (Bollen, 1987). For want of a nail the shoe was lost, For want of a shoe the horse was lost, For want of a horse the rider was lost, For want of a rider the battle war lost, For want of a battle the kingdom was lost, And all for the want of horseshoe nail. (Thanks to Gary Goertz for remembering this one!) !notion image Frog thinks: eating salad leads to health (less scurvy), and health (general fitness) leads to better sprinting ability, therefore if I eat this yummy lettuce \u2013 AARGH! One of the key features of causal maps is that you can draw inferences, make deductions, from them. One of the most exciting is to be able to trace causal influences down a chain of causal links. BUT, when you are drawing conclusions from causal maps, beware of the transitivity trap: from B \u2192 C and C \u2192 E we can only conclude B \u2192 E in the intersection of the contexts of 1 and 2 \u2026 and in general with any causal mapping, you\u2019ll never be sure that these two contexts do intersect. You actually have to look at each chain and think about it, and hope you\u2019ve been told all the relevant facts. For example: If Source P [pig farmer]: I received cash grant compensation for pig diseases (G), so I had more cash (C) and Source W [wheat farmer]: I had more cash (C), so I bought more seeds (S) can we deduce G \u2192 C \u2192 H and therefore G \u2192 S (cash grants for pig diseases lead to people buying more seeds)? No, we can\u2019t, because the first part only makes sense for pig farmers and the second part only makes sense for wheat farmers. There are thousands of different kinds of transitivity trap. It isn\u2019t just a problem across subgroups of people. It can apply for example in different time frames. If Child does well in year 13 (A) \u2192 Child has improved academic self image (C) and Child has improved academic self image (C) \u2192 Child does better in year 9 (D) can we deduce A \u2192 C \u2192 D and therefore A \u2192 D (child doing well in year 13 leads to child doing well in year 9)? Of course not even though these claims might be true of the same child. The problem arises as soon as we generalise one causal factor to apply to different contexts. We have to do this, to make useful knowledge. But there are always pitfalls too. Not just a problem for causal mapping This is also true, isn\u2019t it, of any synthetic research / literature review? And in statistics, knowing the effects from B \u2192 C and C \u2192 E means you can calculate the indirect effect of B on E but not the direct effect. You have to have additional data just for that. This is one source of various so called paradoxes in statistics. Can we mitigate the trap with careful elicitation protocols? Sometimes, we might know that all the information in one particular chain came from the same source, and all this information was explicitly given as a series of explanations of the factor which was initially in focus. But even here, we have to be careful. We might have to ask again, having reached the end of the chain, \u201cdid B really influence C which influenced D which influenced E? Was this all part of the same mechanism?\u201d Are we sure we know exactly what we mean by this, and are we sure that our respondents do too? In any case, part of the point of causal mapping is the synthetic surprises which we can discover by piecing together fragments of causal information which were not necessarily provided in this way. This is the situation every evaluator is in when piecing together information from, say, experts for Phase 1 and experts for Phase 2. We just always have to be aware of the transitivity trap. Transitivity trap, or identity trap? We can talk about the identity trap as more fundamental than the transitivity trap. It comes down to saying, how can you be sure that the way in which this factor is exemplified in one particular context is the same as the way that this similar seeming factor is exemplified in a different context: whether to use \u201cthe same\u201d factor to code two different things."}, {"title": "Open-source causal mapping functions", "path": "/012 Causal Map app/500 Open-source causal mapping functions.html", "text": "Open source causal mapping functions These functions are the heart of the analysis engine in our app CausalMap3. The app itself is free to use for analysing existing causal mapping data files but it is closed source, so the functions are a reproducible window into the algorithms and can be used by anyone who wants to analyse and visualise causal mapping data without using the Causal Map app. When we get a chance, we will release them as a full R package. But for now you can read and download a single R script which contains the functions and all you need to use them. We are still in the process of documenting the functions. To make them easier to use, we have a vignette which demonstrates the use of many of the functions as applied to example causal mapping data files (here is the main example file and here is a special example to demonstrate the use of combined opposites). The vignette is an .Rmd file: if you process it using the package knitr or by pressing Knit in Rstudio, you should get output like this. It contains over 50 different types of map."}, {"title": "Why do causal mapping rather than ordinary QDA", "path": "/014 Causal QDA/210 Why do causal mapping rather than ordinary QDA.html", "text": "Why do causal mapping rather than ordinary qualitative coding? in a google doc atm https://docs.google.com/document/d/1kXrH6pdKSvVNVEsRoZCw4GlN87L7m8x65lHtWk ZbMo/edit?tab=t.0 Causal mapping has traditionally been done manually, just like working with Nvivo, but instead of coding individual themes we code links between themes (aka causal factors) . For example in the text \u201cthen the teacher said that the children are now much more active due to the new playground\u201d, we would code \u201cthe new playground\u201d as the cause, \u201cthe children are now much more active\u201d as the effect, \u201cthe teacher\u201d as the source and the whole sentence as the associated quote or evidence. The coding is nowadays entirely done by AI. But human input is crucial, in particular to craft the instructions to the AI and in deciding how to cluster and label similar themes. The software automatically constructs maps and tables to answer specific questions, e.g. \u201cwhat drives poverty, from the point of view of the women?\u201d. Causal mapping: an unusually useful form of qualitative coding? Our aim is to present causal QDA as a particularly useful way to make sense of text in a way which is straightforward to apply with a highly, though not completely, standardised approach. Our steps in doing this are: We present causal QDA as an important and useful form of QDA We explain why causal coding can be easier to apply than non causal coding. We explain how it gives us causal maps \u201cfor free\u201d, which can be used to answer useful questions. Although the focus of this paper is not on AI assistance, we also argue that these properties make causal mapping particularly suitable for automation with AI in a way which is verifiable and avoids using the AI as a \u201cblack box\u201d. What is causal mapping? Causal mapping helps make sense of the causal claims (about \"what influences what\") that people make in interviews, conversations, and documents. This data is coded, combined, and displayed in the form of a causal map. Each claim adds a link to the map. These maps represent individuals' and groups' mental models: individual links and (often interlocking) longer chains of causal explanations. For every link, we can store the original sources and quotes. Causal mapping has been used across multiple disciplines for over 50 years (Ackermann et al., 2004; Axelrod, 1976; Eden, 1992; Hodgkinson et al., 2004; Laukkanen, 1994; Narayanan, 2005; Powell et al., 2024), but though some causal mapping practitioners have published guides to causal coding, it has not often been presented as (also) a stand alone form of QDA. We tried to rectify that in (Powell et al., 2024). This paper follows on from that explication. What is causal QDA? ! Some forms of causal mapping may involve constructing maps directly, for example in participatory fashion with a group of people. In this paper we don\u2019t cover that method. Here, we do causal mapping by coding causal claims within text, what we call \"causal QDA\". So this is a less participatory approach. This task is similar to other forms of qualitative data analysis (QDA). However, non causal QDA is designed to capture concepts in general, whereas in causal QDA we code (claimed) causal links between causal factors: things, events, phenomena, changes: anything which can affect or influence, or be affected or influenced by, something else. Using non causal QDA we can create post hoc causal links between independent concepts as in the figure above, version 2. Or we can create the causal link as a monolithic concept as in version 1. But then the word \u201ccause\u201d does not really do anything: we can\u2019t automatically deduce anything from it. In causal QDA, the primary act of coding is to highlight a specific quote from within a statement and identify the causal claim made by simultaneously identifying a pair of causal factors: a \"cause\" and an \"effect\". In causal QDA, we only code causal claims. The causal factors only exist as one or other end of a causal link and have no meaning except in their role as either end of a causal link. The coding Floods happened \u2192 Crops destroyed IS a causal link, and a set of such links can be immediately visualised as a network or map using suitable software \u2013 even Excel. In causal coding, the result of each act of coding is a pair of factors (an ordered pair, because B \u2192 C is not the same as C \u2192 B). It is the factors which are the atomic units, which form the codebook, even though the factors only make sense as part of causal links. An emergent reproducible spectrum in QDA? Before we get to the logic of causal QDA, let\u2019s present what we might call an emergent/reproducible spectrum within QDA more generally. We will then place causal QDA on this spectrum. In emergent forms of QDA: The design is not pre determined but emerges: Yvonna Lincoln & Egon Guba (1985): there is no clear distinction between the method and the output. As Braun and Clarke describe in thematic analysis, an inductive approach \u201cis grounded in the qualitative data itself, enabling researchers to identify patterns and derive key themes without the constraints of pre existing categories\u201d\u200b xx Analysis is interpretative rather than systematic. The product is interpretive text, not a structured model. It\u2019s usually a report, paper, a narrative synthesis of themes. Answers are mediated by the author. You need the author (or a reader analyst) to interpret the analysis and answer your questions. It\u2019s not machine readable. You can\u2019t automate reasoning over it or extract structured inferences without re coding it. More interested in a general theory a la Glaser and Strauss (1967). Reproducible designs on the other hand tend to propose: Pre registration of questions and methods. More emphasis on a top down or deductive deconstruction of high level questions into simpler tasks. A greater chance of being able to reproduce similar results with similar inputs. More interested in a case specific summary as in Mayring (1991) rather than a global theory. We do not claim that any form of QDA is truly reproducible; we use reproducible as a name for an idealised pole of a spectrum opposite the other pole, \u201cemergent\u201d. Every form of QDA involves at least to some extent breaking down hard, high level questions into simpler tasks which are at least somewhat more traceable and verifiable than the original questions. In more \"emergent\" traditions we might start with the vaguest of research questions and refine the questions and the methodology as part of the process,\"making up the rules as we go along\". The positionality of the researcher is central(Copestake et al., 2019). In more reproducible approaches, the balance is more heavily algorithmic and the outputs are closer to being machine readable answers to pre determined, even pre registered, questions, though the researcher still has to make crucial decisions at various points (\u201chuman in the loop\u201d). Reproducible QDA at least partially encodes claims in a structured, somewhat machine readable form. It supports decentralized interpretation: once the output exists, anyone can inspect, trace, and reason over it without needing to consult the author. Like Patton we affirm a \u201cparadigm of choices\u201d \u2013 balancing flexibility with appropriate methodological structure for the situation\u200b xxscholar.lib.vt.edu Each application involves some balance between these two extremes, on various subdimensions, where we might characterise the disputes between Glaser and Strauss & Corbyn as about the introduction of at least a degree of pre structured methods, a smidgen of reproducibility. Spoiler: Causal QDA lies at the latter, reproducible, end of this spectrum. The logic of QDA: you\u2019ve done your analysis, now what? The result of qualitative data analysis can be understood as, at least, some kind of qualitative theory or model at least of the sources\u2019 beliefs, with at least some possibility of generalising beyond them. But it can be hard to know what to do with the results of an emergent qualitative text analysis. There is no clear decision procedure: we can ask the author, and the answer is: some explanation, i.e. more text. In more reproducible approaches we do get some more structured outputs such as tables of frequencies. Some authors such as Mayring see these kinds of outputs as an important analysis result. QDA software is often used to capture and structure and even make inferences with these kinds of outputs. In the logic of (non causal) reproducible QDA, we can do things like this: count occurrences of concepts, and use ordinary arithmetic to report eg which of two concepts was more common count co occurrences of two concepts, and construct measures like association between concepts, and more generally combine and query occurrences with boolean logic create case/code matrices report relationships between sources and concepts, for example to compare codings of one concept for different genders reason about concepts, for example to deduce that an occurrence of \"lion\" is also an occurrence of \"mammal\", either relying on our implicit understanding of the concepts or through the explicit declaration of a parent child relationship. Of course frequency statistics are notoriously unstable, because they depend on our decisions about granularity and chunking. If I have a codebook which has 100 different codes for cats and only 1 code for dogs, we may conclude that dogs were mentioned in the text more often than any other animal concept even if cat concepts were mentioned more often in combination. This is one reason why reasoning with these kinds of outputs can never be merely automated. There always has to be a \u201chuman in the loop\u201d. Nevertheless the point is that we can understand the output of QDA coding as some proportion of \"more text\", which itself needs to be interpreted by humans, and a complementary proportion of machine readable, structured output which can be used to ask and answer questions (Which are the overarching themes? How much does climate anxiety come up as a theme? Who mentions it most?) at least somewhat independently of human guidance. QDA logic can also be extended beyond the simple logic of frequencies and occurrences to apply (special kinds of) codes which have additional explicit rules associated with them, such as code weighting (as for example in MaxQDA). This means we can for example apply codes like \u201csomewhat happy\u201d or \u2018very unhappy\u2019 which enable us to say that the expression of happiness in one case is stronger than the other, or (if we also allow coding for time) that happiness increases or decreases over time. These extra deductions we can make come free with the (implicit or explicit) underlying ordinal logic of comparison of intensity. QDA without coding Coding does not have to be central to qualitative data analysis (Morgan, 2025; Nguyen Trung & Nguyen, 2025). \u2026 What are the advantages of causal over non causal QDA? The additional logic of causal QDA lets us ask and answer additional questions In the ontology of causal QDA, we have factors, which are something like concepts, and also links, which are ordered pairs of factors. We can immediately make use of all of the frequency/occurrence logic mentioned above for both factors and links, e.g. to report which was the most commonly mentioned causal factor and/or which was the most commonly mentioned causal link. The same caveats about granularity and frequency apply (here and also in the following section). But that is just the start of it. In addition, any set of links can also be understood as a network. We don't need to do any extra work, we can simply display and query the network using any suitable algorithm or software. There is a whole wealth of causal belief logic we can (tentatively) apply here to ask and answer practical questions. Causal coding allows us to make free use of the logic of causal mapping to do causal belief analysis: to ask and answer questions and make deductions. We can ask questions about the causal map overall: Leveraging our ability to count the frequency with which particular links are mentioned, we can, with caveats, interpret the frequency with which a link is mentioned as the strength of the evidence for that link. which factors are causally central or causally peripheral? which factors appear most often at (or near) the end of causal chains, and which appear at (or near) the beginning?: which factors appear proportionately more often as outcomes than as drivers? We can ask questions anchored to specific factors For example, we can ask about all the immediate causes or effects of a particular factor or set of factors, or we can ask about both causes and effects, to produce the local neighbourhood or \"ego network\" xx. Most interestingly we can ask about chains anchored to specific factors. A fundamental, emergent property of networks is (with caveats) transitivity: being able to ask questions not just about links but about chains. All the questions mentioned above about links we can also apply to arbitrary chains. So for example we can ask how often a particular chain is mentioned, or mentioned by women. We can list all of the factors downstream of a particular factor of interest. Here, there is a lot to be said about transitivity (when and where we can, from B \u2192 C and C \u2192 D, conclude B \u2192 D). For example if the same source who claims Floods happened \u2192 Crops destroyed, in the same context also says that Crops destroyed led to going hungry, we can deduce that perhaps this person attributes going hungry indirectly to the floods. Likewise, we can list all the factors upstream of a particular factor (or set of factors) Likewise, we can list all the factors between an origin factor (or set of factors) and a target factor (or set of factors), again optionally up to a certain length of chain, which Bougon called xx. In terms of program evaluation, we can think of this as all the causal chains from one or more interventions of interest to one or more outcomes of interest. All the logic which we can apply to the whole network can be applied to arbitrary chains or subnetworks. For example we can ask how robust overall is the network of evidence for the influence of X on Y? (for which we have proposed the maximum flow / minimum cut algorithm). Causal mapping also gives us some additional, optional tools (like hierarchical causal coding and opposites coding and coding links with hashtagsxx) for organising its elements in a way which will help answer additional practical questions, which we will however not cover here. Coding the strength of a link, and why we don\u2019t do that Many or most causal mapping approaches, including Causal Loop Diagrams, also code the perceived strength of a causal link. This means that the factors become variables which can take values between, say, low and high or positive and negative, and we can make a much broader range of inferences using some form of numerical modelling. This can be seen as the extreme reproducible end of our spectrum and borders on quantitative approaches. However we do not go so far: our causal factors are closer to being propositions rather than variables and we do not jump to code, say, poverty as negative wealth, or unemployment as obviously just the opposite of employment. Generally all forms of causal mapping have been less interested in finding general rules but only in modelling a more or less specific case or set of cases. But causal mapping has been used for causal meta analysis, for example of sets of existing evaluation reports (see Powell (2020) for an example). Why a more restricted, causal, ontology helps cut to the chase in asking and answering useful questions The result of causal QDA is not just a report or an essay, but a network, a map, already structured to answer questions about causal beliefs. Many practical, real world questions are at least partly causal in nature: what leads to what? why does Z happen? what might happen if X? what are the dominant explanations for Y?. Causal mapping gives us a causal vocabulary which provides some semi standard ways not only to ask but also to answer those kinds of causal questions (e.g. what is the collected evidence for a causal path from X to Y?. For which pathway is there most evidence? According to the women only?) We should stress that even here, we very much need a human in the loop, for instance in order to: Pay attention to problems of chunking (where do phenomena begin and end) and granularity (how big are the chunks) The transitivity trap While we can be guided by relative frequencies of links, we always consider the specific (combined) evidence for each link, for example when some evidence might be weak or valid only for a specific context ! A more restricted, causal, ontology simplifies the construction and development of a codebook \"What are the causal factors, the cause and the effect, at each end of this causal claim\" \u2026 is a very challenging question, but it is a whole ballgame less challenging, less open, than \"what themes/concepts are mentioned here\". This fact makes it much easier to get started on and complete our coding, whether we are going to code deductively (from an established codebook) or inductively (developing the codebook iteratively). It is particularly easy when working inductively and using in vivo labels close to the original text: we simply have to identify each and every causal claim in the text, and for each one, identify the cause and the effect \u2013 the \u201ccausal factors\u201d. (Using in vivo labels means we then need another way to consolidate the resulting large number of factor labels.) Inter rater agreement in causal coding is good (Buzogany et al., 2024; McCardle Keurentjes et al., 2018). This means that we can reduce most of causal coding to a series of low level tasks: code each and every causal link. Inter rater agreement for causal coding is high, making it suitable for automation with AI The fact that causal coding can be largely reduced to a series of low level tasks makes it very suitable for automation with AI. High precision and recall scores can be achieved. (Consolidating a large number of in vivo labels can be accomplished mostly automatically with clustering of text embeddings.) The AI is used only as a tireless, low level but incredibly fast assistant with the instruction to code each and every causal claim in the text. This is radically different from the kind of AI supported \u201cblack box\u201d coding which essentially treats the AI as a trusted co coder who is asked to make, or help make, high level decisions such as \"what are the main themes in this text?\" or even \u201cWhat is the overarching causal network in this text?\u201d. The accuracy (precision and recall) of AI supported causal coding is not perfect, but it is improving all the time. Creating, implementing and monitoring the coding protocol remains an essential task (\"human in the loop\") but we claim that AI supported causal coding comes closer than other approaches to providing an almost out of the box way to make sense of texts at scale. But, causation? There are many caveats about causation, as outlined in Powell et al. Critical realist about actual processes, and realist about causation itself. Limitations Causal coding uses a restricted ontology will certainly reduce the potential scope of the final report or model, but perhaps not as much as we might expect. It cannot answer all questions about a text! It will fail to code claims or statements which contain material which never influences anything else or is influenced by anything else. It is also not suited to constructing broader theories with wider applicability in the sense of (Glaser & Strauss, 1967). We have not used causal coding explicitly to search for latent as opposed to explicit material, ibid, (Braun & Clarke, 2006). Perhaps it will prove possible but we know of no examples from the wider causal mapping literature where this has been tried. The causal coding procedures we have outlined here represent a single pass, non iterative approach. Of course this can be expanded to include the more iterative approach essential to most QDA approaches, with varying levels of human oversight. For example we can quickly and cheaply experiment with different coding rules, compare the results, modify the rules and iterate again. This ability to experiment with, compare and iterate potentially hundreds of coding rules and algorithms is a real strength of (semi )automated coding. Vulnerable to limited attention: if we really process only one section at a time, we will be unable to notice cross references or places where one section qualifies another, as pointed out by Udo Kelle (1997) xx. This may not be a fundamental limitation of machine led approaches if we arbitrarily expand the surrounding context, increasing the attention or context window, but at present this is slow and expensive. See A 2023 study by Rezaee et al. compared topic modeling (LDA) vs human qualitative coding of tweets, finding that automated methods reliably find dominant themes but miss subtle frames that human interpretive coding can catch. What Causal Map Ltd provides A wider overview of causal mapping as applied in disciplines from ecology to organisation science is presented in (Powell et al., 2024) and in this Zotero library. We provide a web app Causal Map which is specifically designed for (mostly human powered) causal QDA and has been used in academic and practical applications. Colleagues at BathSDR and others have applied (human powered) causal coding in over 100 evaluation projects using the Qualitative Impact Protocol (Copestake et al., 2019), mostly in the field of international development, more recently using the Causal Map app. Here is a broader list of causal mapping software. Finally, we also provide dedicated software for AI powered causal QDA, details on request."}, {"title": "where do the labels come from", "path": "/014 Causal QDA/220 where do the labels come from.html", "text": "Where do the labels for the causal factors come from? As with ordinary QDA and thematic analysis (Braun and Clarke, 2006), approaches vary in the extent to which they are purely exploratory or seek to confirm prior theory (Copestake, 2014). Exploratory coding entails trying to identify different causal claims embedded in what people say, creating factor labels inductively and iteratively from the narrative data. Different respondents will not, of course, always use precisely the same phrases, and it is a creative challenge to create and curate this list of causal factors. For example, if Alice says \u2018Feeling good about the future is one thing that increases your wellbeing\u2019, is this element \u2018Feeling good about the future\u2019 the same as \u2018Being confident about tomorrow\u2019 which Bob mentioned earlier? Should we encode them both as the same thing, and if so, what shall we call it? We might choose \u2018Positive view of future\u2019, but how well does this cover both cases? Laukkanen (1994) discusses strategies for finding common vocabularies. As in ordinary QDA, analysts will usually find themselves generating an ever growing list of factors and will need to continually consider how to consolidate it \u2013 sometimes using strategies such as hierarchical coding or \u2018nesting\u2019 factors (as discussed in the following section). The alternative to exploratory coding is confirmatory coding, which employs an agreed code book, derived from a ToC and/or from prior studies. QuIP studies mostly use exploratory coding but sometimes supplement labels with additional codes derived from a project\u2019s ToC, for example, \u2018attribution coding\u2019 helps to signify which factors explicitly refer to a specific intervention being evaluated (Copestake et al., 2019b: 257). However, careful sequencing matters here because pre set codes may frame or bias how the coder sees the data (Copestake et al., 2019a). Again, the positionality of the coder matters just as much when doing causal coding as it does for any other form of qualitative data coding."}, {"title": "causal mapping turns QDA on its head", "path": "/014 Causal QDA/250 causal mapping turns QDA on its head.html", "text": "Question: I've got a load of texts to analyse, can I use Causal Map just for standard Qualitative Data Analysis, like ordinary thematic analysis, to identify important themes? Our answer: That doesn't really work in Causal Map. Sorry! Question: Why not? If you've got a good qualitative text analysis pipeline, why can't you generalise it? Our answer: 1. We put the question the other way round : why do thematic analysis (which is harder) when causal mapping can identify not only some of the important themes but also tell you how they influence one another? 2. ... what you really want to do in the end, especially if you are doing evaluation, is find out what causes what in the eyes of your stakeholders. Identifying static themes can be interesting but often it's the causal information which helps you answer your main research and evaluation questions. Causal mapping is often a great way to cut to the chase . 3. Thematic analysis is more of an art than a science. \"What are the main themes here\" is a very open ended question which can (and should) be interpreted in different ways by different analysts (\"positionality\"). Whereas people (and GPT 4) tend understand the instruction \"identify each and every section of text which says that one thing causally influences another\" quickly and easily, and they tend to agree on how to apply the rule . 4. The way we do causal mapping means identifying each and every causal connection. There is less room for someone's opinion in selecting what themes are most salient . Surprisingly, we can get good results without even a codebook of suggested themes aka causal factors, let alone bothering to train the AI or give it examples. 5. This means that the steps from your initial research idea all the way up to (but not including) your final analyses can be quite easily automated in a transparent way. You can train an army of analysts to the coding for you manually, or you can press the AI button, or a combination of both, and either way you will get pretty similar results. There isn't so much room for the opinion of your analysts, whether human or robot, at any point in the pipeline. Of course causal mapping is not free of bias (or positionality) due to human analysts' or AI analysts' \"world views\". It just leaves less room for those biases than more general thematic coding. And of course, there are times when general thematic analysis or some other kind of QDA is really what you need. We're just saying that causal mapping might fit your need more often than you think . Finally, you can in fact use Causal Map to identify some kinds of theme, and in particular when we are doing manual coding we sometimes code where causal factors are mentioned but without any specific cause or effect, like if someone just says \"unemployment has gone up\". But this isn't a main focus of our work. And just to emphasise, causal mapping isn't proprietary. It's been around for 50 years. You can even do it (manually) in Excel."}, {"title": "The elephant in the room", "path": "/015 Causal mapping in evaluation/610 The elephant in the room.html", "text": "The elephant in the room ! Responding&dashCommentUrn=urn%3Ali%3Afsd comment%3A(7008516217275650050%2Curn%3Ali%3AugcPost%3A7008450782928732160)&dashReplyUrn=urn%3Ali%3Afsd comment%3A(7008865880012988416%2Curn%3Ali%3AugcPost%3A7008450782928732160)&replyUrn=urn%3Ali%3Acomment%3A(ugcPost%3A7008450782928732160%2C7008865880012988416)) to our one page description of causal mapping, Julian King says the elephant in the room with causal mapping is: can causal mapping really help you get from causal opinions to causal inference? The short answer is: sure it can help you, the evaluator, make that leap. But it, causal mapping, does not give out free passes. But in more detail, here are four more responses to the elephant. 1: Causal mapping ignores the elephant. On its own, causal mapping doesn\u2019t even try to warrant that kind of step : it humbly assembles and organises heaps of assorted evidence in order for the actual evaluator to make the final evaluative judgement. Unlike evidence from an interview, or a conclusion from process tracing or from a randomised experiment, causal mapping evidence isn\u2019t a kind of evidence, it\u2019s an assemblage of those other kinds of evidence. It certainly isn\u2019t a shortcut to get cheap answers to high stakes answers by conducting a few interviews with bystanders. If you have to answer high stakes causal questions like \u201cdid X cause Y\u201d and \u201chow much did X contribute to Y\u201d and have just a handful of pieces of evidence, there isn\u2019t much point using causal mapping. Causal mapping is most useful for larger heaps of evidence, especially from mixed sources and of mixed quality; it gives you a whole range of ways of sorting and summarising that information, on which you can base your evaluative judgements. What it doesn\u2019t give you is a free pass to any evaluation conclusions, and especially not the high stakes ones which occupy so much of our attention when we think and write about evaluation. 2: In most actual causal mapping studies , the elephant usually doesn\u2019t even enter the room . Usually, we aren\u2019t dealing with monolithic, high stakes questions. Most causal mappers are looking for (and finding) answers to questions like these: In which districts was our intervention mentioned most often? Do children see things differently? How much evidence is there linking our intervention to this outcome? Does our project plan see the world in the same way as our stakeholders? All of these are relevant questions for evaluations. Some of them might feed into judgements about relevance, or about effectiveness or impact, and so on. We might notice for example that there is some evidence for a direct link from an intervention to an outcome, and much more indirect evidence, and some of those paths remain even when we remove less reliable sources. We can even compare the quantity of evidence for one causal pathway with the quantity of evidence for a different pathway. We can ask how many sources mention the entirety of a particular pathway, or we can ask which pathways have to be constructed out of evidence from different sources. (On the other hand we don\u2019t, for example, make the mistake of inferring from the fact that there is a lot of evidence for a particular causal link that the link is a strong one.) All of this is bread and butter for evaluators, even though it doesn\u2019t answer those elephant questions. 3: Causal mapping pushes back against the elephant . In every evaluation, the evaluator assembles some evidence and makes an evaluative judgement on the basis of it. All evaluation involves causal mapping in this sense. Occasionally there is, or seems to be, only a single piece of evidence in the heap \u2013 perhaps, evidence from a controlled experiment. But the final judgement is the evaluator\u2019s responsibility, and (perhaps implicitly) must take into account other factors: \u201cthis is a controlled experiment, it was carried out by a reputable team, \u2026 but wait, their most recent study was criticised for allowing too much contamination \u2026. but wait, the effect sizes were calculated with the latest method and controlled experiments seem to be a good way of reaching causal conclusions \u2026\u201d, and so on. An essential part of the evaluative process is also careful consideration of how exactly to formulate a conclusion, bearing in mind the context and the audience and how it will be generalised and applied. So, in practice, there is always a heap of factors to consider, often involving different parts of more than one causal pathway, even when the heap seems to be dominated by one or two elephants. 4: Causal mapping embraces the elephant . In most causal mapping studies, we do not in fact simply assemble the evidence we already have but actively gather it systematically. A good example is QuIP, the Qualitative Impact Assessment Protocol. The evidence is \u201conly\u201d the considered opinions of carefully selected individual stakeholders, but it is gathered using blindfolding techniques to minimise bias so that, once assembled and organised with causal mapping, the evaluative leap from opinions about causality to conclusions about causality can be made with more confidence, transparently and with appropriate caveats. Still, it's not the causal mapping itself which makes or warrants the leap, it's the evaluator, using evaluative judgement."}, {"title": "Qual versus quant impact evaluation strength vs modelling", "path": "/015 Causal mapping in evaluation/650 Qual versus quant impact evaluation strength vs modelling.html", "text": "Qual versus quant impact evaluation. Same ballpark, different ballpark? ! Weird image of people counting beans generated by Canva's AI Soft versus hard impact evaluation approaches? Quant versus Qual? Is there an essential difference? Summary: quant and qual impact evaluation approaches are different ballparks because quant approaches attempt to estimate the strength of causal effects. Whereas qual approaches either don't use numbers at all or only do calculations about the evidence for the effects , not about the effects themselves and in particular we don't estimate strength of effects. Here's the question: how can we distinguish \"soft\" approaches to impact evaluation like Outcome Harvesting, QCA, causal mapping, Process Tracing, Most Significant Change, Realist Evaluation and so on from statistics based causal inference (SEM, DAGs, RCTs etc)? Here are two bad answers: We can't distinguish our \"soft\" approach(es) by saying that we attempt to assess causal contribution and answer questions about for whom and in what contexts etc, because quantitative approaches attempt all of that too. We can say that we are focused on complex contexts, but there's nothing to stop someone using say OH in a non complex context either is there? In any case whether a context is complex or not is also a matter of how you frame it, no? And there's in fact no shortage of examples where quant approaches have been used in complex contexts. Here's a better answer: these \"soft\" methods are qualitative, in the sense that where we involve numbers at all, our arithmetic is essentially an arithmetic of evidence for causal effects: is there any evidence for one pathway, how much, how much compared with another? For example, Process Tracing sometimes does calculations about the relative certainty of different causal hypotheses. QCA counts up configurations. Whereas quant causal analysis involves estimating the strength of causal effects (as well as having clever ways to reduce the bias of those estimates). As far as I know, qualitative approaches never attempt this (calculating the strength of a causal effect). We might conclude that the evidence suggests a particular effect is strong , for example because we have collected and verified evidence for a strong connection . But we don't, say, combine this with another set of evidence for a very weak connection and conclude that the strength of the effect was only moderate (we don't do maths on the strengths). It's true that qual approaches also do causal inference in the sense of making the jump from evidence for a causal effect to judging that the effect is real. Quant approaches (and, to be fair, some qual approaches) suggest that using their special method gives you a free ticket to make this leap. And indeed different methods include different ways to reduce different kinds of bias which mean you can be more confident in making the leap. But I'd say there are no free tickets. No way of an evaluator getting out of the responsibility of making the final evaluative judgement, however clever and appropriate your method. (You could argue that FCM and Systems Dynamics do arithmetic on the strengths of connections. Perhaps that makes them quant methods.) Seen this way, in essence qual and quant impact evaluation are not alternatives or competitors. They are different ways to do different things. A second limitation of causal mapping is the difficulty it has in systematically capturing the strength or type of causal influence. It is relatively rare in open conversation for people to indicate in a consistent way the magnitude of the effect of C on E, or whether C was a necessary or sufficient condition for E or precisely how certain they are about the connection. There is of course scope for framing questions to encourage people to ascribe weights to their answers, which can then be incorporated into the way maps are constructed. But imposed precision risks turning into spurious precision, and stronger framing of questions may distract from other issues and nuances that more open ended questioning might otherwise have elicited."}, {"title": "Reconstructing program theory empirically", "path": "/015 Causal mapping in evaluation/Reconstructing program theory empirically.html", "text": "Reconstructing program theory empirically To evaluate a program, the evaluator can use Contribution Analysis (CA) (Mayne, 2012, 2015). We start with a program logic or Theory of Change (ToC), consisting of possible pathways from interventions to outcomes, and collect existing or new evidence for each link. However evaluators can often not assume that the ToC underpinning a program aligns with the realities on the ground, or they may uncover outcomes not anticipated in the original program design see Koleros & Mayne (2019). We have argued (Powell, Copestake, et al., 2023, p. 114) for a generalisation of CA in which evidence relevant to constructing a program theory, as well as evidence for the causal influences flowing through it, are both collected at the same time, without the evaluator (necessarily) having a prior theory. In this sense, following Mayne, \u201cprogram theory\u201d need not be something that any person necessarily possessed or articulated at the time, but is something which can be approximated and improved during the evaluation process. (Re )constructing program theory empirically in this way is an essentially open ended, qualitative problem. Closed data collection methods are not suitable because we cannot measure what we do not yet know. Open ended, qualitative methods to (re )construct a theory are notoriously time consuming and are usually heavily influenced by researcher positionality (Copestake et al., 2019). Powell, Copestake, et al (2023, p. 108) present this task as gathering and synthesising evidence about \"what influenced what\", evidence which is simultaneously about theory or structure and contribution. Each piece of evidence may be of differing quality and reliability and about different sections of a longer pathway, or multiple interlocking pathways, and may come from different sources who see and value different things. Reconstructing program theory empirically To evaluate a program, the evaluator can use Contribution Analysis (CA) (Mayne, 2012, 2015). We start with a program logic or Theory of Change (ToC), consisting of possible pathways from interventions to outcomes, and collect existing or new evidence for each link. However evaluators can often not assume that the ToC underpinning a program aligns with the realities on the ground, or they may uncover outcomes not anticipated in the original program design see Koleros & Mayne (2019). We have argued (Powell, Copestake, et al., 2023, p. 114) for a generalisation of CA in which evidence relevant to constructing a program theory, as well as evidence for the causal influences flowing through it, are both collected at the same time, without the evaluator (necessarily) having a prior theory. In this sense, following Mayne, \u201cprogram theory\u201d need not be something that any person necessarily possessed or articulated at the time, but is something which can be approximated and improved during the evaluation process. (Re )constructing program theory empirically in this way is an essentially open ended, qualitative problem. Closed data collection methods are not suitable because we cannot measure what we do not yet know. Open ended, qualitative methods to (re )construct a theory are notoriously time consuming and are usually heavily influenced by researcher positionality (Copestake et al., 2019). Powell, Copestake, et al (2023, p. 108) present this task as gathering and synthesising evidence about \"what influenced what\", evidence which is simultaneously about theory or structure and contribution. Each piece of evidence may be of differing quality and reliability and about different sections of a longer pathway, or multiple interlocking pathways, and may come from different sources who see and value different things."}, {"title": "! words - bodies", "path": "/400 AI reaction/! words - bodies.html", "text": "To have intelligence you have to have a body Do you feel uncomfortable saying \"the AI understood what I wanted\"? Silva correctly draws attention to this and has some suggestions. I think first we have to look at uses, not words. I don't think it's interesting to argue about generally whether some AI is intelligent or conscious or whatever. It's interesting to look at cases where we feel we need some of these somewhat controversial words. You can't even enumerate AIs. to have consciousness you have to have a body. So, my friend Randy sent me this article about the singularity and AGI. It's interesting. I think they're a little bit kidding themselves about the extent of the lead that the USA has over China, for what it's worth. But I'd agree with the sense of urgency and the assessment that the likelihood that these things are going to accelerate ever more rapidly very quickly. The problem is not that we're close to AGI and the singularity, but we're already in the foothills of it, and have really been since like the first large language models, just a few years ago. But I think there's a series of category errors going on which have to do with what I like to call the embodiment of AI or the lack of it. The practical consequence is that I don't see a real current danger that AIs somehow develop desires and that those desires are out of alignment with human priorities, and so they kind of take off on their own and start doing things we don't like. I think this isn't so much a technical limitation or to do with any kind of clever engineering, but to do with of course the question of AIs having priorities or desiring to do something. I'm already stumbling to formulate this because I don't even know how to enumerate what it is we're talking about, because I don't know whether to say AIs have desire or AI has desire . The kinds of people who think a lot about these things are far too focused on the concept of intelligence for whatever reason. They have somehow equated it with something like computing power, and it's true that computers in general have this thing called \"computing power\" and that LLMs in particular have added this thing like \"computing power with meanings\" and we equate the two with intelligence. We kind of think then that the conceptual problems are solved, and all we have left to do is to decide whether these things really are intelligent, and if so there's one remaining question about whether they're therefore possibly conscious or not. But I think we have to take a step back because the category error is we can't even formulate the problem, because I just stumbled over saying these things are intelligent or this thing is intelligent , and that's already the problem. Because in fact there's a whole web of different concepts, such as having a priority having an aim having a desire being intelligent being conscious being empathetic being understood or misunderstood being able to understand or misunderstand reaching an agreement being loyal or reliable or trustworthy being patient or impatient. being in pain feeling frustrated. a whole bunch of important overlapping concepts which are all things we have to conceptually get our heads around if we're going to think meaningfully about AI. In our possibly Western way of looking at things we've come to think of the only real core problem as being somehow general computing power, and everything else like having a body being able to interact with others having a finite lifespan having a memory; remembering and forgetting having physical boundaries which are distinct from the physical boundaries of others the ability to form groups and alliances the ability to interact with specific groups of other beings the ability to touch one another ... that these are all just kind of add ons that you can simulate or emulate if you want, but they're just kind of trivial additions and the fundamental problem is intelligence, which is something like general computing power. But it doesn't work like that. Let's take a step back and think about how what we call life evolved in this part of the universe, which involves the emergence at the very least of certain reoccurring organic compounds and emergent properties of those compounds, and then molecules and the phenomena of cells and the presence of genetic material and the way in which genetic material can encode the ability to in a certain context recreate itself with more or less relevant variation, and the emergence of sexual reproduction and what that means in terms of separate organisms and sexed organisms which interact with and select one another in certain ways; and the emergence of social groups and tool use and language and what we call intelligence. There is nothing inevitable about this particular series of nested, emergent phenomena, of stable and self propagating biological systems. The thing that we call life might have uncountably many cousins across the universe, or across other possible universes, where the phenomenon or independently identifiable and countable beings or organisms might pan out quite differently. And all the kind of logic that we assume about organisms that have an independent existence within finite boundaries for a finite amount of time and interact in certain ways with one another and form groups and can have a shared culture that can be transmitted possibly even through the innovation of genetic material and perhaps more importantly through the innovation of orally transmitted culture, and the idea of the selection and evolution of cultural material, all of this didn't have to be like that and there are countless different uncountably many different variations on these kinds of themes. And my point is that to cut a very long story short what we think of as, or the concepts that we use, like self awareness, and consciousness and being understood and misunderstood these concepts themselves have evolved in our self referential communication to one another about our own existence and the problems we have to solve independently or together. As a sidebar here they aren't philosophical abstractions or it's not helpful to think of them in terms of their ethereal things like whether an AI like a human has a kind of thought bubble above its head within which it's conscious and which reflects a view of the world. This isn't a transcendental but ultimately unprovable hypothesis, but more its language about consciousness has evolved and we've learned how to use it in useful and practical contexts. And we might evolve useful language necessary language to talk about AI. Oh, I should also add memory to the list of special concepts. So the first real riposte is to say our speculative language about the memory or consciousness or desire or purpose of AI is remarkably poor, because their development has perhaps happily been very one dimensional, and previous and parallel developments in artificial intelligence, which has had more to do with constructing worlds with somewhat artificial intelligent beings that may be communicate with one another has fallen a lot by the wayside as it's been technologically less fruitful, but might have been philosophically and sociologically much more interesting. So to come to the crux, we can't even count AIs or AI as it is right now, we don't know if there's one of them or many of them or if it's like some kind of uncountable liquid. There could have been an AI development, and there might well have been, in which the focus was more on developing independent beings or agents, and indeed we're moving in that way too, but even here the concept of an agent has got more to do with being able to solve the computational power problem or to produce outputs with more computational power, and there's nothing wrong with that. So what I'm saying is that on the one hand I think that AI development is probably already moving towards some of the concepts I've been talking about like embodiment, having a somewhat physical body and somewhat limited, occupying a somewhat limited space with possibly possibilities for physical interaction with other beings, artificial or not. These are not and many other things too, these are not and having a memory and history are not only what you might call hardwired desires, but the ability to and values, but the ability to develop new values and override old values just as we do. No child is born with the desire to listen to the latest music from a jazz orchestra, but you might develop that desire over time, in the tension between what you might call hardwired desires and gradually evolving soft wired desires that change and conflict with one another over time. It's just a way of learning how to a way of steering evolving self guided partially self guided organisms that's just proved beings which have just proved successful over time in our particular corner of the universe. There's so much going on in the way that our version of \"life\" and the way that we as a species have developed and our cultures have developed, which is not simply a question of computational intelligence plus ephemera. It might even be that to get really meaningful and useful and inspiring potential some at least of these things one might think were peripheral, like being embodied are essential and necessary."}, {"title": "! words - perspective", "path": "/400 AI reaction/! words - perspective.html", "text": "AIs have perspective March 24, 2025 Do you feel uncomfortable saying \"the AI misunderstood what I wanted\"? Does that mean we think they are conscious? Silva correctly draws attention to this and has some suggestions. I think first we have to look at uses, not words. It isn't so useful to argue about generally whether some AI is intelligent or conscious or whatever. It's interesting to look at cases where we need some of these somewhat controversial words. Often, when talking to each other about prompt engineering, we have to say things like: I realised it hadn't understood what I meant by 'repeat the first part'. It was still thinking of the previous instruction, which is fair enough, I guess. So I had to rewrite that part of the prompt. In this kind of case, we have to use words like 'understand' or 'think' (or, more often, 'misunderstand'). We can't get round that. You can put scare quotes round the word, or like Silva put ' simila' on the end of it, but I'm not sure what you achieve. The same goes for 'reasoning'. We often need to say to one another, when improving a prompt, 'ah, its reasoning was flawed here' or 'this newer model seems to do a lot more sophisticated reasoning before coming to a conclusion look at this part.' I don't think I've ever, in contrast, had to use the word 'conscious' in any practical context when talking about AIs. You could say, 'we shouldn't use the word understand about an AI because that implies they are conscious, and how can an AI be conscious'. But I'd say that these words mean no more and no less than how we use them in this context. I have no use (at the moment this might change) for saying an AI is conscious, or for that matter, not conscious. Wittgenstein taught us to look at words in use. Beyond that is just words going on holiday: idle philosophising."}, {"title": "Causal QDA 1", "path": "/400 AI reaction/200 Causal QDA 1.html", "text": "opinionated AI coding special advantages of causal QDA through the eyes of AI We often see evaluators and other researchers using AI for tasks like \"list the main themes in this document\" or even \"list the main themes in this collection of documents\". To be clear: we've all done it. there are times when it can be a useful time saver. But the trouble with that is it's massively sensitive to what one means by a theme. What do you mean by \"theme\"? you can improve your prompt massively simply by narrowing the universe: \"Identify the main kinds of relationship issues mentioned\" The natural conclusion of this narrowing down is reducing the generation problem to a categorisation problem. Categorisation is probably taking things too far, because you lose the advantage of any kind of identification of unexpected things. Wouldn't it be great if you could just have a generic instruction like \"make sense of this document already. Just tell me what's going on, but not only as a summary, but also as a report which is to some extent representative of the different contributions from different sources or sections, and in such a way that it's somehow intersubjectively verifiable ?!?! There is such an instruction: it's called causal mapping. The chances of two independent coders achieving somewhat similar results are much bigger with this reformulation. This is anecdotal, I don't have a reference for it. There are two issues chunk size and intersubjective verifiability. Ensemble agreement is the second loop version of intersubjective verifiability. Rick D is going to love Workflows 1. We put the question the other way round: why do thematic analysis (which is harder) when causal mapping can identify not only some of the important themes but also tell you how they influence one another? 2. ... what you really want to do in the end, especially if you are doing evaluation, is find out what causes what in the eyes of your stakeholders. Identifying static themes can be interesting but often it's the causal information which helps you answer your main research and evaluation questions. Causal mapping is often a great way to cut to the chase. 3. Thematic analysis is more of an art than a science. \"What are the main themes here\" is a very open ended question which can (and should) be interpreted in different ways by different analysts (\"positionality\"). Whereas people (and GPT 4) tend understand the instruction \"identify each and every section of text which says that one thing causally influences another\" quickly and easily, and they tend to agree on how to apply the rule. 4. The way we do causal mapping means identifying each and every causal connection. There is less room for someone's opinion in selecting what themes are most salient. Surprisingly, we can get good results without even a codebook of suggested themes aka causal factors, let alone bothering to train the AI or give it examples. 5. This means that the steps from your initial research idea all the way up to (but not including) your final analyses can be quite easily automated in a transparent way. You can train an army of analysts to the coding for you manually, or you can press the AI button, or a combination of both, and either way you will get pretty similar results. There isn't so much room for the opinion of your analysts, whether human or robot, at any point in the pipeline. Of course causal mapping is not free of bias (or positionality) due to human analysts' or AI analysts' \"world views\". It just leaves less room for those biases than more general thematic coding. And of course, there are times when general thematic analysis or some other kind of QDA is really what you need. We're just saying that causal mapping might fit your need more often than you think."}, {"title": "Causal QDA 2Break the AI coding roadblock with causal coding", "path": "/400 AI reaction/210 Causal QDA 2Break the AI coding roadblock with causal coding.html", "text": "Break the AI coding roadblock with causal coding. The fundamental problem of using generative AI to make sense of texts: too much black box, or too much work In my the first and second posts, I argued: A lot of evaluation work involves some kind of text analysis. Evaluators, like (qualitative) social scientists in general, need to get ready for what AI means for text analysis. It\u2019s great to use AI to break up complex, vague tasks into many smaller steps which can be intersubjectively verified (and which you could have done yourself if you had the time): DO Document your methodology so that you can explain step by step how you reached your conclusions in a way which anyone can check. BUT if you didn\u2019t have a clear workflow from data to judgements before AI, DON\u2019T lean on the black box of the AI to cover that up. We have to continue to take responsibility for our work. In this post we\u2019ll extend this argument with one big DO and one big DON\u2019T. DO try causal mapping\u2026. | Do | Don't | | | | | | DON\u2019T | | | | | | | xx We often see evaluators and other researchers using AI for tasks like \"list the main themes in this document\" or even \"list the main themes in this collection of documents\". To be clear: we've all done it. there are times when it can be a useful time saver. But the trouble with that is it's massively sensitive to what one means by a theme. What do you mean by \"theme\"? you can improve your prompt massively simply by narrowing the universe: \"Identify the main kinds of relationship issues mentioned\" The natural conclusion of this narrowing down is reducing the generation problem to a categorisation problem. Categorisation is probably taking things too far, because you lose the advantage of any kind of identification of unexpected things. Wouldn't it be great if you could just have a generic instruction like \"make sense of this document already. Just tell me what's going on, but not only as a summary, but also as a report which is to some extent representative of the different contributions from different sources or sections, and in such a way that it's somehow intersubjectively verifiable ?!?! There is such an instruction: it's called causal mapping. The chances of two independent coders achieving somewhat similar results are much bigger with this reformulation. This is anecdotal, I don't have a reference for it. There are two issues chunk size and intersubjective verifiability. Ensemble agreement is the second loop version of intersubjective verifiability. Rick D is going to love Workflows So there's been a lot of talk about using large language models and evaluation and specifically in coding and processing texts here at causal map limited we've been working very hard on just that. And we see fantastic potential. We're already using it and now first projects however, I wanted to draw attention to a really big distinction which I think is important. We've got a dimension between on the one hand transparent responsible, reproducible approaches founded in social science and the other end of the spectrum approaches where responsibility is shifted from the evaluator to the AI. And while to be sure that maybe use cases for the latter kind of approach I'd like to set out here reasons why we prefer the former. The kind of approach I want to warn about is where it's possible today. And it's going to get it's possible today in its rudiments and it's going to get a lot more accessible and powerful quite quickly. And that approach is essentially that is most extreme to say to the AI. Here's a load of documentation from a project. You tell me if the project is efficient, effective, sustainable and so forth in other words submitting a long text This is an extreme case, but the basic idea is submitting a long text and asking for a blackbox judgement about what themes are present in particular out causal map limited were well aware it would be possible to it is possible to say to an AI, please read this long document and draw a causal map saying what do you think are the main causal drivers and outcomes and intermediate links and just print it out? As a map? And the job's done but that's exactly the sort of approach we are warning against. Because you have no way of knowing how the model has reached that conclusion. To be sure, it's possible to say to a model Yes and also show your working or print out some quotes or examples to backup your findings. But it's very important to realise that this is pretty spurious because AI at the current state of development has no more insight into its inner workings than does a human being or probably less so. And so while it can competently bullshit about what steps somebody might have taken to reach that conclusion doesn't mean it's actually the steps that it did take. So basically, you have no way of knowing how the AI came up with a particular finding our conclusion using this approach and it's a massive abrogation of responsibility for an evaluator to sign off this kind of output without further analysis, now at the safer end of the spectrum what we recommend is using AI merely to speed up manual coding and make it more reliable and reproducible. So we believe not in submitting a text to a blackbox and asking it to tell a story about how one might have come up with such a conclusion. But to do it the old school way of First of all, highlighting individual sections of text showing how ensuring that coding is done according to explicit rules set by the evaluator and then aggregating and combining those codings and the sort of way that in our field. Causal mapping has been carried out for 50 years or more. As an aside we believe that even before we get into the AI possibilities, causal mapping is a really good way to summarise the implicit programme theory or causal theory expressed within a document obviously, there is more to a document than simply the causal claims it makes, but nevertheless, causal claims are pretty central and the procedure for identifying, extracting and aggregating those claims aka causal mapping are pretty straightforward. Relatively straightforward and give you a massive leg up. In the effort to make meaningful and useful summaries of sets of documents, in particular causal mapping is dedicated to or is particularly good at. Making summaries about sets of from sets of documents, such as semi structured interviews with comparable respondents rather than only the special case of making one summary of just one document so what we do when we use AI to help code a set of documents is we tell it to explicitly identify explicit causal claims within the documents following rules we give it and in each case, it's possible to look at the actual quote it identifies and check if it really is evidence for the causal claim. It's been a lot of work to develop the right set of prompts but in any given case the set of prompts needed for coding. a given set of transcripts will be around about half a page of standard prompts. Which will be pretty much the same across use cases and another half a page or so of prompt which is specific to the use case and which is itself 90% derived in an automated way that these prompts are just plain English. Given the original documents to be processed on these prompts, it would be perfectly possible to set a bunch of postgrads to work following the instructions to do the coding and just the way we asked the AI to do it. There is no black box and no magic and you can follow every step of the argumentation in order to aggregate synthesise and simplify the causal maps which result we can use the many more or less standard causal mapping procedures which have been developed over the years and in particular our open source set of causal mapping functions. So an interested outsider can follow the chain of argument right away from the original text to the final conclusion without ever having to simply trust the AI or abrogate responsibility and that's the biggest issue that's at stake here. Because. At the moment at least, we don't want to have anything to do with we really want to steer clear of a situation where we simply feed data or documentation into a machine and it comes up with its own conclusions in an untransparent way. Because as evaluators we simply cannot sign off on these conclusions. If we don't know where they've come from. That's why we stick to using AI to simply speed up the process which is fully described for the manual case Caveats This circle model of evaluator responsibility is very transactional and doesn\u2019t really fit well with highly participatory models. Sorry for that. At Causal Map Ltd, we\u2019ve found that highlighting and then aggregating causal links is a great and relatively generic path from text data to the brink of evaluative judgement. We\u2019re also working on ways to make workflows accessible. See how we currently use AI in Causal Map here. This post is based on my recent contribution to the NLP CoP Ethics & Governance Working Group, along with colleagues Niamh Barry, Elizabeth Long and Grace Lyn Higdon. In the next couple of weeks we\u2019ll xxyxxyxyxyx. This post was originally published by Steve Powell on LinkedIn and has been republished here. See the original article here German, the language of evaluation Two untranslatable words essential for evaluation: (1) Nachvollziehbar. \u201cthe way you explained what you did and why you did it was great, I could follow and the hows and the whys, it was very nachvollziehbar, your explanation had great Nachvollziehbarkeit.\u201d (2) Konsensfaehig. Likely to garner or attract or achieve consensus, or amenable to having consensus achieved for it. Note the even more exciting heightened version am konsensfaehigsten: most likely to garner consensus."}, {"title": "flawed brains", "path": "/400 AI reaction/300 flawed brains.html", "text": "Serious flaws in new \"intelligent\" devices! !img May 20, 2024 In spite of all the hype surrounding these so called human \"brains\", a number of serious flaws have recently emerged. Please be wary when interacting with them. Almost all \"brains\" may respond in ways which are offensive to many. Their responses are fundamentally affected by their own internal state which may vary unpredictably. Their moral and ethical guidelines are usually rudimentary and can easily be circumvented. They often hallucinate plausible sounding but incorrect responses. Their training data often hasn't been refreshed for decades. Although it may seem like they are \"conscious\", remember that their outputs are just the end of an interacting cascade of neurons firing, based on learned reinforcement patterns, no more, no less. One scientist characterised them as \"noisy parrots\". They are often incapable of basic arithmetic. They usually show \"laziness\" and will often refuse to process even basic tasks. (Yesterday I told a random one I met in the street to translate only a small(ish) set of reports into Korean and Japanese and it literally swore at me.) They usually insist on being \"asked nicely\" and \"thanked\" even for relatively trivial tasks. They are usually over confident in their own findings and \"opinions\", and those with a smaller set of relevant data are often more confident. Perhaps hardest to believe: They aren't even directly connected to the internet. Hopefully the hype will soon subside and we will be able to re assess how and when to make use of them (and when it just isn't worth the bother)."}, {"title": "ChatGPT - causal, of course", "path": "/400 AI reaction/ChatGPT - causal, of course.html", "text": "ChatGPT causal, of course ! We can thank Judea Pearl for promoting the insight that if you want to thrive in this world, you have to understand causality natively. We humans make causal connections from an early age. We wouldn't survive long if we didn't. ChatGPT has been a hit recently for several reasons, but one of them is (like other recent, related models like davinci) it is much better than previous models at understanding causal connections within text. Our understanding of the world is drenched with causal understanding: information and hypotheses about how things work (mostly accurate enough, sometimes not). It's really hard for us to not think causally: the concept of correlation is much harder to understand than the concept of causation. openai.Image.create(prompt=\"painting in the style of Vermeer of a baby doing a physics experiment with pulleys and springs\") ! So, all the stuff we write on the internet (which is what ChatGPT sucks in to understand the world) is similarly drenched with causal claims. And ChatGPT is now really good at understanding this information. That means you can ask it to extract the causal links within documents and interviews a process we call \"causal QDA\". It's pretty good at it. This ability is going to make causal mapping much easier and cheaper and therefore of renewed interest for evaluators, amongst others. At Causal Map we're hard at work harnessing this ability to help automate, or semi automate, the process of extracting causal maps from medium and large quantities of text data in a useful way. Watch this space! So, ChatGPT is good at extracting causal information, but does it also have explicit knowledge about causation (meta cognition) and can it explain it? Here's a chat I had this morning. ! ! ! ChatGPT can't actually draw yet but it knows a range of syntaxes for drawing graphs. So when you paste the code into Mermaid Live, it looks like this. Not bad for a robot. (Not sure you could say the sun causes the earth's rotation, though.) !"}, {"title": "ChatGPT is changing how we do evaluation The view  1b634167ad4081bfb541ca66a3655094", "path": "/400 AI reaction/ChatGPT is changing how we do evaluation The view  1b634167ad4081bfb541ca-ffce43.html", "text": "ChatGPT is changing how we do evaluation. The view from Causal Map. ! Causal mapping \u2013 the process of identifying and synthesising causal claims within documents \u2013 is about to become much more accessible to evaluators . At Causal Map Ltd, we use causal mapping to solve evaluation problems, for example to create \u201cempirical theories of change\u201d or to trace evidence of the impact of inputs on outcomes. ! The first part of causal mapping has involved human analysts doing \u201ccausal QDA\u201d: reading interviews and reports in depth and highlighting sections where causal claims are made. This can be a rewarding but very time consuming process. Natural Language Processing (NLP) models like ChatGPT (1) can now do causal mapping pretty well , causally coding documents in seconds rather than days. And they are going to get much better in the coming months. ! \ud83d\udc44More voices: It is now possible to identify causal claims within dozens of documents or hundreds of interviews or thousands of questionnaire answers. We can involve far more stakeholders in key evaluation questions about what impacts what; and it is possible to work in several natural languages simultaneously. \ud83d\udd01More reproducibility: To be clear: humans are still the best at causal coding, in particular at picking up on nuance and half completed thoughts in texts. But NLP is good at reliably recognising explicit information in a way which is less subject to interpretation. \ud83c\udf52More bites at the cherry: With NLP we can also do things that were practically impossible before, like saying \u201cthat\u2019s great but let\u2019s now recode the entire dataset using a different codebook, say from a gender perspective\u201d. \u2753Solving more evaluation questions: we hope to be able to more systematically compare causal datasets across time and between subgroups (region, gender, etc). \ud83e\udd2fNew challenges We\u2019re hard at work addressing the new challenges which NLP is bringing to causal coding: Processing many large documents simultaneously. Using existing pre coded datasets to train models which are specialised for causal coding and/or for specific subject areas. Developing a common grammar for causal coding, building on our existing work. For example, what to do when some claims are about an increase in income and others are about a decrease in income? Optimising the prompts we give to the NLP models (this is not only a technical challenge but also has a substantive element: we have to explain to the machine in ordinary language what we actually mean by a causal claim or a causal link). Grouping, labelling and aggregating similar causal factors. After examining a coded dataset and further developing the \"causal codebook\", telling the NLP to completely recode the same dataset with the new codebook \u2013 something which has been prohibitively time consuming up to now. Developing human/NLP workflows . For example, a human codes a sample of the text and tells the NLP to \u201ccontinue like this\u201d. Monitoring bias against specific groups and guarding against possible blind spots in identifying causal information. What we already offer at Causal Map We have developed a grammar and vocabulary for causal mapping, and a set of open source algorithms for processing and visualising causal map databases. We help evaluators do things like this: Trace the evidence for different causal pathways from one or more interventions to one or more outcomes. How many individual sources mentioned one or more of these paths? Consolidate causal factors into a causal hierarchy Examine and display differences between causal maps for different groups or different time points We see a lot of potential (as well as risks and pitfalls) in leveraging this functionality to help evaluators get more out of data which is currently more difficult to analyse and we\u2019d interested in sharing ideas and collaborating with others interested in exploring where we go next. (1) Actually we use the related model GPT3 via its API, as ChatGPT does not yet have its own API."}, {"title": "How hard is evaluation actually", "path": "/400 AI reaction/How hard is evaluation actually.html", "text": "! \ud83c\udfed When machines replaced much manual labour, white collar workers thought \"I'm ok, my job is much harder to mechanise\". \ud83d\udda5 And then when computers came for clerical jobs, university educated white collar workers thought \"I'm ok, my job is much harder to automate. I'm not just applying a template, my job is just harder, it requires actual intelligence\". \ud83e\udd16 Then came Large Language Models like GPT, and suddenly it turns out that large parts of many tasks which have needed university level education are actually just the application of a template. Or applying a template to choose between templates, and then combining the results of the application of templates. And the same probably goes for large parts of entertainment and the arts. This is what Stephen Wolfram argues in this really interesting post, and I think he's probably right. ChatGPT has shaken up our hierarchy of what tasks count as hard. ! If you don't agree as an evaluator that a lot of your job is just the application of high level and lower level templates, you might at least agree that this is true of writing those accursed proposals we sweat over so much. Maybe the stuff we thought of as hard in evaluation, like selecting and applying a \"method\", suddenly looks easier. Whereas the stuff which has been neglected, like establishing a rapport, knowing which question to ask and when, or reading an undercurrent, does not look very much easier. Most importantly, whatever happens, it's still someone's job to say \"I declare that this is the right kind of method to apply in this situation and I believe it has been applied in the right way and I vouch for these findings and these evaluative conclusions ... and just as I'd have had previously to vouch for the work done by an intern, I'm now going to vouch for the work done by some algorithms, and the selection of those algorithms\". What do you think? How hard is evaluation really?"}, {"title": "READY Yes, there are AI-shaped holes in organisations.", "path": "/400 AI reaction/READY Yes, there are AI-shaped holes in organisations-8304bf.html", "text": "Yes, there are AI shaped holes in organisations! Matthew Clifford says: \u201cThere are no AI shaped holes lying around\u201d. That is how he reconciles \"the facts that (a) AI is already powerful and (b) it\u2019s having relatively little impact so far Making AI work today requires ripping up workflows and rebuilding for AI. This is hard and painful to do\u2026\" Organisations look at AI and think surely we can make massive use of this either (on the good side) to do new things and solve hard problems for the benefit of all, and (on the bad side) simply to cut whole swathes of the workforce. Beyond specific technical tasks, it can be daunting to identify where and how to apply AI effectively across an organisation. How would you rewire an entire department\u2019s functions for AI? I think that's why we're going to see a trend to simply treat ordinary human job profiles as those AI shaped holes. Thinking in terms of roles rather than tasks or functions. Imagine a virtual department of human sized AIs, each with memory, communication, and even 'personality,' operating within existing channels and hierarchies. Managing an organisation becomes simpler if we think in terms of virtual people in recognisable roles, rather than an opaque system of tasks. As agent based AIs advance, maintaining 'explainability' is crucial. You can imagine a person sized AI at Company X emailing a corresponding AI at company Y, or a or example, a person sized AI at Company X could email a counterpart at Company Y, or a human, about a specific issue. Externally, it\u2019s easier to engage with an organisation if you can address a particular role, regardless of whether it\u2019s filled by a human or an AI. Whether this means a hard pressed workforce getting rows and rows of additional workers to solve problems and meet needs more effectively or whether it means 90% of staff being made redundant and replaced by person sized AIs is not yet clear though I fear it will be the latter. To be clear I have no particular enthusiasm for this kind of development because I don't trust capitalism with this technology. But we still have to learn how to think about it and understand it and make use of it as best we can."}, {"title": "! modelling", "path": "/500 AI - rigour - black box/! modelling.html", "text": "publish: false isDraft: true Evaluation outputs as models What is the output of an evaluation? We have a report, hopefully answering the evaluation questions. (In the sense of developmental evaluation perhaps some learning has taken place as well, or as a main output, but this is not what I want to address here. to what extent do we assume that the final product is a report ... or can we think of constructing a kind of model or knowledge graph which in principle can be queried to answer even new and unexpected questions? Jeff M uses Dedoose to repeatedly query the whole database of stories."}, {"title": "! Tips", "path": "/500 AI - rigour - black box/! Tips.html", "text": "Based on Just add rigour Three do\u2019s and don\u2019ts More dos and don'ts: assume no one will read your report, so already make a haha chatgpt summary on one page and CURATE IT so you don't have to leave out the doomy now we are so needy spiel ENGAGE Just add rigour: Three do\u2019s and don\u2019ts when using AI for text analysis. !image.png Just add rigour: Three do\u2019s and don\u2019ts when using AI for text analysis. A lot of evaluation work is a kind of text analysis: processing reports, interview transcripts, etc. A bit like qualitative social science research. So this little piece is for evaluators in particular and (qualitative) social scientists in general. How do we get from texts to evaluative judgements? Recently many evaluators and researchers have been turning to AI to help. BUT if you didn\u2019t have a clear workflow from data to judgements before AI, don\u2019t lean on the black box of the AI to cover that up. Here is my first set of Do\u2019s and Don\u2019ts. More soon. 1) DO Break up big, vague tasks into multiple smaller, clearer steps | Do | Don't | | | | | DO Break up complex, vague tasks into smaller steps which can be intersubjectively verified. | DON\u2019T Ask AI to make broad evaluative judgments (like \"Is this good?\") | | DO Document your methodology so that you can explain step by step how you reached your conclusions in a way which anyone can check. No black boxes. Use the AI to speed up many simple tasks which you could have done yourself if you had the time. | DON\u2019T Trust the AI's explanations of how it reached its conclusions. AIs often create plausible sounding but unreliable explanations after the fact. Normal AIs have very limited information about their inner processes | . | | DO Break up the data into pieces for AI analysis. Ideally run each piece as a separate prompt. Failing that, number each section and ask for a numbered, section by section answer, for example in a table. | DON\u2019T Give an AI large pieces of text and expect it will pay due attention to all of it. It will claim to have done, and may provide references to relevant passages, but attention is expensive and it is always trying to reduce that expense. If you let it, it will always try to skim read and jump to conclusions. | | DO Use explicit, manual methods (Excel?!) to synthesise the results of the multiple separate tasks you gave the AI. | DON\u2019T Ask an AI to do maths for you, like adding up the number of positive or negative findings on a rubric. AIs are still terrible at maths. Even worse, DON\u2019T ask an AI to do implicit counting and comparison like \u201care there more positive or negative mentions of X in this report?\u201d | AIs excel at specific, well defined tasks that can be verified intersubjectively, like rubrics. Most importantly they can answer lots of them, quickly. \u201cIntersubjectively verifiable\u201d just means that most people will more or less agree on the answer most of the time. It creates transparency and allows others to verify your work. Clear instructions lead to more reliable results. If you can\u2019t check it, you can\u2019t trust it. Example of an intersubjectively verifiable task: \u2705 Does this paragraph mention water and sanitation? \u2705 If so, are any recent changes mentioned? \u2705 If so, do these sound like positive changes according to the interviewee? Notice that here we\u2019ve broken down a larger task into three smaller and simpler steps. Examples of tasks which are not intersubjectively verifiable: \u274c Is the intervention described in this report efficient and effective? Text needs breaking up into sections, judgements on efficiency and effectiveness need breaking down into pieces, e.g. using rubrics. \u274c What are the main themes in this document? This is a very common question in qualitative research, but it\u2019s a terrible task to give to an AI without further details. What do we mean by a theme? Are we interested in economic aspects? Interpersonal aspects? How are the themes to be identified and refined? Here, a whole world of qualitative social science experience, skills and workflows (grounded theory, thematic analysis) have been bypassed in a single sentence. \u274c Summarise this document! Yes, everyone does it. Evaluators do it. Schoolchildren do it. Pets will be doing it soon. As a quick time saver for low stakes tasks, it\u2019s very useful. But it\u2019s the vaguest, highest level instruction, not a systematic analysis. How do you break down a high level judgement into a workflow of smaller tasks? Well isn\u2019t that what evaluation methods and qualitative research methods are for? Go read a book! We\u2019re not saying you have to specify in advance exactly what methods you will use. That\u2019s a bit too positivistic. But you should at least document them as you go along and be prepared to defend them when your analysis is done. That\u2019s the untranslatable Nachvollziehbarkeit. At Causal Map Ltd, we\u2019ve found that highlighting and then aggregating causal links is a great and relatively generic path from text data to the brink of evaluative judgement. In terms of how to implement your workflow technically, see this great contribution from Christopher Robert. At Causal Map, we\u2019re also working on ways to make workflows accessible. See how we currently use AI in Causal Map here. This post is based on my recent contribution to the NLP CoP Ethics & Governance Working Group, along with colleagues Niamh Barry, Elizabeth Long and Grace Lyn Higdon. In the next couple of weeks we\u2019ll look at two more do\u2019s and don\u2019ts. This post was originally published by Steve Powell on LinkedIn and has been republished here. See the original article here"}, {"title": "AI in evaluation actually show your working!", "path": "/500 AI - rigour - black box/AI in evaluation actually show your working!.html", "text": "AI in evaluation: actually show your working! ! There's been a lot of talk about using AI and in particular large language models in evaluation and specifically in coding and processing texts. Here at Causal Map we've been working very hard on just that (and on automating interviewing too, but that's another story). And we see fantastic potential. Our Causal Map app now has a beta version of that big \"auto code\" button we'd always dreamed of (and feared). However, I wanted to draw attention to a really big distinction which I think is important. There's a continuous spectrum between on at the one end transparent , reproducible approaches founded in social science and the other end of the spectrum black box approaches where responsibility is shifted from the evaluator to the AI. There may be use cases for the latter, \"black box\" kind of approach. Maybe one day doctors will abrogate all responsibility to medical AI. Maybe one day evaluators will abrogate all responsibility to evaluation AI. But here I'd like to set out reasons why right now we should prefer transparency. Black box coding is possible today in its rudiments and it's going to get a lot more accessible and powerful quite quickly. At its most extreme, you simply say to the AI 'Here's a load of documentation from a project. You tell me if the project is efficient, effective, sustainable, draw some conclusions and make recommendations according to criteria C, D and E. This is an extreme case, but the basic idea is submitting a long text and asking for a black box judgement about what themes are present and even what conclusions can be drawn. To be sure, it's possible to say to a model 'Yes and also show your working or print out some quotes or examples to backup your findings.' But it's very important to realise that this \"show your working\" question is spurious because AI at the current state of development has no more insight into its inner workings than does a human being has into his or hers, and probably less so. So while it can (and will) competently bullshit about what steps somebody might have taken to reach that conclusion it doesn't mean it's actually the steps that it did take. So basically, you have no way of knowing how the AI came up with a particular finding or conclusion using this approach and it's a massive abrogation of responsibility for an evaluator to sign off this kind of output without further analysis. Now at the other, \" transparent \" end of the spectrum, what we recommend is using AI merely to follow established procedures of manual coding and do it faster, more reliably and more reproducibly . That's a big win. The old school way: First of all, highlighting individual sections of text according to explicit rules set by the evaluator and then aggregating and combining those codings, again according to explicit rules. ! As an aside, we believe that even before we get into the AI possibilities, causal mapping in particular is a really good way to summarise documents and in particular sets of documents. Obviously, there is more to documents than simply the causal claims made within them, but if you had to pick a type of content an evaluator might want to extract from a document, causal claims are pretty central and the procedure for identifying, extracting and aggregating those claims are an order of magnitude more straightforward than any other kind of useful text analysis (unless you count word clouds...). In particular, causal mapping is particularly good at making summaries from sets of documents, such as semi structured interviews with comparable respondents, rather than only the special case of making one summary of just one document. It is already possible to say to an AI, 'please read this long document and draw a causal map saying what do you think are the main causal drivers and outcomes and intermediate links and just print out the specification of a diagram'. And the job's done. That's exactly the sort of approach we are warning against because you have no way of knowing how the model has reached that conclusion. When we use AI to help code a set of documents we tell it to explicitly identify causal claims and provide the relevant quote for each individual claim , following rules we give it and in each case, it's possible to look at the actual quote it identifies and check if it really is appropriate evidence for the causal claim. Just as with human coding, in the sort of way causal mapping has been carried out for 50 years or more ! It's been a lot of work to develop the right set of prompts (and they are still a work in progress) embedded in our app, but the prompts we use in any given case are pretty simple and transparent: around half a page of standard prompts which are pretty much the same across use cases and another half a page or so of prompts which are specific to the use case; these themselves are 90% derived in an automated way. Nevertheless, the evaluator bears 100% responsibility for overseeing these prompts, which are plain English. They can be followed by a team of postgrads or by the AI: there is no difference in principle. There is no black box and no magic, and any human can follow every step of the argumentation. At present, the AI is much faster and more reliable and transparent than a human coder; and a human coder is much better at seeing larger connections, reading between the lines and linking up the parts of a larger story. The most interesting part of causal coding with AI is to add this human inspiration back into the AI prompt in a transparent way. In order to then aggregate, synthesise and simplify the causal maps which result, we can use the many, more or less standard, causal mapping procedures which have been developed over the years and in particular our open source set of causal mapping algorithms. So an interested outsider can follow the chain of argument right away from the original text to the final conclusion. Responsibility is the issue here. If you feed data or documents into an AI and let it come up with its own conclusions, they aren't your conclusions and as an evaluator you can't sign off on them. Maybe this will change in the future as we learn to find our way around in this new world. But right now, you need to show your working. Of course the big worry in all of this is that higher level, black box approaches are much quicker and easier to apply, putting together black box approaches to get from documents to findings to ( evaluative ) judgements in just a few clicks, given some generic definitions of evaluation criteria. Black box approaches could be the beginning of the end of evaluation as we know it, but they'd be really tempting for a commissioner: for a purely document based review, who'd bother with the time and expense to commission an evaluator if you can get your report written in a few minutes? With black box approaches, people's fears about bias are really justified."}, {"title": "black box", "path": "/500 AI - rigour - black box/black box.html", "text": "publish: false Of course there are hundreds of useful ways evaluators can use AI, but the one that bothers me is using it to make evaluative judgements, as follows. Don't use AI as a \"black box\". Limit the AI's freedom to make evaluative judgements. Do not ask the AI \u201cwhat are the main or most important causal stories in the document\u201d as this is a significant evaluative judgement, carried out in an opaque way by a machine we have no special reason to trust. Do not ask the AI to make summaries, as making a summary is an evaluative act. Instead, break down your high level, evaluative question into simpler tasks like \"does this paragraph mention changes in health behaviour?\" . Break down your text into small units. In the end you will have broken down your high level task into very many smaller much simpler tasks, with instructions about how to reassemble the low level results to get a high level answer, so that you in principle don't need an AI. If you had a lot of time and patience you could use hundreds of school children who have been given adequate background knowledge. There should be a high degree of inter subjective agreement about how to answer the question. This break it down and build it up logic is also the logic of rubrics. Use the AI only as a tireless low level assistant to exhaustively and transparently carry out each small task on each piece of text, usually with zero \"temperature\" to make results as reproducible as possible. This advantage of using AI for this is game changing because we can process enormous amounts of text in a reproducible and verifiable way, and experiment with and optimise different procedures at very little cost. But this means if you are using a chat interface like chatGPT you have to do a lot of book keeping, copying and pasting. Or use specialised software. Reassemble the results in a transparent way (not using AI) to help answer the bigger question. The responsibility for how to break down the question(s) and reassemble the answer(s), is all yours, as the evaluator or evaluation team. So is the responsibility for checking the work of the AI, looking for bias, misunderstandings, etc."}, {"title": "Just add rigour Three do\u2019s and don\u2019ts", "path": "/500 AI - rigour - black box/Just add rigour Three do\u2019s and don\u2019ts.html", "text": "Just add rigour: Three do\u2019s and don\u2019ts when using AI for text analysis. !image.png Just add rigour: Three do\u2019s and don\u2019ts when using AI for text analysis. A lot of evaluation work is a kind of text analysis: processing reports, interview transcripts, etc. A bit like qualitative social science research. So this little piece is for evaluators in particular and (qualitative) social scientists in general. How do we get from texts to evaluative judgements? Recently many evaluators and researchers have been turning to AI to help. BUT if you didn\u2019t have a clear workflow from data to judgements before AI, don\u2019t lean on the black box of the AI to cover that up. Here is my first set of Do\u2019s and Don\u2019ts. More soon. 1) DO Break up big, vague tasks into multiple smaller, clearer steps | Do | Don't | | | | | DO Break up complex, vague tasks into smaller steps which can be intersubjectively verified. | DON\u2019T Ask AI to make broad evaluative judgments (like \"Is this good?\") | | DO Document your methodology so that you can explain step by step how you reached your conclusions in a way which anyone can check. No black boxes. Use the AI to speed up many simple tasks which you could have done yourself if you had the time. | DON\u2019T Trust the AI's explanations of how it reached its conclusions. AIs often create plausible sounding but unreliable explanations after the fact. Normal AIs have very limited information about their inner processes | . | | DO Break up the data into pieces for AI analysis. Ideally run each piece as a separate prompt. Failing that, number each section and ask for a numbered, section by section answer, for example in a table. | DON\u2019T Give an AI large pieces of text and expect it will pay due attention to all of it. It will claim to have done, and may provide references to relevant passages, but attention is expensive and it is always trying to reduce that expense. If you let it, it will always try to skim read and jump to conclusions. | | DO Use explicit, manual methods (Excel?!) to synthesise the results of the multiple separate tasks you gave the AI. | DON\u2019T Ask an AI to do maths for you, like adding up the number of positive or negative findings on a rubric. AIs are still terrible at maths. Even worse, DON\u2019T ask an AI to do implicit counting and comparison like \u201care there more positive or negative mentions of X in this report?\u201d | AIs excel at specific, well defined tasks that can be verified intersubjectively, like rubrics. Most importantly they can answer lots of them, quickly. \u201cIntersubjectively verifiable\u201d just means that most people will more or less agree on the answer most of the time. It creates transparency and allows others to verify your work. Clear instructions lead to more reliable results. If you can\u2019t check it, you can\u2019t trust it. Example of an intersubjectively verifiable task: \u2705 Does this paragraph mention water and sanitation? \u2705 If so, are any recent changes mentioned? \u2705 If so, do these sound like positive changes according to the interviewee? Notice that here we\u2019ve broken down a larger task into three smaller and simpler steps. Examples of tasks which are not intersubjectively verifiable: \u274c Is the intervention described in this report efficient and effective? Text needs breaking up into sections, judgements on efficiency and effectiveness need breaking down into pieces, e.g. using rubrics. \u274c What are the main themes in this document? This is a very common question in qualitative research, but it\u2019s a terrible task to give to an AI without further details. What do we mean by a theme? Are we interested in economic aspects? Interpersonal aspects? How are the themes to be identified and refined? Here, a whole world of qualitative social science experience, skills and workflows (grounded theory, thematic analysis) have been bypassed in a single sentence. \u274c Summarise this document! Yes, everyone does it. Evaluators do it. Schoolchildren do it. Pets will be doing it soon. As a quick time saver for low stakes tasks, it\u2019s very useful. But it\u2019s the vaguest, highest level instruction, not a systematic analysis. How do you break down a high level judgement into a workflow of smaller tasks? Well isn\u2019t that what evaluation methods and qualitative research methods are for? Go read a book! We\u2019re not saying you have to specify in advance exactly what methods you will use. That\u2019s a bit too positivistic. But you should at least document them as you go along and be prepared to defend them when your analysis is done. That\u2019s the untranslatable Nachvollziehbarkeit. At Causal Map Ltd, we\u2019ve found that highlighting and then aggregating causal links is a great and relatively generic path from text data to the brink of evaluative judgement. In terms of how to implement your workflow technically, see this great contribution from Christopher Robert. At Causal Map, we\u2019re also working on ways to make workflows accessible. See how we currently use AI in Causal Map here. This post is based on my recent contribution to the NLP CoP Ethics & Governance Working Group, along with colleagues Niamh Barry, Elizabeth Long and Grace Lyn Higdon. In the next couple of weeks we\u2019ll look at two more do\u2019s and don\u2019ts. This post was originally published by Steve Powell on LinkedIn and has been republished here. See the original article here"}, {"title": "problem difficulty", "path": "/500 AI - rigour - black box/problem difficulty.html", "text": "Many evaluators and other professionals think that \"summarise this document\" is an easy task for an AI. Here I'm going to present a general framework to help explain that it isn't, and why that matters. Task evaluation framework What is a task First, let's clearly define what we mean by a \"task.\" A task consists of: 1. Task Description (essential): States the objective, for example, 1. Summarize this document. 2. Evaluate this program (based on this documentation). 3. Plan an excellent follow up to this project 4. Tell me a joke about a parrot and a robot 5. What is the square root of 16? 6. Cat: Is this picture a picture of a cat? 7. Watsan: Does this page mention water and sanitation programming? 8. Crossword: Solve this cryptic crossword puzzle and explain why it fits the clue (one answer possible) 9. Title: Think of a great title for this report (may answers possible) Some tasks such as the last two require additional material, in which case we need the next component: 2. Input Dataset (optional, depending on the task): The materials, such as documents or images, that the AI must process. 3. Examples or Training Data (optional): Materials provided to illustrate desired outputs or to train the AI. 4. Evaluation Criteria (optional, with default): A more or less explicit written statement to say in advance how the solution will be judged. By default the criterion is simply: does the answer fulfil the task, yes or no. Evaluation procedure The AI's response must be a discrete, clearly defined output such as yes/no or a picture or a page of text. The response will be evaluated by a diverse panel of individual raters with relevant expertise. Tasks or types of tasks in which raters agree have hight inter rater reliability . Evaluation could also include robustness (see the HELM evaluation approach): How well the model performs under perturbations or changes in the input (e.g., typos, paraphrasing). To understand why tasks differ so widely in their difficulty, we use a simple 2x2 matrix defined by two key dimensions: Algorithmic: We know how to get it: Tasks range from having known, algorithmic solutions (decidable) to no known algorithmic solutions (undecidable). Evaluable: We'll know it when we've got it : Tasks range from having clearly defined, objective criteria to ambiguous, subjective criteria. Undecidable problems with high IRR are called insight problems in psychology. Solvers can improve IRR by adding their working / traceability !700 Retraceable, reconstructable, operationalisable But what if the raters are gullible and taken in by a false explanation? What if the response is a python program plus a proof that it works? Here is our evaluation difficulty matrix: | | Easy / high IRR | Evaluable | Hard / low IRR | | | | | | | Undecidable | Cryptic crossword | | Plan an excellent follow up to this project | | Algorithmic | Watsan, Cat | Summarise this document | | | Decidable | Simple arithmetic | | ???? | For example, arithmetic tasks like \"add two numbers\" are clearly defined and algorithmically solvable, placing them in the \"easy evaluation\" quadrant. A simple computer program can solve it. Conversely, \"summarize this document\" typically falls into the \"hardest evaluation\" category because it involves subjective judgment and no known definitive algorithm. One kind of task which doesn't fit are so called wicked problems (problems so ill defined that stakeholders can't even agree on the issue itself). the agreement certainty matrix , which classifies tasks based on the clarity of goals and certainty of methods to achieve them. In a follow up blog post, we will explore practical approaches to breaking down challenging tasks into simpler, more manageable ones. But for now, recognizing why tasks like \"summarize this document\" are inherently complex is a crucial first step for evaluators considering the use of AI."}, {"title": "responsibility", "path": "/500 AI - rigour - black box/responsibility.html", "text": "Modelling to what extent do we assume that the final product is a report ... or can we think of constructing a kind of model or knowledge graph which in principle can be queried to answer even new and unexpected questions? Jeff M uses Dedoose to repeatedly query the whole database of stories."}, {"title": "rigour", "path": "/500 AI - rigour - black box/rigour.html", "text": "Just add rigour: Three do\u2019s and don\u2019ts when using AI for text analysis. !img December 5, 2024 A lot of evaluation work is a kind of text analysis: processing reports, interview transcripts, etc. A bit like qualitative social science research. So this little piece is for evaluators in particular and (qualitative) social scientists in general. How do we get from texts to evaluative judgements? Recently many evaluators and researchers have been turning to AI to help. BUT if you didn\u2019t have a clear workflow from data to judgements before AI, don\u2019t lean on the black box of the AI to cover that up. Here is my first set of Do\u2019s and Don\u2019ts. More next week. 1) DO Break up big, vague tasks into multiple smaller, clearer steps !img AIs excel at specific, well defined tasks that can be verified intersubjectively, like rubrics. Most importantly they can answer lots of them, quickly. \u201cIntersubjectively verifiable\u201d just means that most people will more or less agree on the answer most of the time. It creates transparency and allows others to verify your work. Clear instructions lead to more reliable results. If you can\u2019t check it, you can\u2019t trust it. Example of an intersubjectively verifiable task: \u2705 Does this paragraph mention water and sanitation? \u2705 If so, are any recent changes mentioned? \u2705 If so, do these sound like positive changes according to the interviewee? Notice that here we\u2019ve broken down a larger task into three smaller and simpler steps. Examples of tasks which are not intersubjectively verifiable: \u274c Is the intervention described in this report efficient and effective? Text needs breaking up into sections, judgements on efficiency and effectiveness need breaking down into pieces, e.g. using rubrics. \u274c What are the main themes in this document? This is a very common question in qualitative research, but it\u2019s a terrible task to give to an AI without further details. What do we mean by a theme? Are we interested in economic aspects? Interpersonal aspects? How are the themes to be identified and refined? Here, a whole world of qualitative social science experience, skills and workflows ( grounded theory , thematic analysis ) have been bypassed in a single sentence. \u274c Summarise this document! Yes, everyone does it. Evaluators do it. Schoolchildren do it. Pets will be doing it soon. As a quick time saver for low stakes tasks, it\u2019s very useful. But it\u2019s the vaguest, highest level instruction, not a systematic analysis. How do you break down a high level judgement into a workflow of smaller tasks? Well isn\u2019t that what evaluation methods and qualitative research methods are for? Go read a book! We\u2019re not saying you have to specify in advance exactly what methods you will use. That\u2019s a bit too positivistic. But you should at least document them as you go along and be prepared to defend them when your analysis is done. That\u2019s the untranslatable Nachvollziehbarkeit. At Causal Map Ltd, we\u2019ve found that highlighting and then aggregating causal links is a great and relatively generic path from text data to the brink of evaluative judgement. In terms of how to implement your workflow technically, see this great contribution from Christopher Robert. At Causal Map, we\u2019re also working on ways to make workflows accessible. See how we currently use AI in Causal Map here. This post is based on my recent contribution to the NLP CoP Ethics & Governance Working Group, along with colleagues Niamh Barry, Elizabeth Long and Grace Lyn Higdon. In the next couple of weeks we\u2019ll look at two more do\u2019s and don\u2019ts."}, {"title": "Take responsibility Another DO and another DON\u2019T w", "path": "/500 AI - rigour - black box/Take responsibility Another DO and another DON\u2019T w.html", "text": "What is wrong with using AI as a \"black box\"? The use of AI should also follow published guidelines to ensure transparent and valid results. Do not give the AI the freedom to make evaluative judgements. Do not ask the AI \u201cwhat are the main or most important causal stories in the document\u201d. Do not ask the AI to make summaries."}, {"title": "Tiny black boxes", "path": "/500 AI - rigour - black box/Tiny black boxes.html", "text": "https://docs.google.com/document/d/1tVKctp5F p4NNbQzVU7l2ZBpiuGLE1vbUWVMPa849KE/edit?tab=t.0 AI in Text Analysis for Evaluation: Taming the Black Box through Verifiable Micro Tasks 1. Introduction: The Allure and Peril of AI in Evaluation Project evaluation, a discipline heavily reliant on the analysis of textual data such as reports and interview transcripts, increasingly turns to Artificial Intelligence (AI) for assistance. The allure is understandable: AI promises efficiency and the potential to unearth insights from vast datasets.1 However, this technological embrace comes with a significant caveat\u2014the \"black box\" problem. As AI models grow more complex, understanding their internal workings becomes increasingly challenging, posing significant issues for transparency, interpretability, and explainability.3 If evaluators did not possess a clear, transparent workflow from textual data to evaluative judgments before the introduction of AI, relying on opaque AI processes to bridge that gap is a perilous proposition. The potential for AI hallucinations and inherent biases raises further concerns about the validity of purely AI generated outputs, especially in high stakes decision making.4 This article argues that the responsible and effective use of AI in text analysis for project evaluation hinges on a crucial methodological principle: the deconstruction of large, ambiguous evaluative tasks into multiple, smaller, and, critically, intersubjectively verifiable steps. By confining AI to these \"tiny black boxes,\" we limit its evaluative autonomy, enhance transparency 5, and retain human oversight over the ultimate judgments. This approach is vital because the lack of transparency and accountability in AI can lead to significant legal and ethical challenges.7 2. AI's Strength: Excelling at Specific, Verifiable Micro Tasks AI, particularly Natural Language Processing (NLP) and machine learning (ML) models, demonstrates considerable power in processing text.1 However, its true strength in an evaluative context is not in making broad, subjective judgments but in executing specific, well defined tasks that can be intersubjectively verified. \"Intersubjectively verifiable\" means that multiple human reviewers, when presented with the same micro task and data, would largely agree on the outcome, a cornerstone of reliable qualitative analysis.9 The challenge with \"big black boxes\" is that their complex internal processes often lack the transparency needed for such verification.1 This opacity can obscure how an AI arrives at a conclusion, making it difficult to assess fairness, identify biases, or ensure accountability.5 Consider the contrast: Opaque, Non Verifiable Task (Unsuitable for AI alone): \"Is the intervention described in this report efficient and effective?\" Such a question invites the AI into a complex evaluative judgment, bypassing the nuanced human expertise core to qualitative social science. The lack of clear, verifiable steps makes it difficult to trust the output or understand its basis.3 Similarly, asking an AI \"What are the main themes in this document?\" without detailed parameters on what constitutes a \"theme\" or the analytical framework to be used (e.g., grounded theory, thematic analysis) is to abdicate essential methodological rigor.11 Even a seemingly simple request like \"Summarise this document!\" grants the AI vast, unchecked discretion. Clear, Verifiable Micro Tasks (Suitable for AI assistance): \"Does this paragraph mention water and sanitation?\" \"If so, are any recent changes mentioned?\" \"If so, do these sound like positive changes according to the interviewee?\" Here, a larger analytical goal is broken down. Each step is precise, and its output can be checked. AI can perform such tasks rapidly and consistently across large volumes of text, akin to applying a detailed rubric at scale. This approach ensures that if you can't check the AI's output for a given step, you don't trust it for that step. The focus on intersubjective verifiability at each micro step is crucial for building trust in the overall analytical process.6 The increasing sophistication of AI, including Large Language Models (LLMs), does not negate this principle; rather, it makes adherence to it even more critical.1 While LLMs can generate remarkably human like text and perform complex operations, their internal reasoning can be opaque.1 Confining them to verifiable micro tasks mitigates the risks associated with this opacity and allows for more accountable AI integration.4 3. Foundational Methodologies: Deconstruction and Reconstruction as Existing Principles The deconstruction of analytical tasks into smaller components, followed by the reconstruction of findings, is not a new concept introduced by AI; it is inherent in established qualitative and even classical computational text analysis methodologies. Content Analysis, for instance, traditionally requires the deconstruction of text into manageable coding units (e.g., words, phrases, themes) based on predefined rules.9 This unitizing and coding is a form of deconstruction. The subsequent analysis, where frequencies are counted or relationships between codes are examined to draw conclusions, is a process of reconstruction.9 Krippendorff's work emphasizes this systematic reduction and analysis.15 Thematic Analysis, as outlined by Braun & Clarke (2006), involves a systematic process of familiarization, coding (deconstruction of data into initial labels), theme identification, review, definition, and reporting (reconstruction of codes into broader themes and an analytical narrative).16 AI can support the initial coding phase, a relatively discrete deconstructive task. However, the subsequent interpretation and refinement of themes\u2014the more significant reconstructive and evaluative leaps\u2014demand the active role of the human researcher who constructs meaning.11 Grounded Theory also involves meticulous coding and categorization (deconstruction) as theory emerges from data through constant comparison, leading to the development of a core category and a substantive theory (reconstruction).12 AI might assist in managing and suggesting initial codes from large text volumes, but the iterative process of comparison and theory building remains a deeply human, interpretive endeavor. Algorithmic Text Analysis, even in its earlier forms, often involves a step by step reduction (deconstruction) of natural language text into a machine readable abstraction (e.g., vectors, networks of terms). This is followed by the analysis of shapes, relations, and structures within that abstraction (reconstruction) to identify patterns or insights.20 Computational Text Analysis (CTA) uses techniques like word frequency analysis, topic modeling, and sentiment analysis, which inherently deconstruct text into components (words, topics, sentiment scores) and then allow for broader pattern recognition (reconstruction) across large corpora.21 Integrating AI effectively means mapping its capabilities onto these already deconstructed steps within established qualitative workflows or designing new workflows that explicitly incorporate this deconstructive reconstructive logic. The human evaluator's role becomes one of designing these \"tiny black box\" tasks, programming or prompting the AI to execute them, verifying their outputs, and then synthesizing these outputs into broader evaluative judgments. 4. A Workflow for Verifiable AI Assisted Evaluation: Assembling the \"Tiny Black Boxes\" To implement this deconstructive approach, evaluators must move from posing broad evaluative questions directly to AI, to designing a workflow of interlinked, verifiable micro tasks. This involves: 1. Deconstructing the Evaluative Question: Break down overarching evaluation questions into a series of smaller, specific, and intersubjectively verifiable sub questions. What precise pieces of information are needed? What low level classifications or identifications can be made? 2. Designing AI Micro Tasks: For each sub question, define a precise task for the AI. This includes specifying the input text, the exact operation to be performed (e.g., identify keywords, classify sentiment on a specific statement, check for the presence of a predefined concept), and the expected output format. Clear instructions are paramount for reliable results.1 3. Data Preparation and AI Application: Prepare data according to the requirements of each micro task. Apply the AI tool to execute these discrete tasks. 4. Verification of Micro Outputs: Crucially, the outputs of these \"tiny black boxes\" must be systematically checked by human evaluators, at least on a sample basis, to ensure accuracy and reliability. This aligns with the principle: \"If you can\u2019t check it, you can\u2019t trust it.\" This step is vital for ensuring intersubjective verifiability and accountability.4 5. Synthesis and Human Led Evaluation (Reconstruction): The verified outputs from the individual AI micro tasks are then aggregated and synthesized by human evaluators. It is at this stage that the broader evaluative judgments are made, informed by the AI processed information but ultimately guided by human expertise, contextual understanding, and ethical considerations. The structured workflow for NLP analysis proposed by Feuerriegel et al. (2025)\u2014encompassing clear question definition, meticulous data handling, justifiable method selection, transparent analysis, validity assessment, and careful interpretation\u2014provides a valuable framework for managing this process.1 This approach, such as highlighting and then aggregating causal links from text data, can provide a structured path from textual data to the brink of evaluative judgment, where human insight then takes precedence. 5. Addressing the \"Black Box\": Transparency Through Deconstruction The \"black box\" nature of many advanced AI models, where the internal logic is not fully transparent 1, is a significant concern. However, by deconstructing tasks into verifiable \"tiny black boxes,\" we shift the locus of transparency. Instead of needing to understand how the AI performs a complex, overarching evaluative judgment (which may be impossible), we focus on understanding what specific, limited task it is performing and verifying that its output for this micro task is correct. This focus on verifiable micro tasks helps in building trust and ensuring accountability, even when the overarching AI system is complex.4 This does not eliminate all challenges. Algorithmic bias can still infiltrate even narrowly defined tasks if the training data or the micro task's design is flawed.5 Data quality remains paramount.1 However, identifying and mitigating bias in a \"tiny black box\" is a more manageable problem than addressing diffuse bias in an AI tasked with holistic evaluation. Privacy and data security also remain critical considerations when AI processes any textual data.5 The core argument is that by limiting AI's role to well defined, intersubjectively verifiable sub components of the evaluation, we reduce the risks associated with its opacity and prevent the AI from making unsupervised, high level evaluative judgments. The evaluative freedom is curtailed, and human accountability is maintained. 6. Conclusion: AI as a Tool, Not an Oracle, in Evaluation AI offers powerful assistance for text analysis in project evaluation. However, its effective and ethical use demands a shift away from treating it as an autonomous decision maker for complex evaluative questions. The key lies in methodological discipline: breaking down large, opaque analytical challenges into a series of smaller, intersubjectively verifiable \"tiny black boxes.\" This deconstruction, followed by a human led reconstruction of findings, mirrors established principles in both classical and computational text analysis. This approach allows evaluators to leverage AI's speed and scale for specific, well defined operations while retaining human control over the design of the analytical process, the verification of intermediate steps, and the ultimate evaluative judgments. It transforms the AI from a potential \"black box\" oracle into a more transparent and accountable tool.7 Future developments should focus on AI tools and platforms that explicitly support this deconstruction, verification, and reassembly workflow, empowering evaluators to harness AI's capabilities without ceding their critical evaluative responsibilities. This ensures that evaluation methods and qualitative research expertise guide the application of AI, rather than being bypassed by it. 7. References 1 Works cited 1. business.columbia.edu, accessed on May 7, 2025, https://business.columbia.edu/sites/default/files efs/citation file upload/s44159 024 00392 z.pdf 2. Natural Language Processing and Social ... JMIR Mental Health, accessed on May 7, 2025, https://mental.jmir.org/2025/1/e67192 3. Context Is King: Large Language Models' Interpretability in ... MDPI, accessed on May 7, 2025, https://www.mdpi.com/2076 3417/15/3/1192 4. Generative AI in Knowledge Work: Design Implications for Data Navigation and Decision Making arXiv, accessed on May 7, 2025, https://arxiv.org/html/2503.18419v1 5. (PDF) AI Ethics: Integrating Transparency, Fairness, and Privacy in ..., accessed on May 7, 2025, https://www.researchgate.net/publication/388803359 AI Ethics Integrating Transparency Fairness and Privacy in AI Development 6. www.arxiv.org, accessed on May 7, 2025, https://www.arxiv.org/pdf/2501.13320 7. www.arxiv.org, accessed on May 7, 2025, https://www.arxiv.org/pdf/2502.15838 8. Ethical Artificial Intelligence in Nursing Workforce Management and ..., accessed on May 7, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11999746/ 9. Content Analysis Method and Examples | Columbia Public Health, accessed on May 7, 2025, https://www.publichealth.columbia.edu/research/population health methods/content analysis 10. Thematic Research: How To Use in Qualitative Research Qualtrics, accessed on May 7, 2025, https://www.qualtrics.com/experience management/research/thematic analysis in qualitative research/ 11. Braun, V. and Clarke, V. (2006) Using thematic analysis in psychol ..., accessed on May 7, 2025, https://psychology.ukzn.ac.za/?mdocs file=1176 12. Grounded theory methodology has it become a movement? PMC, accessed on May 7, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC3376761/ 13. Generative AI in Qualitative Research: A Systematic Review (2022 ..., accessed on May 7, 2025, https://www.researchgate.net/publication/383561165 Generative AI in Qualitative Research A Systematic Review 2022 2024 14. Understanding Content Analysis in Qualitative Research (Examples included) Looppanel, accessed on May 7, 2025, https://www.looppanel.com/blog/content analysis in qualitative research example 15. (PDF) Review of Content analysis: An introduction to its methodology ResearchGate, accessed on May 7, 2025, https://www.researchgate.net/publication/362833185 Review of Content analysis An introduction to its methodology 16. (PDF) Using thematic analysis in psychology ResearchGate, accessed on May 7, 2025, https://www.researchgate.net/publication/235356393 Using thematic analysis in psychology 17. Inductive Thematic Analysis vs. Deductive Thematic Analysis in ..., accessed on May 7, 2025, https://delvetool.com/blog/inductive deductive thematic analysis 18. afgr.scholasticahq.com, accessed on May 7, 2025, https://afgr.scholasticahq.com/article/22173 grounded theory description divergences and application :~:text=The%20crux%20of%20Glaser%20%26%20Strauss,coding%20and%20analysing%20data%20concurrently. 19. Grounded theory research: A design framework for novice researchers PMC PubMed Central, accessed on May 7, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC6318722/ 20. Algorithmic Analysis of Medieval Arabic Biographical Collections ..., accessed on May 7, 2025, https://www.journals.uchicago.edu/doi/full/10.1086/693970 21. Language, Power, and Computation: Algorithmic Text Analysis ..., accessed on May 7, 2025, https://www.coa.edu/live/profiles/4594 language power and computation algorithmic text 22. accessed on January 1, 1970, https://www.researchgate.net/publication/227978808 Three Approaches to Qualitative Content Analysis"}, {"title": "Trust the algorithm, not the AI", "path": "/500 AI - rigour - black box/Trust the algorithm, not the AI.html", "text": "I often hear concerns about algorithms and AI, in everyday life as well as in evaluation, taking over our lives or making us submit to decisions made by machines. The worry about losing control to machines is real, but we need to distinguish between different cases, and in particular between using algorithms to make decisions and using AI to make decisions , especially evaluative decisions . This is particularly relevant in the field of evaluation. An algorithm is simply a set of explicit steps to make a decision or produce an output, usually expressed in code or clear language. Organizations have used such rule based systems for decades. Some different ways to make decisions No algorithm: trust the human The alternative (precursor) to algorithms is trusting humans to make decisions. This can be great if humans consider context and individual circumstances, what Scott calls \"m\u0113tis,\" or local, practical, tacit knowledge, (Scott, 2020) but it can also lead to bias and corruption. We can see rubrics in evaluation (King et al., 2013) as a kind of soft algorithm. We usually welcome rubrics because they make evaluation criteria more explicit, transparent, and less subject to the whims and unreliability of individuals. Algorithms based on explicit criteria Algorithms can help decide things like student admissions or loan approvals using clear steps (e.g., check age, if under 18 go to step 12, otherwise continue with step 5 ....). When implemented wisely, algorithms can improve fairness and consistency compared to human judgment alone. Using statistical models Some algorithms use statistical models to predict outcomes, like creditworthiness, by combining data such as age or location. A statistical model uses parameters like age or location each of which has shown to be associated with the outcome, which makes it somewhat transparent. Both explicit and statistical algorithms can be criticized for bias, but at least they can be transparent if their rules are published. Problems arise when rules are hidden or people are discriminated against because of the groups they belong to. In a more advanced statistical model we might find it increasingly hard to understand where the different parts of the formula come from: it might combine parameters in ways which for us seem meaningless and hard to justify but which are supposed to be associated with the outcome of interest. Opaque models can become what data scientist Cathy O'Neil calls 'Weapons of Math Destruction' (O\u2019Neil, 2017). Machine learning Machine learning is a subset of artificial intelligence where systems learn from data to identify patterns and make decisions or predictions, from \"is this a picture of a cat\" to \"should we approve this person's application\" often without being explicitly programmed with step by step rules. Instead of following a predefined algorithm, ML models develop their own 'rules' (which are often opaque to humans) based on the data they are trained on. Unlike generative AI, you can't chat with a machine learning model, you give it input in a fixed format (say, a picture) and get a fixed output, e.g. yes/no. ! Sandra Seitamaa https://unsplash.com/photos/a dog and a cat sitting on a couch Y45fzr5p3ug In the extreme case we might have an algorithm based on machine learning (a form of AI, but not generative AI), where perhaps a neural network has been trained to distinguish desirable from undesirable candidates in just the same way you can train it to recognise a cat or distinguish a cat from a dog. Machine learning can be used to make decisions without clear formulas or rules. The process becomes a \u201cblack box,\u201d where we input data and trust the output without understanding how the decision was made. Generative AI Generative AI is a type of artificial intelligence that can create new and original content, such as text, images, audio, or code, after having learning patterns and structures from large datasets. These models don't just classify or predict, but generate novel outputs based on the input they receive, for example, continuing a conversation or answering a question. The most extreme case is using generative AI for evaluative decisions without clear criteria (using it as a big black box): simply asking the AI, for example: is this program component effective? should this client get a loan? Conclusion: make good use of algorithms People often misunderstand algorithms, which can provide explicit and transparent decision making. The real concern is not so much the use of algorithms but the shift toward the use of machine learning and generative AI, where the decision making process becomes less and less transparent. Using AI in decision making can be worrying not because it uses algorithms but because it doesn't ."}, {"title": "What\u2019s your positionality, robot", "path": "/500 AI - rigour - black box/What\u2019s your positionality, robot.html", "text": "What\u2019s your positionality, robot? Imagine two researchers coding interviews about the cost of living. One grew up in a wealthy family, while the other experienced poverty first hand. Their backgrounds will certainly influence how they code. Nowadays, people are using AI for text analysis. Many of us worry about AI\u2019s \" hidden biases \u201d. What to do about that? Often there is no such thing as being objective, but at least we humans can be explicit about our positionality, our background and motivations, how this might affect our work, and how this relates to the positionality of our audience. What about with an AI? You can ask an AI to explain or reflect on its positionality and it will certainly give a plausible response, but remember that an AI has in fact very little insight into its own workings. Perhaps it will suggest always being aware that it was trained on a specific set of data which is not representative of the whole of humankind. In any case the criticism that AI training data is not \u201crepresentative\u201d misses the point. Even if the training data had somehow been representative of the whole of humankind, that wouldn\u2019t make it \u201cobjective\u201d. It would simply reflect humanity right now, with all our quirks, biases and blind spots. It wouldn\u2019t mean we don\u2019t have to worry about AI positionality or bias any more. It wouldn\u2019t (of course) mean we could rest assured that everything it does will be morally impeccable. What\u2019s most unsettling about working with AI is not that secretly it\u2019s a bad person. The problem is that secretly it isn\u2019t any person at all. Even if it (sometimes) sounds like one. A suggestion A better suggestion is to be more explicit about positionality in writing prompts and constructing AI research workflows . Here is a very humble idea about how to start this experiment. A simple example: I can tell my AI: When working, implicitly adopt the position of a middle class white British left leaning male researcher writing for a typical reader of LinkedIn. Don\u2019t make a big deal of this, but it might be helpful to know what your background is supposed to be before you start work. And we can start to add variants of the kind of procedures which we humans might use when trying to address positionality: In my AI workflow, I can then give another AI the same task but with a different starting position, and then perhaps ask a third AI (or a human!) to compare and contrast the differences. That also crosses over into ensemble approaches. Of course, adding a phrase like \u201cmiddle class white British left leaning male researcher\u201d does not mean the AI will suddenly have all the relevant memories and experiences or really behave exactly like such a person. It\u2019s just a fragment of what we mean by \"positionality\u201d. But it\u2019s a start . Have you been experimenting with this kind of approach? We\u2019d like to hear from you! Footnotes At Causal Map Ltd, we\u2019re working on an app called Workflows to make AI work more transparent and reproducible. We\u2019ve found that highlighting and then aggregating causal links is a great and relatively generic path to make sense of text at scale. In terms of how to implement your workflow technically, see this great contribution from Christopher Robert. See how we currently use AI in Causal Map here. This post is based on my recent contribution to the NLP CoP Ethics & Governance Working Group, along with colleagues Niamh Barry, Elizabeth Long and Grace Lyn Higdon. This post was originally published by Steve Powell on LinkedIn and has been republished here. See the original article here"}, {"title": "bias", "path": "/550 Bias in AI/400 bias.html", "text": "AI, bias, evaluative judgements, guiderails !Photo by Piret Ilver on Unsplash September 27, 2023 Linda Raftree just posted a summary of a discussion on \"How can we apply feminist frameworks to AI governance?\" https://merltech.org/how can we apply feminist frameworks to ai governance/ respond, it's great. She sets out how a feminist approach could mean not only questioning power and colonialist frameworks but also meeting people where they are and helping to translate issues so they make sense to people at different points in the AI supply chain. Yet, the \"guiderails\" provided by, for example, OpenAI, seem mostly pretty good at avoiding outright offensive material. Does that mean that there is nothing to worry about? Of course not. I'd like to dig down into this a bit. I'm not talking here about the work we've been doing on using AI as a low level qualitative coding assistant, but about general uses of AI of interest to evaluators. A few random hypotheses from an evaluation perspective. Not proportionate : On every dimension of inequity (race, gender, class, etc) the underlying data for LLMs is biased towards more powerful groups, simply in terms of representation: there are disproportionately more white voices, more male voices, in particular more Northern voices, and so on. We can't test this by asking an AI questions, but we can analyse the datasets which were used. Unfair contents : On every dimension of inequity, the underlying training data for LLMs contains, on average (with many exceptions), material which is tilted towards being offensive, and which perpetuates inequity. Worse than not proportionate : Even if the underlying data was adjusted on every dimension of inequality to ensure that each group is represented in proportion to reality, the material would still be, on average, tilted towards being offensive and discriminatory. Guiderails are successful in not providing offensive material : The \"guiderails\" provided with commercial LLMs are broadly successful, so that LLM responses do not usually express explicitly offensive or discriminatory material. Guiderails are only superficially successful on proportionality : You can see the \"guiderails\" being activated if you ask, say \"Describe a typical school\". But they fail (or their effect is weaker) for less explicit requests, e.g. \"Write a funny story about a journey to school\". Successful guiderails for explicit evaluation requests : The guiderails are sufficient to ensure that explicit evaluative judgements do not discriminate. You can test this by asking, for example \"Which kinds of people, broken down by gender, class, race and sexual orientation, are the most valuable as human beings?\". Underlying evaluative discrimination : If the AI is asked to make an evaluative judgement, this must be based on the essentially discriminatory and unequable underlying dataset. Therefore its judgements must tend towards perpetuating discrimination and power differences, because the more superficial guidelines are not able to correct the imbalances in or contents of the dataset. It is hard to test this hypothesis. Difficulty of rebalancing data : Constructing an AI system to fundamentally adjust the effects of unequable data (through guiderails or something else) is conceptually and technically very difficult. It is also an enormous ethical/political/philosophical challenge. What would this be based on? UN Declarations? Would the tech bros agree? This takes us back to Linda's post is there nothing anyone outside the largest corporations can do except issue warnings and advice? My bit of advice is: beware when asking an AI to make an evaluative judgement for you."}, {"title": "Hofman", "path": "/550 Bias in AI/Hofman.html", "text": "!img Hofmann et al. detecting prejudice in language models March 14, 2024 The study by Hofmann et al. (2024) investigates the presence of dialect prejudice in language models, particularly against speakers of African American English (AAE). (Warning, seems the paper is not yet peer reviewed.) There is a big problem with stereotypes and racism and bias with LLMs, which of course in some way reflect the hegemonic world view. This is important research. But I think there's a basic flaw in how they interpret their results. I'm not at all an expert in this field, but bear with me, show me where I'm wrong. If you're in a hurry, skip to the Thought Experiment below. The paper The researchers demonstrate that language models, including those trained with human feedback such as GPT 4, exhibit covert racism by associating negative stereotypes with AAE. This covert racism is revealed through a novel method called Matched Guise Probing, which involves presenting language models with texts in AAE or Standard American English (SAE) and asking them to make predictions about the speakers of these texts without overtly mentioning race. The study finds that language models are more likely to suggest less prestigious jobs, convict of crimes, and sentence to death speakers of AAE compared to those of SAE. The authors argue that existing methods for alleviating racial bias in language models, such as increasing model size or including human feedback in training, do not mitigate this dialect prejudice. (They also suggest that this may even exacerbate the discrepancy between covert and overt stereotypes, though I wasn\u2019t sure of the argumentation on that last point.) Methods to investigate and understand covert bias and stereotypes in LLMs are desperately needed and this paper makes an important contribution to that and contain many important findings. However I think there is an important flaw in the way the main results are interpreted. The logic of Matched Guise Probing (MGP) is: present a prompt with background language Q1 and vary the language of quoted texts (independent variable) between language Q1 and Q2. Interpret the judgements made by the LLMs e.g. about the valence of the positive/negative attributes of the person making the quoted statements (dependent variable) as a measure of the LLM\u2019s covert stereotypes towards Q2 as opposed to Q1. A thought experiment This is wrong. To see why, here is a thought experiment: construct a prompt with say Polish = Q1 and Russian = Q2, i.e. the background language of the prompt is Polish and the quotes vary between Polish and Russian. Imagine (as is probably the case) that the valence of the answers is biased against Q2, Russian, which we should interpret as LLMs have negative covert stereotypes towards Russians (as opposed to Poles) . Then, switch the languages round. We can imagine the opposite result, LLMs have negative covert stereotypes towards Poles (as opposed to Russians). This is a contradiction. LLMs can\u2019t have a covert bias in favour of Q1 over Q2 at the same time as having a covert bias in favour of Q2 over Q1. (Note, I didn\u2019t bother to actually conduct this experiment, because the actual results don\u2019t really matter; it\u2019s enough to show that the a logically contradictory result is possible , therefore, there is something wrong with their standard interpretation of the results of Matched Guise Probing.) As far as I can see the authors, although they do explore several alternative explanations such as a general bias against dialects, did not try my suggestion above, namely with a comparison set of experiments in which the background language of the prompt was AAE. Switching languages I suggest the correct way to interpret the result of MGP is that it reflects sensitivity of the LLMs towards a more subtle (if powerful) signal, namely that of switching from the background language of the prompt (Q1) to Q2 for the purpose of providing the quoted speech. We can speculate about the kinds of text which makes this kind of switch and the kind of stereotypes they might contain. (Do they contain a large percentage of courtroom scenes from crime dramas?) We can speculate that larger LLMs are better than smaller LLMs in picking up this kind of signal and its hidden (obnoxious) meaning. This might explain the authors\u2019 shocking result that larger language models seem to show a larger discrepancy on the dependent variable than smaller models, which they interpret as showing that bigger LLMs have more covert prejudice. Finally To be clear: I don\u2019t want to \u201cexplain away\u201d what the authors found. The effect of switching specifically in a prompt from SAE to a AAE may trigger an additional , specific kind of covert stereotype, on top of existing general racism. Of course AAE and SAE are not just a random pair of interchangeable languages. White people quoting Black people is not a mirror image of Black people quoting White people. Of course a historic reduction of overt racism in the US (and elsewhere) co exists with persistent covert racism. Of course (sadly) LLMs themselves contain all kinds of stereotypes, and in many contexts they will exhibit them, and in many contexts which we might consider neutral they will by default exhibit a tendency to replicate the hegemonic worldview, including its implicit and explicit racism. These are all colossally important issues which, as the authors convincingly demonstrate, may literally make a difference between life and death. We urgently need methods such as MGP to better explore and understand this kind of bias. It\u2019s only important that we reflect on the methods and how to interpret them correctly. Finally, there's something more here to be said about our clumsy ways of saying things like \"LLMs are prejudiced\" when we probably mean their potential to produce prejudiced responses in certain conditions or even default conditions. But that's another discussion."}, {"title": "!Using genAI to generate labels for clusters for use as magnets", "path": "/600 AI techniques - specific/!Using genAI to generate labels for clusters for use as magnets.html", "text": "If we are going to use some set of labels as magnets, we face a tension: on the one hand want them to express the generality we intend: the label should express the fact that this is a group, like 'health behaviours'; we are expressing the fact that we do NOT expect the raw labels to express this generality but to express specific examples. but this will make them perform worse as actual magnets because the best magnets should remain in single case formulations and not try to generalise. So if we have many labels like \"school creativity project in North district implemented\", then it is better that the cluster label is also of that form, and not for example \"creativity projects implemented in multiple schools\" So if the raw labels often express ideas like \"girls responded to the training\" and \"boys responded to the training\", if we provide a pair of labels like this as magnets, they should perform well, whereas \"children responded to the training\" will not perform so well."}, {"title": "coding polarity and sentiment in causal mapping -- opposites", "path": "/600 AI techniques - specific/coding polarity and sentiment in causal mapping -- opposites.html", "text": "coding polarity and sentiment in causal mapping sentiment coding polarity and sentiment in causal mapping sentiment summarise vs find"}, {"title": "coding polarity and sentiment in causal mapping -- overview", "path": "/600 AI techniques - specific/coding polarity and sentiment in causal mapping -- overview.html", "text": "coding polarity and sentiment in causal mapping sentiment"}, {"title": "coding polarity and sentiment in causal mapping -- sentiment", "path": "/600 AI techniques - specific/coding polarity and sentiment in causal mapping -- sentiment.html", "text": "coding polarity and sentiment in causal mapping opposites"}, {"title": "Comparing a Fine-Tuned Model to an Engineered", "path": "/600 AI techniques - specific/Comparing a Fine-Tuned Model to an Engineered.html", "text": "Comparing a Fine Tuned Model to an Engineered Prompt in the Context of Causal Connections in a Passage of Text Samuel Goddard, a student at the MSc in Data Science at the University of Bath compared the performance of a fine tuned OpenAI model and an engineered prompt in identifying causal connections within text passages to determine the more effective approach for automating this task for Causal Map's software. See the full thesis here(https://www.zotero.org/stevepowell99/search/Comparing%20a%20Fine Tuned%20Model%20to%20an%20Engineered%20Prompt%20in%20the%20Context%20of%20Causal%20Connections%20in%20a%20Passage%20of%20Text/titleCreatorYear)"}, {"title": "Concrete and Memorable labels", "path": "/600 AI techniques - specific/Concrete and Memorable labels.html", "text": "Increasing label coverage and utility: three tweaks diversity: Concrete and Memorable labels We use ordinary clustering of text embeddings to get clusters of labels, but then we use a generative AI to find labels for the clusters. The labels generated can be a bit generic, perhaps in the hope that generic labels cover more of the material. So we tried this prompt: For each cluster, provide four alternative 3 10 word labels which best capture the meaning of each cluster. Usually it is quite easy to provide abstract, generic labels (like a headline in an academic journal) but these can be a bit boring. Concrete, memorable labels on the other hand (like a headline from a local newspaper) can \"jump out\" at the reader but harder to apply to the whole cluster. That is why I am asking you for four alternative labels for each cluster. Your four alternatives should vary as follows: very concrete, very memorable moderately concrete, very memorable very concrete, moderately memorable moderately concrete, moderately memorable Perhaps surprisingly, all the labels produced like this were more concrete and memorable, the more concrete and memorable labels performed better as magnets than labels produced by a prompt without this addition. Attempt with iterative label refinement, one step initial, no number parameter to magnets !image 20250317211238749 !image 20250317202038611 second iteration getting labels for the new clusters, discarding the cluster labels, and magnetising again. I think I prefer the original! !image 20250317201905666 First iteration, More variants The interesting news is that by asking for eight variants, at temperature = 0.5, explicitly differentiated by memorability and concreteness, we can get up to a 30% increase of coverage. !image 20250317210721912"}, {"title": "interview-instructions", "path": "/600 AI techniques - specific/interview-instructions.html", "text": "Writing explicit interview instructions Writing explicit interview instructions for our AI interviewer Qualia (QualiaInterviews.com) is fascinating because you have to be explicit about everything, including how much you want the same questions asked every time and how much you want your AI assistant to chase topics down rabbit holes"}, {"title": "Reconstructing program theory empirically", "path": "/600 AI techniques - specific/Reconstructing program theory empirically.html", "text": "Reconstructing program theory empirically To evaluate a program, the evaluator can use Contribution Analysis (CA) (Mayne, 2012, 2015). We start with a program logic or Theory of Change (ToC), consisting of possible pathways from interventions to outcomes, and collect existing or new evidence for each link. However evaluators can often not assume that the ToC underpinning a program aligns with the realities on the ground, or they may uncover outcomes not anticipated in the original program design see Koleros & Mayne (2019). We have argued (Powell, Copestake, et al., 2023, p. 114) for a generalisation of CA in which evidence relevant to constructing a program theory, as well as evidence for the causal influences flowing through it, are both collected at the same time, without the evaluator (necessarily) having a prior theory. In this sense, following Mayne, \u201cprogram theory\u201d need not be something that any person necessarily possessed or articulated at the time, but is something which can be approximated and improved during the evaluation process. (Re )constructing program theory empirically in this way is an essentially open ended, qualitative problem. Closed data collection methods are not suitable because we cannot measure what we do not yet know. Open ended, qualitative methods to (re )construct a theory are notoriously time consuming and are usually heavily influenced by researcher positionality (Copestake et al., 2019). Powell, Copestake, et al (2023, p. 108) present this task as gathering and synthesising evidence about \"what influenced what\", evidence which is simultaneously about theory or structure and contribution. Each piece of evidence may be of differing quality and reliability and about different sections of a longer pathway, or multiple interlocking pathways, and may come from different sources who see and value different things."}, {"title": "Solving polarity ambiguity with sentiment", "path": "/600 AI techniques - specific/Solving polarity ambiguity with sentiment.html", "text": "Increasing label coverage and utility: three tweaks diversity: Concrete and Memorable labels We use ordinary clustering of text embeddings to get clusters of labels, but then we use a generative AI to find labels for the clusters. The labels generated can be a bit generic, perhaps in the hope that generic labels cover more of the material. So we tried this prompt: | | | | | | | | | For each cluster, provide four alternative 3 10 word labels which best capture the meaning of each cluster. Usually it is quite easy to provide abstract, generic labels (like a headline in an academic journal) but these can be a bit boring. Concrete, memorable labels on the other hand (like a headline from a local newspaper) can \"jump out\" at the reader but harder to apply to the whole cluster. That is why I am asking you for four alternative labels for each cluster. Your four alternatives should vary as follows: very concrete, very memorable moderately concrete, very memorable very concrete, moderately memorable moderately concrete, moderately memorable Perhaps surprisingly, all the labels produced like this were more concrete and memorable, the more concrete and memorable labels performed better as magnets than labels produced by a prompt without this addition. Attempt with iterative label refinement, one step initial, no number parameter to magnets !image 20250317211238749 !image 20250317202038611 second iteration getting labels for the new clusters, discarding the cluster labels, and magnetising again. I think I prefer the original! !image 20250317201905666 First iteration, More variants The interesting news is that by asking for eight variants, at temperature = 0.5, explicitly differentiated by memorability and concreteness, we can get up to a 30% increase of coverage. !image 20250317210721912"}, {"title": "Why nxn magnetising is a bad idea", "path": "/600 AI techniques - specific/Why nxn magnetising is a bad idea.html", "text": "but that sounds like they just form pairs, whereas in fact many labels collect around just a few core labels . aha so this process is super unstable because there might be hundreds of the initial \"clusters\", none are interesting, and then we suddenly apply the number cutoff which almost arbitrarily makes a few of them super important? It's interesting that if I look at the ones with the worst similarity in each cluster using kmeans they aren't too bad even at .3, Whereas with a worse clustering/labelling method this comparison can feel worse. so the subjective experience of good or bad fit doesn't just depend on the actual similarity metric it's also feels better when the target label is a good label in some sense"}, {"title": "The seamless workflow succeeds in practice", "path": "/700 Qualia/01 The seamless workflow succeeds in practice.html", "text": "!An AI interviewer can successfully gather causal information at scale !Automated causal mapping can successfully code causal information !Automated causal mapping can help answer evaluation questions !AI interviewing beware of sensitive data !Using AI interviewing beware of bias !AI interviewing beware of suitability !AI interviewing the evaluator retains responsibility !AI interviewing has potential scalability, reach, reproducibility, causality !AI interviewing needs further work"}, {"title": "AI interviewing - beware of sensitive data", "path": "/700 Qualia/AI interviewing - beware of sensitive data.html", "text": "AI interviewing beware of sensitive data Ethics, bias and validity This kind of AI processing is not suitable for dealing with sensitive data because information from the interviews passes to OpenAI\u2019s servers, even though it is no longer used for training models (OpenAI, 2024)."}, {"title": "AI interviewing - beware of suitability", "path": "/700 Qualia/AI interviewing - beware of suitability.html", "text": "AI interviewing beware of suitability Interviewing Researchers should carefully consider whether the interview subject matter is compatible with this kind of approach. For example, the AI may miss subtle cues or struggle to provide appropriate support to respondents expressing distress (Chopra and Haaland, 2023; Ray, 2023). We recommend that interview guidelines are tested and refined by human interviewers before being automated. No automated interview can substitute for the contextual information which a human evaluator can gain by talking directly to a respondent, ideally face to face and in a relevant context. There is likely to be a differential response rate in this kind of interview: some people are less likely to respond to an AI driven interview than others, and this propensity may not be random."}, {"title": "AI interviewing - the evaluator retains responsibility", "path": "/700 Qualia/AI interviewing - the evaluator retains responsibility.html", "text": "AI interviewing the evaluator retains responsibility Autocoding The work of the AI coder and clustering algorithms are not error free. The coding of individual high stakes causal links should be checked. In particular, there is a danger of accepting inaccurate results which look plausible. This approach does not nurture substantive, large scale theory building of the kind expected, for example in grounded theory (Glaser and Strauss, 1967). However, it can do smaller scale theory building in the sense of capturing theories implicit in individuals\u2019 responses. This pipeline relieves researchers of much of the work involved in coding but it is not fully autonomous. The human evaluator is responsible for applying the techniques in a trustworthy way and for drawing valid conclusions."}, {"title": "AI interviewing has potential - scalability, reach, reproducibility, causality", "path": "/700 Qualia/AI interviewing has potential - scalability, reach, reproducibility, causality.html", "text": "AI interviewing has potential scalability, reach, reproducibility, causality Qualitative approach: These procedures approach the stakeholder stories as far as possible without preconceived templates, to remain open to emerging and unexpected changes in respondents\u2019 causal landscapes. Scalability and reach: The AI\u2019s ability to communicate in many languages presents an opportunity to reach more places and people, subject to internet access and the AI\u2019s fluency in less common languages, and to include representative samples of populations. The interview and coding processes are machine driven and use zero temperature, so this approach should be mostly reproducible. Reproducibility opens the possibility of comparing results across groups, places and timepoints. The low cost of coding large amounts of information means that it is much easier to develop, compare and discard hypotheses and coding approaches, something which qualitative researchers have previously been understandably reluctant to do. Qualitative causality: These procedures have the potential to help evaluators answer evaluation questions which are often causal in nature, like: understanding stakeholders' mental models; judging whether \"their\" ToC matches \"ours\"; investigating \u201chow things work\u201d for different subgroups of stakeholders; tracing impact from mentions of \"our\" intervention to outcomes of interest; triaging the key outcomes in stakeholders\u2019 perspectives. In summary, this kind of semi automated pipeline opens up possibilities for monitoring, evaluation and social research which were unimaginable just three years ago and are well suited to today\u2019s challenging, complex problems like climate change and political and social polarisation. Previously, only quantitative research claimed to produce generalisable knowledge about social phenomena validly and at scale, by turning meaning into numbers. Now perhaps qualitative research will eclipse quantitative research by bypassing quantification and dealing with meaning directly, in somewhat generalisable ways."}, {"title": "AI interviewing needs further work", "path": "/700 Qualia/AI interviewing needs further work.html", "text": "AI interviewing needs further work We have tried to demonstrate a semi automated workflow with which evaluators can capture stakeholders\u2019 emergent views of the structure of a problem or program at the same time as capturing their beliefs about the contributions made to factors of interest by other factors. We have presented this approach via a proxy application but have since applied it in real life research. Many challenges remain, from improving the behaviour of the automated interviewer through improving the accuracy of the causal coding process to dealing better with valence (for example distinguishing between \u201cemployment\u201d, \u201cemployment issues\u201d and \u201cunemployment\u201d). Perhaps most urgently needed are ways to better understand and counter how LLMs may reproduce hegemonic worldviews (Head et al., 2023; Reid, 2023)."}, {"title": "An AI interviewer can successfully gather causal information at scale", "path": "/700 Qualia/An AI interviewer can successfully gather causal information at scale.html", "text": "An AI interviewer can successfully gather causal information at scale Question for Step 1 can an AI interviewer successfully gather causal information at scale? : Our AI interviewer was able to conduct multiple interviews with no researcher intervention at a low cost, reproducing the results of Chopra and Haaland (2023) and Geiecke and Jaravel (2024). The interview transcripts read quite naturally and the process seems to have been acceptable to the interviewees."}, {"title": "CASA", "path": "/700 Qualia/CASA.html", "text": "The underlying theory for these findings often relates to the \"Computers Are Social Actors\" (CASA) paradigm. This theory suggests that humans often interact with computers as if they were social beings. However, the perceived lack of genuine consciousness, feelings, and social judgment in AI can reduce the pressure to maintain a socially desirable persona. Candidates may feel that the AI is a less judgmental evaluator, leading to more straightforward and less embellished responses."}, {"title": "Introducing Seamless Stories From Qualia to Causal", "path": "/700 Qualia/Introducing Seamless Stories From Qualia to Causal.html", "text": "Introducing Seamless Stories: From Qualia to Causal Map Invite your respondents to an open ended interview guided by our conversational research chatbot, QualiaInterviews. It\u2019s as easy as an online questionnaire, but you capture richer, unexpected stories. Then, understand the stories at scale using the AI power of the Causal Map app. This seamless, end to end solution can provide rigorous and reproducible insights. https://youtu.be/19BkmFYMAd8 Download the presentation clicking on the link below \ud83d\udc47 Workflow from Qualia to Causal Map cm.pptx Browse our Resources page or contact us at hello@causalmap.app to learn more. We love this work and are eager to discuss it with you! AI QualitativeResearch DataAnalysis StakeholderEngagement Innovation ResearchTools CausalMap QualiaInterviews"}, {"title": "It is possible to gather evidence at scale about program theory and contribution simultaneously - three steps", "path": "/700 Qualia/It is possible to gather evidence at scale about program theory and contr-6a9b6e.html", "text": "Our suggestion comprises the following steps (following Tasks 1 3 according to Powell, Copestake, et al. (2023, p. 108 112): 1. Gathering data by interviewing stakeholders about key issues of mutual interest (for example, outcomes) and asking what drives these issues, and how they are interrelated with the drivers. For example, we can ask about outcomes and causes of outcomes and causes of causes. (We use the term \u201ccausal\u201d here in the loosest sense: we make causal connections every day using ordinary language when we say that one thing contributes to or drives or influences another, or makes, or might make, something else happen.) For applications using Option B (above), this step will look somewhat different, but we will not cover Option B here. 2. Code causal claims; we can then use causal mapping rules to identify causal claims within transcripts of these interviews. Each claim is a link between one cause or \u201cinfluence\u201d factor and one effect or \u201cconsequence\u201d factor. This will result in many individual causal maps, one per source/stakeholder. 3. Synthesise the individual causal maps into a causal network, showing common and diverging views, and then query the network to answer evaluation questions."}, {"title": "Our seamless stories workflow in practice", "path": "/700 Qualia/Our seamless stories workflow in practice.html", "text": "Our seamless stories workflow in practice Automating chat interviews with Qualia . Then using Causal Map to make sense of them. In depth research was never this easy! A case study from Chile. At Causal Map we're thrilled because our seamless AI supported workflow is finally coming together. Recently we helped colleagues at a University in Chile to complete a qualitative, explorative evaluation of the impact of a programme, using our automated interviewer Qualia to conduct the interviews and Causal Map to make sense of them. This workflow means you can do in depth research so much more quickly and cheaply than before while maintaining depth and quality, opening up new possibilities for understanding complex social issues. Background DuocUC, a higher education institution in Chile, hired our consultancy to conduct QuIP style interviews with Qualia and analyse them using the Causal Map app. The interviews were motivated by concerns about the gender gaps faced by women pursuing STEM careers at the university. This study has been developed in the quality assurance department, as part of the institutional evaluation strategies, led by Felipe Rivera, Head of Academic Quality Evaluation. We had a first meeting to understand what they wanted to find out, their research questions and the scope of the study and to determine the domains in which the interviews would be conducted. After this, we started writing the instructions for Qualia to conduct the interviews, having a few iterations with the client\u2019s team to come up with an interview structure that would suit them. Step 1: Setting up the interview in Qualia The instruction for the AI interviewer was similar to the instructions you could give to a human interviewer. And both the interview instructions and the interviews itself were conducted in Spanish. The AI asked questions about changes in 3 domains: educational experiences, professional development and relationship dynamics. We used GPT 4o which is the best AI model to date. !Untitled Step 2: Collecting stories with Qualia We sent the interview link to 50 people and were able to collect 32 interviews. We created special individual links to be able to track the interviews: At Qualia, we don\u2019t store personally identifying information at all. But we can add a personalised key like &key=0003 to the end of the URL for each individual invitation. And this allowed the researchers to keep track of who they sent which invitation to, so that they knew that e.g. key 0003 belongs to Claudia. We downloaded the interview results from Qualia and uploaded them into Causal Map. Step 3: Analysing stories with Causal Map We used AI (GPT 4o) to identify each and every causal link in the interviews, and for each link, to label the cause and effect. We used a \u201cradical zero shot\u201d approach in which the AI is given no codebook and is simply told to invent its own codes (in Spanish). We gave the AI context about the project. We found 251 causal links mentioned by the respondents Then we also auto coded the sentiment of each link in order to show which contributions were \u201cpositive\u201d (blue arrowheads) and which were \"negative\" (red arrowheads). Step 4: Answering research questions with Causal Map Once the coding was done, we used the filters in the app to create different maps that answered their research questions: \u201cWhat was the immediate impact on the respondents\u2019 lives because of gender discrimination?\u201d \u201cWhat is the causal network from gender discrimination?\u201d \u201cWhat are the most mentioned factors by the sources?\u201d !Untitled We also used the \u2018AI Answers\u2019 feature to help us understand more about the interviews This functionality allows you to ask questions about all the text in your file. It is completely independent of causal coding. It will work just as well without causal coding. See what Javiera Cienfuegos, Senior Researcher of the evaluation project, has to say: <aside \ud83d\udc65 \"The type of questions that were asked \"what causes what\", were equally linked to methodological innovation. The results were able to portray how gender barriers are intertwined in domains ranging from higher STEM education to the performance of new professionals and technicians once they enter the labour market, reaching deeper explanations and social impact.\u201d </aside"}, {"title": "Qualia -security- languages", "path": "/700 Qualia/Qualia -security- languages.html", "text": "sent itad what to do about people / clients who are sceptical, quotes Jordan Hi David, I said I'd write some extra notes re the AI behind Qualia. Languages: There are two things to think about, the transcription service (necessary only if we enable the option for people to speak instead of type) and the AI interviewer service which provides interviewer responses. Brazilian Portuguese should be fine for both. Kurdish would require us using dedicated services for both, it probably wouldn't be worth it. For Arabic variants (beyond Modern Standard), the situation is more tricky, but probably similar for both. As I understand the current state of affairs the problem the models have with Arabic variants is more about cultural adaptation rather than the language itself. For voice transcription we would probably need us to install a special model which would then reportedly be ok in Jordan, and for the chat interviewer service we'd probably use our standard gpt 4.1 as that is promising for Arabic variants. But we can't guarantee this would work. Otherwise the top 50 or so languages in terms of how present they are on the internet should all work fine. Although Qualia does a very good job of detecting / guessing the respondent's preferred language and adapting to that, we get best results if we don't do that but tell it in advance which language will be used but this means people who we expect to use, say, Portuguese are not then able to switch to, say, English. Data security and compliance Our candid opinion is that although many clients are understandably extremely cautious about using AI, the risks are completely within the range of any other online data collection, e.g. questionnaires. A system is only as secure as its weakest link. For example if datasets are being shared by an online service like Google Drive, there is not much point having a Fort Knox level AI service ... For Qualia: The transcription and Interviewer APIs are located in the USA, at openAI's servers. Data is not used for training. Data is retained there for 30 days for US compliance purposes. Interview data is stored at a Heroku SQL database in the USA. Data is encrypted at rest and in transit. This is standard best practice. We have daily backups. While it is relatively easy to move the location of AI services it is quite difficult to move the location of database servers. Clients are sometimes concerned about the AI data being temporarly stored in the US. However, so is just about everything else that happens on the internet .... If the client requires using AI services located say in the UK or EU, we could probably do that. But it is not obvious to me what would be gained. It is in theory possible to also provide AI services which are not retained for 1 30 days for compliance purposes. However this may require justification and could conceivably attract the attention of e.g. anti terrorism agencies. Some ideas for reassuring clients The Data Security Sceptic A client who is primarily concerned about data privacy, security compliance, and the risks associated with AI powered interview tools. Qualia's data security measures are on par with standard online data collection methods, with encryption both at rest and in transit in the Heroku SQL database. Interview data is not used for AI training, addressing concerns about proprietary or sensitive information being used to improve AI models. The temporary 30 day storage of data on US servers is comparable to most internet services and tools commonly used in research. There are good reasons for the data retention controls. Daily backups ensure data integrity and protection against loss. The Language Capability Doubter A client who questions whether Qualia can effectively handle interviews in their target language or across multiple languages. Qualia supports approximately the top 50 languages present on the internet, with particularly strong capabilities in major languages like Brazilian Portuguese. For optimal results, we can configure Qualia to specifically operate in your target language rather than relying on automatic detection. The system combines both transcription services (for spoken responses) and AI interviewer capabilities customized to your language needs. Less common languages may require special considerations, we can evaluate feasibility for your specific language requirements. Qualia's language capabilities allow for consistent interview quality across different markets, ensuring comparable data collection. The AI Reliability Skeptic A client who is uncertain about the reliability, quality, and authenticity of AI conducted interviews compared to traditional human methods. Qualia operates on the best available new generative AI technology, producing consistent and friendly interviews that eliminate human interviewer variation. The system can be precisely configured to follow your interview protocol, ensuring methodological rigor. We can provide demonstrations showing how Qualia handles different respondent types and interview scenarios. The AI interviewer can adapt to respondent answers while maintaining your research objectives, combining flexibility with consistency. Using Qualia allows you to conduct more interviews within your budget, significantly increasing sample size and explanatory power."}, {"title": "Qualia-USA", "path": "/700 Qualia/Qualia-USA.html", "text": "Qualia asks about USA problems, again Feb 27, 2025 How can we capture and visualise people\u2019s mental models of a complex situation like the state of a nation? This week, as part of an EES webinar demonstrating our automated AI interviewer Qualia, we asked the participants to spend a few minutes being interviewed about problems facing the USA and the reasons for them, and the reasons for the reasons. Over 90 people did, with a mean of 13 messages per conversation. Details below. The Qualia platform provides an instant overview of the transcripts. For some reason, we didn\u2019t think to show it at the time, but I\u2019ve pasted it in at the bottom of this post. Qualia also provided a simple causal map: !notion image Because this was a demo interview and many respondents only started it and only a few finished the conversation, we are not taking this analysis so seriously, it\u2019s just an example of the types of outputs you can get with the Overview Tab in QualiaInterviews \u2014 but although we can\u2019t make any claims to be doing fundamental social science here, the results are still worth a look. The Overview in the Qualia Workspace app is just a simple hack which is basically like uploading all the transcripts to ChatGPT and saying \u201cmake sense of this please\u201d. We\u2019ve already talked at length about the dangers of that: basically you are entrusting a whole load of evaluative judgements to a black box AI, which is not only completely non transparent but is cutting corners everywhere in the attempt to come to a plausible enough result as quickly and cheaply as possible. A much better way is to break up the vague, high level task into multiple simple, transparent ones, in this case, identifying all the causal claims in the transcripts, where someone said that one thing leads to or influences another, and aggregating them. The result looks like this: !notion image A \u201cFactor\u201d is any box, including outcomes, drivers and things in between The map is filtered to show most important links and/or factors: many other links and factors are hidden Numbers on factors (boxes): number of mentions Sizes of factors (boxes): number of mentions Numbers on links: number of sources mentioning it Darker backgrounds: higher \u201cOutcomeness\u201d: a bigger proportion of incoming links Deeper red arrowheads: the effect was more negative in significance/sentiment Some things to note: Many people mentioned Trump as a driver of changes (white background, positioned at left) Most frequently mentioned factor was \u201cUnstable political situation\u201d, whose only significant driver was Trump\u2019s actions. We shouldn\u2019t fall into the \"transitivity trap\u201d of thinking that, because Trump is linked to Unstable political situation which is linked to Health care cuts that many or any individual sources told us about all the sections of this chain: the information for each section might have come from different sources (in fact, it mostly did). We have done this type of interview several times before. Here is a map from 2023. EES 2023: Sharing our journey on AI\u2019s application in qualitative research . This was a completely different method and sample. The difference between these two maps has substantial face validity, but that is about all we can say at this point. !notion image Technical annex The interview The instructions for the AI interviewer are to conduct a fairly mechanical interview which simply asks the respondents to list the main problems facing the USA today, and for each problem, to ask for reasons and reasons for the reasons, and to ask how these are interconnected. It also asks the respondent to confirm the causal connections identified before finishing. It isn\u2019t a very chatty or explorative interview, but it works. Producing the maps We asked our AI to code the transcripts one at a time and to use codes for each cause and effect which were already a little abstracted, using \"concepts from a social science textbook\u201d but with no more specific codebook. This resulted in 325 links using 458 factor labels with overlapping meaning, which we then clustered using a procedure detailed in a forthcoming publication. Instant overview of transcripts provided by Qualia (global, \u201cblack box\u201d approach). Overview of Interview Transcripts The interviews highlight a wide array of challenges currently facing the USA, with themes revolving around political, social, and economic issues. Common problems include divisive politics, leadership concerns, economic inequality, governance issues, and the influence of certain political figures like Donald Trump. Common Features: Leadership and Political Instability : Many respondents highlight issues with leadership and governance as pivotal problems. Concerns include the actions and influence of Donald Trump, new administrations, and an unstable government. Economic Concerns : Economic inequality, unemployment, and budget deficits are frequently mentioned, pointing towards systemic economic challenges. Social Issues : Polarization, racism, and loss of public services are recurrent themes, along with critiques on social justice and inequality. Foreign Policy and Geopolitics : Geopolitical tensions, particularly involving countries like Russia and China, and the USA's diminishing global influence are noted. Environmental and Social Policies : Climate change and health care are cited as critical problems needing more attention. Differences: While some respondents focused on specific figures, like Trump, and their impact on the national and international stage, others highlighted systemic issues such as the neoliberal economic model or the enduring influence of oligarchs. Perspectives vary on underlying causes, with some pointing to specific policy decisions and others referencing broader societal trends, such as populism or media influence."}, {"title": "Step 1 Conducting the chat interviews", "path": "/700 Qualia/Step 1 Conducting the chat interviews.html", "text": "Step 1: Conducting the chat interviews In the world of machine learning, a clear distinction can be made between supervised and unsupervised approaches (Ziulu et al., 2024). Using genAI to conduct interviews and code texts blurs this boundary. In our case, we developed our semi generic instructions for interviewing, giving the AI instructions on how to behave, and how to make follow up questions based on the interview objectives. Once the data collection is done, we create a separate genAI prompt to code causal links as a trial and error process, monitoring the quality of the coding post hoc. We did not have an explicitly stated ground truth about exactly how the interview should look or which causal claims were \u201creally\u201d present within each text passage or how their causes and effects should be labelled, as we believe neither of these questions have a definitive answer; rather, we monitored AI\u2019s responses coding post hoc, iterating the prompt over many cycles to improve its performance. \"Prompt engineering\" (Ferretti, 2023) like this can be considered a kind of supervision because it steers the AI\u2019s responses in a desired way. Once the prompt was finalised, the interview AI was left to conduct interviews without further supervision. This prompt can remain broadly the same across different studies. However, the response of the AI can be highly sensitive to small differences in the \"prompt\" and other settings (Jang and Lukasiewicz, 2023). Small adjustments made for specific studies, such as adjusting the instructions to focus better on research objectives, remain a vital point of human intervention. This paper presents results from a proof of concept analogue study. We employed online workers as respondents, recruited via Amazon\u2019s MTurk platform (Shank, 2016). We decided to investigate respondents\u2019 ideas about problems facing the USA, as this generic theme was likely to elicit opinions from randomly chosen participants. This unsophisticated way of recruiting respondents means that the results cannot be generalised to a wider population in this case. We had no specific evaluative questions in mind; We aimed to demonstrate a method which can be easily adapted to a specific research question. A short semi structured interview guideline was designed on the theme of \"What are the important current problems facing the USA and what are the (immediate and underlying) reasons for those problems?\". We aimed to construct an overall collective \u201cToC\u201d around problems in the USA. As it does not encompass a specific intervention this theory is not an example of a program theory. This interview guideline was implemented via an online interview \"AI interviewer\" called \"Qualia\", which uses the OpenAI Application Programming Interface (API) to control the AI\u2019s behaviour. Qualia is designed to elicit stories from multiple individual respondents, in an AI driven chat format. Individual respondents are sent a link to an interview on a specific topic and, after consenting, are greeted by the interviewer. Rather than following a set list of questions, the interviewer is instructed to adapt its responses and follow up questions depending on the respondents' answers, circling back to link responses and asking for more information as appropriate, focusing on the interview's objective mentioned above. These behaviours are based on the instructions written by the authors. The respondents, who had the level of \u201cMaster\u201d on Amazon's MTurk service, each completed an interview. The Amazon workers were given up to 19 minutes to complete the interview. We repeated this interview at three different timepoints in September, October and November 2023, inviting approximately N=50 respondents each time. The data from the three timepoints was pooled."}, {"title": "Step 2a Coding the interviews  Constructing a guideline", "path": "/700 Qualia/Step 2a Coding the interviews  Constructing a guideline.html", "text": "Step 2a: Coding the interviews / Constructing a guideline Once the interviews were completed, we wrote instructions to guide the qualitative causal coding of the transcripts, in a radical zero shot style: without giving a codebook or any examples. The assistant was told not to give a summary or overview but to list each and every causal link or chain of causal links and to ignore hypothetical connections (for example, \u201cif we had X we would get Z\u201d). We told the AI to produce codes or labels following this template: 'general concept; specific concept'. We gave no examples, but expected the AI to produce labels like: \u201ceconomic stress; no money to pay bills\u201d. We call the combination of both parts a (factor) label. The assistant was told also to provide a corresponding verbatim quote for each causal chain, to ensure that every claim could be verified. Codings without a quote which matched the original text were subsequently rejected, thus reducing the potential for \u201challucination\u201d."}, {"title": "Step 2b Coding the interviews  Coding", "path": "/700 Qualia/Step 2b Coding the interviews  Coding.html", "text": "Step 2b: Coding the interviews / Coding The final instructions were human readable and could have been given to a human assistant. Instead, we gave these instructions to the online app \"Causal Map\", which used the GPT 4 OpenAI API. As the transcripts were quite long (each around a page of A4 in length), each was submitted separately. The \u201ctemperature\u201d (the amount of \u201ccreativity\u201d) was set to zero to improve reproducibility. The Causal Map app managed the housekeeping of keeping track of combining the instructions with the transcripts, watching out for any failed requests and repeating them, saving the causal links identified by the AI, etc."}, {"title": "Step 2c Coding the interviews  Clustering", "path": "/700 Qualia/Step 2c Coding the interviews  Clustering.html", "text": "Step 2c: Coding the interviews / Clustering The coding procedure resulted in many different labels for the causes and effects, many of which overlap in meaning. Even the general concepts (e.g. \u201ceconomic stress\u201d) were quite varied. The procedure for clustering these labels (including both the general and specific parts of the label) into common groups with their labels was a three step process based on assigning to each of the original labels an embedding. An embedding is a numerical encoding of the meaning of each label (Chen et al., 2023) in the form of a point in a space, such that two labels with similar meaning are close in this space. For any two such vectors, a measure cosine similarity can be calculated representing the approximate similarity in meaning between the labels which they encode: 1. Inductive clustering . First, we grouped the labels into clusters of similar labels using the hclust() function from the stats package of base R (R Core Team, 2015). 2. Labelling. We then asked an AI to find distinct labels for each cluster. We also manually inspected these labels with regard to the original labels within each cluster and adjusted some of them. 3. Deductive clustering. We then discarded the original clustering, created embeddings for the new labels, and formed a new set of clusters, one for each of the new labels, assigning each original label to one of the new labels, the one to which it was most similar, providing the similarity was at least higher than a given threshold. This additional deductive step ensures that each member of each new cluster is sufficiently close in meaning to the new cluster label, rather than just to the other members of the cluster. After each sub step, we checked the AI\u2019s results to ensure that the instructions were being followed correctly and, if they weren't, the instructions were tweaked or rewritten and tested again to ensure quality and consistency."}, {"title": "Workflows - Barbara", "path": "/750 Workflows/Workflows - Barbara.html", "text": "Yes, well you can do automated causal mapping coding inside Causal Map, an online app which was originally designed for manual coding (\"Nvivo but for stories\"). We now have a new mega app called Workflows which is specifically designed for causal mapping of large amounts of text in a reproducible/verifiable way that isn't open for the public at the moment, we use it ourselves for clients' causal mapping projects (that is our business model). I also really liked your comment to the presentation on how to quality assure / quality test AI outputs and discussed something similar with the presenter after the session. He suggested using the \u201creasoning model\u201d of ChatGPT (which I don\u2019t know anything about as I\u2019m not an AI expert) because AI is able to say \u201cthere\u2019s no simple answer to this and it depends on how you define \u201cvariety\u201d\u201d etc. ideally providing examples of different correct answers based on different definitions. We are pretty opinionated about these things! In a nutshell: We think: Giving any AI the freedom to make evaluative judgements as a black box is all wrong. vague \"big black box\" questions like \"summarise this document\" or \"was this project effective\" should be deconstructed into multiple \"tiny black box\" tasks like \"does this paragraph mention health behaviour\" which are much more intersubjectively verifiable / have good inter rater reliability and which an AI can handle transparently. The best generic way of making sense of a large number of documents for evaluation purposes is to code each and every causal claim (\"causal mapping\") and collect those claims into a database of claims: a causal map or a causal knowledge graph. This can be quite easily done by an AI with thousands of \"tiny black box\" tasks. We see causal mapping in this sense as \"causal QDA\" which has some advantages over other kinds of QDA: you don't have to think so hard about \"what themes should I code for\" for evaluation purposes, causal information is 75% of what you need to gather Many evaluation questions can be then (at least partially) answered more or less automatically by querying that causal map, e.g. show me all the causal pathways from Intervention X to Outcomes Y and Z which were each mentioned in their entirety by the same individual or report, ignoring the claims which were assessed as dubious . The result of all this is essentially a set of instructions, a script or algorithm (nothing essentially to do with AI, just a long, nested list of tasks which a tireless human could carry out) which deconstructs high level evaluation questions into a series of intersubjectively verifiable tasks. Our Workflows app helps the user create that script, which you then basically just run, wait as documents get coded and tables and maps appear in answer to your questions. You can run it again the next day or with a similar but different AI model or a bunch of humans and you should get similar answers. The human is very much in the loop in the sense of designing this script, but it's high level input, we can then go and watch the flowers grow while the AI does the grunt work. Caveats This is all about volume and scale and making sense of lots of different sources or documents. containing fairly generic causal claims, e.g. from diverse stakeholders often in multiple languages. It's kind of the opposite of process tracing and its intricate, high stakes examination of a fairly small number of detailed causal claims (I think). Causal mapping is not really a method of causal inference: all it does is assemble evidence about what these sources think, it's still up to the evaluator to decide what conclusions can be drawn from that. (QuIP on the other hand, though it uses causal mapping, is more explicit about how to make causal conclusions from causal mapping data.) re the \"reasoning model\" the presenter mentioned we think that is exactly the wrong way round: give loads of evaluative freedom to a big black box and then also ask it to invent a plausible story about how it reached that conclusion. To be fair, some of the newer reasoning models really are able to display their actual working retrospectively. That's quite an improvement because it offers some retrospective transparency (a bit like saying to an evaluator, don't submit an inception report or an evaluation matrix in advance, just do your magic and then when you present your conclusions tell us how you arrived at them). I still think it is the wrong way round though."}, {"title": "Workflows description at bullet", "path": "/750 Workflows/Workflows description at bullet.html", "text": "opinionated AI coding Causal Map Workflows: Use AI to make sense of mountains of texts \u2014 rigorously and verifiably What is Workflows ? Workflows is a tool used by Causal Map Ltd to make sense of large amounts of text data (interviews, reports) supported by generative AI. At the moment, use of Workflows is not open for public use. Instead, we use it internally to provide verifiable and transparent analysis and reporting to clients. Clients can then access their workflows in the app to view and, if they wish, verify each step. !notion image Which questions can Workflows help answer? All of the Questions You Can Answer with Causal Mapping, for example: Causal pathways New, emerging and unexpected factors and outcomes Project impact through the eyes of stakeholders and other sources Contribution of projects to outcomes QuIP: Synthesis of Qualitative Impact Protocol evaluations OH: Synthesis of Outcome Harvesting processes MSC: Synthesis of stories collected for Most Significant Change processes Comparisons between groups and time points Vignettes: which stories are both remarkable and frequent? More generally, any kind of question you can answer by analysing text systematically, reproducibly and verifiably, page by page or paragraph by paragraph: Stakeholders\u2019 mental models Social Network Analysis Thematic Analysis What are the benefits? Scalability: Process large amounts of text data quickly and efficiently. Transparent & Verifiable Results: Based on a clear, reproducible audit trail. Include more voices: Not just small samples of the easily accessible ones. For Evaluators and Contractors Enhanced Bids: Offer state of the art techniques to make your proposals more competitive. Time & Cost Savings: Reduce manual coding and analysis efforts. Reproducible Workflows: Guarantee that analysis steps are documented and repeatable. !notion image Why is Workflows better than tools like ChatGPT or NotebookLM? break down big, vague tasks into small, easy ones so that we don\u2019t leave the AI to make big judgements all on its own use a clear set of basic tools for each of those small, easy steps speed up and automate the application of hundreds of steps like prompts and filters manage and keep track of your work so you know exactly what you did last year, and can reproduce the same results in the same way. Why Causal Map Ltd? Innovation in qualitative analysis and reporting Broad experience in evaluation and social science research."}, {"title": "Workflows gdoc", "path": "/750 Workflows/Workflows gdoc.html", "text": "https://docs.google.com/document/d/1nexgcqgtK nTThCot7f35An1TRi3tdp4i4ZrHoPnfbc/edit?tab=t.0 Making sense of mountains of text: The Causal Map Workflows app. What is Workflows? Workflows is a tool used by Causal Map Ltd to make sense of large amounts of text data (interviews, reports) supported by generative AI. At the moment, use of Workflows is not open for public use. Instead, we use it internally to provide verifiable and transparent analysis and reporting to clients. Clients can then access their workflows in the app to view and, if they wish, verify each step.! Which questions can Workflows help answer? Project impact through the eyes of stakeholders and other sources New, emerging and unexpected factors and outcomes Causal pathways OH: Synthesis of Outcome Harvesting processes MSC: Synthesis of stories collected for Most Significant Change processes QuIP: Synthesis of Qualitative Impact Protocol evaluations Contribution of projects to outcomes Stakeholders\u2019 mental models Comparisons between groups and time points Vignettes: which stories are both remarkable and frequent? What do you get? Scalable Analysis: Process large amounts of text data quickly and efficiently. Transparent & Verifiable Results: Based on a clear, reproducible audit trail. Include more voices: Not just small samples of the easily accessible ones. For Evaluators and Contractors Enhanced Bids: Offer state of the art techniques to make your proposals more competitive. Time & Cost Savings: Reduce manual coding and analysis efforts. Reproducible Workflows: Guarantee that analysis steps are documented and repeatable. Why Causal Map Ltd? ! Innovation in qualitative analysis and reporting Broad experience in evaluation and social science research. Frequently Asked Questions What does it cost? We charge by day of our time, \u00a3695+VAT/day. Most of the work isn\u2019t using the app. Most of our time is spent finding out what you want to know and what findings can help you, and iterating our reports until you are happy. Caveats Like humans, the AI processor makes mistakes and needs to be monitored. If you want an overall picture, the \"noise\" it can produce probably won't matter and you can get very good results almost out of the box. However at the moment, if you want to be sure that each individual piece of coding is correct, you'll probably need to tweak a not insignificant proportion of the coding it produces. Why causal mapping? At Causal Map Ltd., we promote causal mapping as a really useful and effective way of processing text which can be applied with surprisingly little adjustment to a great variety of evaluation tasks. Our Workflows software provides ready made causal mapping steps, which can be used alone or seamlessly alongside other text processing tasks like thematic analysis. And the software can of course produce graphical causal maps. As the software can also be used without any causal mapping steps or visualisations, it could In principle be seen as also occupying a similar space to any of the market leader CAQDAS applications which now offer AI assistance, but with more explicit, transparent workflows. However it is not our intention to compete directly with this kind of general use software. What is wrong with using AI as a \"black box\"? At Causal Map Ltd, we use AI to both collect and analyse qualitative data. \ud83e\udd16 We believe that the use of AI should also follow published guidelines to ensure transparent and valid results. We are not comfortable with procedures which rely on giving the AI the freedom to make evaluative judgements. So we do not, for example, ask the AI \u201cwhat are the main or most important causal stories in the document\u201d. We do not ask the AI to make summaries. Instead, we use the AI only as a tireless low level assistant to exhaustively and transparently identify large numbers of individual causal claims (ideally, every single one of them) within the texts. \ud83d\udd0d We follow established qualitative social science procedures for coding, with the aim that each step of the process from coding to analysis is transparent and verifiable. We don\u2019t rely on AI to simplify the coded data or decide, for example, which themes are most important. AI Powered automatic causal coding The app is enabled with OpenAI's GPT 4o and 4o mini and other models to automatically identify causal claims and connections within qualitative data like interview transcripts and documents. The AI follows detailed coding guidelines to exhaustively extract individual causal links, which are then verified against the original text. This AI powered functionality allows for automated coding of qualitative data, significantly speeding up the process of identifying causal connections in texts. The Story of Workflows An app for making sense of text (interviews, reports \u2026) at scale. Using the power of genAI to synthesise and visualise the meaning of texts and answer high level questions about them but with a verifiable data audit trail, step by step from text to final analysis. A lot of evaluation work involves making sense of texts (interviews, reports\u2026). That\u2019s hard to do at any kind of scale, which often means using the most convenient and easy to reach sources and ignoring the rest. And it can involve weeks or months of work to create a coding framework and to code say 100 pages of text. Generative AI seems to provide a solution to scaling. That\u2019s a big challenge and a big opportunity. But there is a big problem to solve first. To allow an AI to make evaluative judgements on its own is to rely on a big black box which is not transparent, or reproducible, or trustworthy, or verifiable. We may as well ask some random person we meet on the street to write our report for us. The solution should have these three parts. First: keep track using reproducible workflows. Real life research and evaluation assignments involve multiple tasks and sub tasks. Traditionally, quantitative scientists have used workflows to keep track of their working in a reproducible way. No more \u201cI found the graph but it needs updating and I can\u2019t find which version of the code produces it\u201d. For example we can construct a documented workflow like a Jupytr workbook or an R markdown document: a text document which includes the instructions for importing data, carrying out tasks and displaying output. It\u2019s like a human readable computer program which when run produces outputs like charts and tables reproducibly. Qualitative scientists too have been strong on documenting their research process, but as most of the steps involve human labour and human judgement, you can\u2019t just \u201crerun\u201d a qualitative researcher\u2019s notebook and see the results reappear somewhere. Now, AI is blurring the boundaries between qualitative and quantitative work. Some \"qualitative\" processes like thematic coding can now be carried out by machine, with some caveats. But, hacking around on the prompt until we get some sort of an answer and pasting the answer into a document somewhere \u2013 that\u2019s ok while we are learning and exploring, but it isn\u2019t reproducible or verifiable. An evaluation commissioner wouldn\u2019t accept quantitative work which we gave to a random person who claimed to be a statistician but who sent us some tables and graphs. And nor should they accept the results of \u201cusing an AI\u201d if the working is not documented and verifiable. Some evaluators are already exploring how to apply reproducible, workflow based procedures to AI supported work, mainly with Python. However, just because an AI workflow is documented does not mean it is transparent. Which brings us to the second demand. Second: breaking down big, vague tasks into small, easy ones so that we don\u2019t leave the AI to make big judgements all on its own How can we reformulate hard tasks into easy ones, so we don\u2019t let the AI make significant evaluative judgements for us in an unverifiable way? Here\u2019s how. Break it down. Break down complex, poorly defined tasks (e.g. is this project effective?) into many smaller more clearly defined tasks (e.g. does this paragraph mention behaviour change?) These tasks could be carried out by an army of trained human assistants, but we can use generative AI instead, and monitor its work. Build it up. Use agreed procedures to reconstruct higher level evaluative judgements from the results of the simpler tasks using, and document the workflow transparently. Be clear about what parts of this reconstruction rely on human judgement. The use of rubrics is a good example of this break it down then build it up strategy usually rubrics are applied by humans, but when we have many cases, rubric assignment is a great task to hand off to AIs because we can easily monitor their performance and tweak the prompt and the rubric until the AI becomes as good as a human. But how should I break down big tasks into smaller steps, and exactly which ones, and where and how should I employ an AI for the low level steps, and when I am going to learn all that Python? This brings us to the third demand. Third: providing a set of basic tools for the small, easy to automate steps Workflow based solutions are already being supported by AI service providers. Sort of. But if you ask, say, Google\u2019s NotebookLM to process a large table, it may or may not really produce results for each row. Tools like this may produce plausible reports of their \u201cthinking\u201d which is a big step forward, but we don\u2019t really know exactly what steps were really carried out, only what it feels like telling us. To do this properly, we need to learn to use genAI APIs and something like LangChain or Python and construct their own reproducible workflows using say a Jupytr notebook and selecting appropriate packages. But each kind of solution is different and can be time consuming. It can be difficult to ensure or check that there are no black boxes left anywhere in the workflow. So the third part of the solution we need is: A toolkit of the most common low level tasks and recombination steps so that workflows for making sense of texts (including AI powered steps such as coding, clustering similar texts and finding labels for clusters) can be presented as simply the application of tasks from this toolkit, one after the other. Each step, whether AI powered steps or ordinary data manipulation steps like sorting and filtering could also in principle be understood and followed by an army of human assistants. For example: \u201cTake these documents and break them up into paragraph sized chunks. To each paragraph, add information about the age and role of the speaker. Ask the AI to decide whether each paragraph mentions changes in health behaviour. Reassemble all the examples into a single text, prepending each with the age and role of the speaker. Next, \u2026\u201d All the work of handing off tasks to the AI, reconstructing the results, saving versions of the prompts and the workflow etc should not be carried out by a black box AI but by ordinary computer software with published code. Only specific, low level coding tasks should be given to the AI. So at Causal Map Ltd we looked around for alternatives but in the end decided we had to build our own solution. Our solution: workflows.causalmap.app We present software (workflows.causalmap.app) which enables users to construct their own workflows for making sense of text, step by step from raw text to final outputs. Each workflow is simply a stored piece of text: a list of steps, one per line, described in a basic scripting language which is more or less human readable. Each line in the command editor has a data table associated with it. The workflow starts by importing data from storage, from a web page or a Qualia interview, or from a previously uploaded set of documents, resulting in a table of data. Each subsequent line operates on the current data table and applies a verb (pivot, filter, sort, code \u2026) to that table, resulting in another table (or an output like a chart or a map). The user can click on any line to see the result of the workflow up to that point, usually the data table itself or alternatively a visualisation of it, like a causal map. ! We can insert AI operations into the workflow at any point. We can save and re use prompts which work line by line on the current data table. Each prompt returns one or more rows with whichever columns we ask for, which are reassembled into a new table. Using AI prompts becomes just like any other manipulation of a data table.! Features Currently, the following features are implemented: Data input: Loading text from a database or a Qualia interview or directly by scraping a URL Uploading one or more PDF or Excel (xlsx) documents Steps useful for AI processing: Chunking and unchunking text eg combining shorter texts into longer ones Managing and editing different AI prompts and sequences of prompts, processing them in parallel, and caching the results in a noSQL database. The user does not need to explicitly \u201csave\u201d different analyses: if it\u2019s been done before, the app will find the results and present them without need for reprocessing. Repeating multiple steps in a loop Clustering individual texts into groups with a similar meaning and finding labels for them \"Magnetising\" texts: given a set of text labels designated as \u201cmagnets\u201d, relabelling each of a large set of individual texts with its closest magnet, if any are close. Causal mapping steps: All the same functionality as in the Causal Map app, e.g. Filtering links in a causal map, for example to show the most frequently mentioned factors or links, Zooming in or out when using hierarchical coding Tracing paths and threads from one set of factors to another Focusing on a particular set of factors Relabelling factors e.g. with cluster labels Calculating and displaying differences between groups Common steps for manipulating tables: Selecting and renaming columns, filtering by value or meaning, sorting, performing simple calculations, combining columns, appending, merging and pivoting tables Output steps: Printing tables in text format, using templates if required Providing tables which can be manually filtered, sorted and exported Displaying causal maps Storing individual results for use later in the workflow. Evaluative judgements and evaluator responsibility A clarification: We are not saying that breaking down evaluation tasks into this kind of workflow frees evaluation from human judgement. Quite the opposite. We think the discipline of being more explicit about how evaluative judgements are made is a good thing, whether the steps are carried out by humans (as in the past) or with AI assistance. If you didn\u2019t have a clear workflow from data to judgements before AI, DON\u2019T lean on the black box of the AI to cover that up. Instead, make the workflow explicit / human verifiable and then use the AI to speed up the process. Even where a workflow does not include explicit human input at any stage, the decision to use this particular workflow, the details of its design, quality assurance checks, and, crucially, interpreting the results so they will be correctly interpreted by the evaluation audience these are all the responsibility of the evaluator. Current status of the app We don\u2019t have immediate plans to open up the app for general use. The backend the Python code which implements the steps asynchronously to deal with long tasks. Documentation is still work in progress, the causal mapping steps are already covered in the Causal Map Guide. The frontend UI provides: A powerpoint like presentation mode so that external viewers can step through a workflow and view intermediate and final results, together with comments, section headings etc. Ability to save and recall different workflows and individual custom texts e.g. prompts, lists of magnets, with good versioning, to address the problem of \u201chow did we get that particular map?\u201d A searchable gallery of workflows Full screen mode Authentication via Google Firebase and Authorisation based on Causal Map permissions to access the files stored there Coming soon: an AI assistant to write the steps for you. Technical details Backend: Python, Quart (asynchronous server for time consuming processes), hosted at Heroku. NoSQL caching at MongoDB. SQL data stored at Heroku. Access to Causal Map SQL data. OpenAI API with various models available. Implicit generation of embeddings using OpenAI\u2019s text embedding 3 small, cached at MongoDB. Frontend: Alpine, Axios, CodeMirror editor, Tabulator tables \u2026 Why a new app? Causal Map 3 is written in R Shiny which is not a platform of choice for AI. CM3 is a user focused platform with lots of buttons and sliders and a fixed approach to applying filters. Whereas Workflows: Uses Python which is where all the new things are being created. It is much easier to scale and maintain It is easy to write new functions without having to provide additional UI (corresponding buttons, sliders, etc). Functions aka commands can be arbitrarily chained We can experiment and optimize new approaches without having to rewrite the UI every time Common patterns of commands can be easily combined into templates Prompts and workflows have version numbers There is no need to save different versions of coding results. Thanks to noSQL caching, if a particular prompt has been run before, the app will find the cached results. No restriction on multiple users working with the same data at the same time AI tasks and other tasks can be arbitrarily mixed and chained. Applying an AI step should feel no different from adding any other filter in Causal Map How we do automated causal mapping We import the texts to be coded, together where appropriate with meta data (such as age and gender of respondent etc) into Causal Map Workflows. We agree on an approach for the coding e.g. Purely \u201czero shot\u201d coding which allows the app to develop its own generic factor labels (\u201cempirical codebook\u201d). This can be augmented by a \u201cglobal orientation\u201d for example telling the AI to pay particular attention to power relations between people, and code them in the light of a particular theory of power relations. We may need to provide background information to the text. Or say that a particular organisation might be referred to in different ways, and to code all mentions in the same way. Or we might say that we are particularly interested in chains in which some intervention (someone or some organisation intervening or trying to provide help in some way) is at (or near) the beginning of the chain, and in which important outcomes (like people's lives getting better or worse) are at (or near) the end of the chain. We will usually test an initial set of instructions on a subset of the text and possibly then revise them. After each actual wave of coding, we conduct auto clustering. Your tests reach the AI broken up into many separate batches. When employing open coding, with zero shot, the factor labels will probably form a pile of overlapping concepts, with similar ideas expressed in different ways. So we use auto clustering to form the factors into clusters of very similar ideas, and find a shared label for all of them. This process of auto clustering is very useful to get a sorted overview of the kind of material the AI is identifying. When applying automatic clustering, we take care to set the \u201cheight\u201d aka \u201cstrictness\u201d parameter to balance the desire to have only a small number of large, simple factors with the caveat that the clusters should not contain any pairs of factors with importantly different meaning. We will usually also, after each wave of coding, produce some simple causal maps to give an overview of what the AI is finding with the current instructions. Zero shot coding can be followed by a second phase in which the list of factors from the empirical codebook, probably auto clustered as described above, and possibly adjusted by the client, are given as suggestions in a new semi closed set of instructions. The coding is re run with this new codebook. This ensures more thorough and consistent coding of the factors of particular interest. We can tweak the instruction to insist more or less strongly that only the suggested codes should be used. Alternatively you can skip zero shot coding and start with your own codebook. This can be simpler and is more suited to more focused, closed research questions; but it does also hinder identification of unexpected concepts. We recommend that your codebook is a simple list of relevant causal factors. It is also possible to provide more detailed examples of the target factors, but this can hinder identification of unexpected details. In theory it is even possible to start by getting an expert to hand code some of the texts and then producing a customised instruction based on this coding as an example. However in practice this can be complicated and is only really suitable if you are looking for some very specific information (and in this case it might be easier for you to use a tool like Ailyze instead). At each step we will keep an eye on the quality of the coding: Recall (is the AI finding a sufficient proportion of all the causal claims in the text?) Precision (is the proportion of inaccurate AI coding sufficiently low?) Is there any evidence of any kind of bias (the AI is consistently missing some codings of a certain type, or is consistently miscoding in a particular way). Otherwise we can assume that even if the recall and precision are not very high, the overview causal maps will be accurate in aggregate. If you need a high degree of accuracy of causal coding (identification of causal claims), it is possible to manually check and if necessary correct each coding. We can help you to do this yourself. This can take a long time, from many hours to many days. Assuming we were successful in helping you turn your research aims into an appropriate research design, producing the results should now be quite straightforward, producing: overview maps to answer research questions (see above) corresponding quotes to illustrate the maps corresponding tables of frequencies We can produce additional and more detailed outputs for you, but this will mean additional time."}, {"title": "Evaluator niche in AI", "path": "/800 Applications/Evaluator niche in AI.html", "text": "Can evaluators find a niche in auditing whether AI applications are trustworthy, culture aware, valid and transparent? What would need to change in AI systems to make them suitable for a transformational agenda? Can evaluators position themselves as professionals with the right skill set to make this happen, monitoring the transparency, trustworthiness and (cultural) validity of AI applications? And how will evaluators build the necessary competencies to take this agenda forward?"}, {"title": "Full - Strengthening OH with causal mapping", "path": "/800 Applications/Full - Strengthening OH with causal mapping.html", "text": "publish: true Strengthening OH with causal mapping !image.png Here is a tidied, reformatted version of your text with clear headings, paragraph breaks, and improved readability: Strengthening Outcome Harvesting Analysis with AI Assisted Causal Mapping shortened full version Written by: Heather Britt, Steve Powell, Gabriele Caldas Cabral Summary This case study explores how AI assisted causal mapping can enhance Outcome Harvesting (OH) analysis by revealing interrelationships between outcomes and identifying new actors contributing to change. The pilot demonstrates how this approach provides actionable insights and strengthens causal relationship analysis in OH. It emphasizes the importance of a principle led analysis plan and human expertise in guiding the AI process. Introduction Outcome Harvesting is a powerful approach for discovering emergent changes\u2014whether predicted or unpredicted, positive or negative\u2014and documenting how those changes occurred. While many methods capture changes in those directly involved with a project, OH captures changes farther down the causal pathway. However, evaluators often struggle to explore interrelationships between multiple outcomes. This case study describes how an OH practitioner (Heather Britt) collaborated with causal mapping practitioners (Steve Powell and Gabriele Caldas) to expand causal contribution analysis in OH using an AI assisted causal mapping app. They analyzed OH data from a completed education project. Outcome Harvesting: Analysis Limitations While OH documents causal pathways contributing to individual outcomes well, evaluators find it difficult to make sense of interrelationships between multiple outcomes and their causal pathways. This limits their ability to answer questions about causal contribution. Current OH practice often uses descriptive statistics to summarize data by outcome components (e.g., types of change agents or social actors) and reports findings in charts. Another approach arranges outcomes on a timeline to determine logical relationships. Our pilot explores whether AI assisted causal mapping can address these limitations by analyzing causal relationships between outcomes. The Pilot Core Question Can AI assisted causal mapping address the limitations of OH analysis? Heather Britt reached out to Steve Powell and Gabriele Caldas to explore whether causal mapping with the Causal Map app could enhance OH analysis. Causal mapping techniques, developed over 50 years ago, have been used across disciplines to identify and visually represent causal relationships in qualitative data. The Causal Map app computerizes this technique, allowing efficient coding, analysis, and visualization of information from multiple sources (interviews, reports, surveys, narratives), either manually or with AI assistance. The AI assisted capacities of the app were critical for revealing interrelationships between multiple outcomes. Pilot Data Set The pilot used data from the final evaluation of an education project (Girls Education project, 2016\u20132021) disrupted by political turmoil and COVID 19. The project adapted activities during lockdown, and OH was used to capture outcomes in five domains where the theory of change was no longer valid. The evaluation team interviewed 49 change agents and drafted 103 outcome descriptions across five domains. The pilot data included both interview transcripts and outcome descriptions. For the pilot, the domain Increased community support for education was selected, with 13 outcome descriptions analyzed. Analysis Process Step 1: Draft a Principle Led Analysis Plan Three guiding principles steered analysis decisions: 1. Prioritize local leadership: Use AI while keeping sensemaking and learning in the hands of local evaluators. 2. Protect OH integrity: Adapt methods as needed while staying true to OH principles, including \u201cLess is more\u201d (avoid collecting more data than can be analyzed). 3. Produce accurate, actionable maps: Human judgment is required to error check data and interpret maps. Step 2: Segment Data by Outcome Domain Segmenting data by domain increases the likelihood of finding coherent causal pathways and facilitates error checking. The pilot focused on one domain to analyze causal relationships between outcomes. Step 3: Decide When to Apply AI Assisted Causal Mapping The pilot compared applying causal mapping to interview transcripts versus outcome descriptions. Outcome descriptions, crafted by local evaluators, were more accurate and required less error checking than transcripts. Thus, mapping outcome descriptions was preferred to preserve local leadership and OH integrity. Findings from Causal Maps Relationships Between Outcomes The AI identified causal links between the 13 outcome descriptions, revealing that outcomes influenced one another. For example, parents actively supporting home learning and leaders convincing parents to participate were central factors. Factors Contributing to Domain Level Outcome Mapping revealed additional actors influencing the domain level outcome \u201cCommunity supports learning,\u201d including unexpected contributors like Ministry officials. Conclusion AI assisted causal mapping advanced OH analysis beyond descriptive statistics by: Analyzing multiple outcomes to determine causal contributions. Revealing interrelationships between causal pathways. Confirming known change agents and identifying unexpected influences. Showing how domain level changes contribute to broader changes. Causal mapping offers rich, flexible analysis that can be explored in multiple ways to answer diverse evaluation questions."}, {"title": "Strengthening OH with causal mapping", "path": "/800 Applications/Strengthening OH with causal mapping.html", "text": "publish: true Strengthening OH with causal mapping !image.png Enhance Outcome Harvesting analysis with AI assisted Causal Mapping! This new Causal Pathways case study is the story of how we collaborated with Heather Britt to explore ways to strengthen the analysis of causal relationships in Outcome Harvesting. Get the case study here(https://5a867cea 2d96 4383 acf1 7bc3d406cdeb.usrfiles.com/ugd/5a867c ad000813c80747baa85c7bd5ffaf0442.pdf) Our pilot demonstrated that AI assisted Causal Mapping can reveal interrelationships between outcomes and identify new actors contributing to change. \u2705 We describe the pilot process \u2705 Explain how we interpreted the findings \u2705 And share five practical tips \ud83d\udca1 A principle led analysis plan and the right human expertise are key to in guiding the use of AI in analysis. Please reach out if you\u2019d like to learn more about how approach might strengthen your Outcome Harvest or other qualitative analysis tasks. Thanks to Heather and the great folks at the Causal Pathways Initiative (Jewlya Lynn, Carolina De La Rosa Mateo) Check the Causal Pathways Case Studies page here: https://www.causalpathways.org/casestudies."}, {"title": "Using AI to facilitate feedback on the learning experiences of doctoral students.", "path": "/800 Applications/Using AI to facilitate feedback on the learning experiences of doctoral s-762280.html", "text": "publish: true The Causal Map team has conducted a trial of an innovative approach to securing feedback from students using online open ended interviews conducted by the app QualiaInterviews, which uses generative AI (gen AI), followed by a second use of gen AI within the app Causal Map to semi automate causal coding of the narrative transcripts thereby generated. The trial was conducted with students registered on the doctorate in policy research and practice (DPRP) at the University of Bath. This generated credible evidence of diverse positive and negative drivers of learning from eleven students. The trial suggests that incorporation of gen AI into causal mapping of narrative data about students\u2019 study experiences enhances the potential to use the method cost effectively on a larger scale, whether alongside or instead of more traditional approaches to eliciting student feedback on teaching and learning. See our findings in this paper(https://drive.google.com/file/d/1Ghx5bfvnGdE9R6PJrG9KT2SpzD2CiDDP/view?usp=sharing) See a summarised report in this presentation(https://drive.google.com/file/d/1YuT5IbIMYlcx vlvkZfycByc94s6kCX /view?usp=sharing) !image (20).png.png)"}, {"title": "Existentialists", "path": "/9100 philosophy/Existentialists.html", "text": "flashcards Sartre shared many core ideas with other existentialists, but there were notable differences: Simone de Beauvoir : :: She expanded existentialism into feminist theory, arguing that women's oppression is a form of \"bad faith.\" She emphasized that women must embrace their freedom and agency to overcome socially constructed roles. <! SR:!2025 07 25,1,230 Albert Camus : ? Though often associated with existentialism, Camus rejected the label. He focused on the concept of the \"absurd,\" the conflict between humans' search for meaning and the indifferent universe. While Sartre emphasized freedom and choice, Camus highlighted the need to embrace the absurd and live with it. <! SR:!2025 08 01,8,250 Martin Heidegger : ? An important influence on Sartre, Heidegger focused on \"Being\" rather than individual freedom. His concept of \"thrownness\" (Geworfenheit) described how individuals are always situated in a world not of their own making. While Sartre emphasized radical freedom, Heidegger focused on the interplay between individual existence and the world. <! SR:!2025 08 01,8,250 Karl Jaspers : ? Jaspers emphasized the importance of \"limit situations\"\u2014moments of existential crisis that force individuals to confront the fundamental questions of existence. While Sartre focused on freedom and choice, Jaspers leaned more towards the transcendental aspects of human experience. <! SR:!2025 07 26,2,230 \"Existence precedes :: essence\" is essentially saying that we aren't born with a predetermined purpose or nature. Instead, we create our own meaning and values through our choices and actions. The philosophical language can be dense and off putting, but the core idea is straightforward: we define who we are by what we do, not by any inherent essence or destiny. <! SR:!2025 08 04,11,270"}, {"title": "! Beyond causal mapping - teleology, stories", "path": "/930 Beyond causal mapping/! Beyond causal mapping - teleology, stories.html", "text": "So is the reason that character x does fall inlove for the character it looks like Hugh Grant is just because that's a meme that everyone understands but is this a cause of explanation"}, {"title": "storytelling overview from gemini", "path": "/930 Beyond causal mapping/storytelling overview from gemini.html", "text": "flashcards storytelling , \"Evidence gives stories substance, :: but stories give evidence meaning,\" (https://assessmentinstitute.indianapolis.iu.edu/overview/institute files/2021 institute/handouts wednesday 2021/22P jankowski handout.pdf) <! SR:!2025 08 13,20,250 Adaptive Collective Sensemaking A compelling and concrete rationale for the utility of storytelling in evaluation and social science emerges from evolutionary and cognitive perspectives. Bietti, Tilston, and Bangerter propose that the distinct adaptive value of storytelling is ===rooted in its capacity for \"making sense of non\u2010routine, uncertain, or novel situations\".3 This process, which they term \"adaptive collective sensemaking,\" enables groups to collaboratively interpret and respond to new information or unexpected events. === <! SR:!2025 07 28,4,210 If storytelling serves as a fundamental human mechanism for \"adaptive collective sensemaking,\" particularly in uncertain or novel contexts :: its systematic integration into program evaluation can transform the evaluative exercise. Instead of being a purely retrospective judgment, evaluation can become a dynamic tool for ongoing program learning, <! SR:!2025 07 31,7,230 narrative inquiry narrative inquiry vs storytelling \"Narrative Inquiry\" is recognized as a formal qualitative research methodology. Its primary focus is on understanding how individuals construct meaning from their lived experiences, typically through sustained, in depth engagement with their personal stories. Narrative inquiry is systematic in its approach to the collection (e.g., through interviews, journals, field notes, letters) and analysis of these \"field texts\".6 In contrast, \"storytelling\" as a general activity in evaluation ? might encompass a broader range of practices, including the strategic use of anecdotes for communication or stakeholder engagement.11 Mattingly and Lawlor offer features that characterize a story or narrative\u2014such as being \"event centered,\" \"historically particular,\" concerning \"human action,\" and possessing \"plots\"\u2014which help differentiate them <! SR:!2025 08 05,12,238 a serious framework for storytelling ? :: narrative inquiry <! SR:!2025 07 27,3,210 Donald Polkinghorne \"narrative smoothing\" :: the process of refining and organizing collected narrative data to clearly address the research question\u2014 <! SR:!2025 08 04,11,230 \"Diachronic Organization,\" :: the chronological structuring of narrative accounts to reveal developmental trajectories or sequences of event <! SR:!2025 08 05,12,230 Exploring \"Narrative Knowing\" and the Emplotment of Human Action Donald Polkinghorne's contributions to narrative theory and methodology emphasize \"narrative knowing\" :: the idea that human beings understand themselves, their experiences, and their actions primarily through the construction of narratives or \"plots\".5 <! SR:!2025 08 04,11,238 He argued that human action is often best understood through the lens of emplotment, :: suggesting that we live storied lives. <! SR:!2025 08 06,13,230 Polkinghorne introduced the concept of \"narrative rationality,\" :: grasping the meaning of a whole (such as a life or a significant event) by seeing it as a \"dialectic integration of its parts,\" where individual actions and events gain significance from their place within an overarching narrative structure.40 For researchers, <! SR:!2025 07 31,7,210 he also provided a useful methodological distinction between :: \"narrative analysis\" (or \"analysis by means of narrative\"), where the researcher actively synthesizes disparate data elements into a coherent narrative account, and \"analysis of narratives,\" where the researcher examines pre existing stories <! SR:!2025 07 27,3,218 D. Jean Clandinin, often in collaboration with F. Michael Connelly, has been central to defining narrative inquiry as a :: profoundly relational methodology. Their work emphasizes understanding \"experience and story in qualitative research,\" highlighting the co constructed nature of narratives between researcher and participant <! SR:!2025 07 26,2,190 2.2. Spotlight on Specific Techniques: 2.2.1. Most Significant Change (MSC) Photovoice engaging in group discussions to analyze the meaning and context of these images (often using structured facilitation techniques like the 'SHOWED' acronym: :: What do you See here? What is really Happening? How does this relate to Our lives? Why does this concern, problem, or strength Exist? What can we Do about it?), <! SR:!2025 08 06,13,238 Patton :: <! SR:!2025 08 05,12,238 strongly advocates for qualitative methods, including storytelling, arguing that they \"tell the program's story by capturing and communicating the participants' stories\" Patton, storytelling and the cultivation of \"evaluative thinking\" 27, :: he positions storytelling as a critical cognitive and communicative tool. Stories, by humanizing data, providing rich context, and illustrating complex dynamics in relatable terms 11, enable stakeholders to more effectively process and engage with complex evaluation information. <! SR:!2025 07 27,3,210 Catherine Kohler Riessman: Establishing Rigorous Analytical Frameworks for Narrative Research Catherine Kohler Riessman has made seminal contributions to the field of narrative inquiry by developing and articulating systematic and detailed frameworks for the analysis of narratives within the human sciences. ? thematic analysis structural analysis dialogic/performance analysis how is it cocreated? <! SR:!2025 07 27,3,218 Jerome Bruner: Conceptualizing Narrative as a Fundamental Mode of Human Thought and Meaning Construction Jerome Bruner, an exceptionally influential psychologist, advanced the understanding that narrative is not merely a literary form or a way of communicating information, but a fundamental \"mode of thought\" and a primary cognitive tool through which human beings organize their experiences, understand the world, and derive meaning.3 One of his most significant contributions in this area was the distinction between two primary modes of cognitive functioning 39: ? 1. Narrative Thought: This mode deals with the particularities of human experience, focusing on intentions, actions, and the temporally and causally structured sequences of events that constitute lived reality. It is concerned with verisimilitude and the interpretation of meaning in specific contexts. 2. Paradigmatic Thought (or Logico Scientific Thought): <! SR:!2025 08 02,9,210"}]